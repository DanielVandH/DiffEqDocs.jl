<!DOCTYPE html><HTML lang="en"><head><meta charset="utf-8"/><meta content="width=device-width, initial-scale=1.0" name="viewport"/><title>Sensitivity Analysis Â· DifferentialEquations.jl</title><link href="https://cdnjs.cloudflare.com/ajax/libs/normalize/4.2.0/normalize.min.css" rel="stylesheet" type="text/css"/><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script data-main="../assets/documenter.js" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.2.0/require.min.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link href="../assets/documenter.css" rel="stylesheet" type="text/css"/><link href="https://docs.juliadiffeq.org/stable/analysis/sensitivity/index.html" rel="canonical"/><meta content="noindex" name="robots"/><script data-is-old-version="">document.addEventListener("DOMContentLoaded", function(){
    const div = document.createElement('div');
    div.setAttribute('style', 'position: fixed; bottom: 1em; right: 1em; z-index: 999; background-color: #ffaf9c; color: rgba(0, 0, 0, 0.7); border: 1px solid #d54625; border-radius: 4px; padding: 2em; text-align: center');
    const closer = document.createElement('div');
    closer.setAttribute('style', 'position: absolute; top: 0; right: 5px; padding: 5px; cursor: pointer; width: 12px; height: 12px;')
    // Icon by font-awesome (license: https://fontawesome.com/license, link: https://fontawesome.com/icons/times?style=solid)
    closer.innerHTML = '<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="times" class="svg-inline--fa fa-times fa-w-11" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 352 512"><path fill="currentColor" d="M242.72 256l100.07-100.07c12.28-12.28 12.28-32.19 0-44.48l-22.24-22.24c-12.28-12.28-32.19-12.28-44.48 0L176 189.28 75.93 89.21c-12.28-12.28-32.19-12.28-44.48 0L9.21 111.45c-12.28 12.28-12.28 32.19 0 44.48L109.28 256 9.21 356.07c-12.28 12.28-12.28 32.19 0 44.48l22.24 22.24c12.28 12.28 32.2 12.28 44.48 0L176 322.72l100.07 100.07c12.28 12.28 32.2 12.28 44.48 0l22.24-22.24c12.28-12.28 12.28-32.19 0-44.48L242.72 256z"></path></svg>';
    closer.addEventListener('click', () => {
        document.body.removeChild(div)
    })
    const href = documenterBaseURL + '/../stable'
    div.innerHTML = 'This is an old version of the documentation. <br> Click <a href="' + href + '">here</a> to go to the newest version.';
    div.appendChild(closer)
    document.body.appendChild(div);
});
</script><script data-outdated-warner="">function maybeAddWarning () {
    const head = document.getElementsByTagName('head')[0];

    // Add a noindex meta tag (unless one exists) so that search engines don't index this version of the docs.
    if (document.body.querySelector('meta[name="robots"]') === null) {
        const meta = document.createElement('meta');
        meta.name = 'robots';
        meta.content = 'noindex';

        head.appendChild(meta);
    };

    // Add a stylesheet to avoid inline styling
    const style = document.createElement('style');
    style.type = 'text/css';
    style.appendChild(document.createTextNode('.outdated-warning-overlay {  position: fixed;  top: 0;  left: 0;  right: 0;  box-shadow: 0 0 10px rgba(0, 0, 0, 0.3);  z-index: 999;  background-color: #ffaba7;  color: rgba(0, 0, 0, 0.7);  border-bottom: 3px solid #da0b00;  padding: 10px 35px;  text-align: center;  font-size: 15px; }  .outdated-warning-overlay .outdated-warning-closer {    position: absolute;    top: calc(50% - 10px);    right: 18px;    cursor: pointer;    width: 12px; }  .outdated-warning-overlay a {    color: #2e63b8; }    .outdated-warning-overlay a:hover {      color: #363636; }'));
    head.appendChild(style);

    const div = document.createElement('div');
    div.classList.add('outdated-warning-overlay');
    const closer = document.createElement('div');
    closer.classList.add('outdated-warning-closer');

    // Icon by font-awesome (license: https://fontawesome.com/license, link: https://fontawesome.com/icons/times?style=solid)
    closer.innerHTML = '<svg aria-hidden="true" focusable="false" data-prefix="fas" data-icon="times" class="svg-inline--fa fa-times fa-w-11" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 352 512"><path fill="currentColor" d="M242.72 256l100.07-100.07c12.28-12.28 12.28-32.19 0-44.48l-22.24-22.24c-12.28-12.28-32.19-12.28-44.48 0L176 189.28 75.93 89.21c-12.28-12.28-32.19-12.28-44.48 0L9.21 111.45c-12.28 12.28-12.28 32.19 0 44.48L109.28 256 9.21 356.07c-12.28 12.28-12.28 32.19 0 44.48l22.24 22.24c12.28 12.28 32.2 12.28 44.48 0L176 322.72l100.07 100.07c12.28 12.28 32.2 12.28 44.48 0l22.24-22.24c12.28-12.28 12.28-32.19 0-44.48L242.72 256z"></path></svg>';
    closer.addEventListener('click', function () {
        document.body.removeChild(div);
    });
    let href = '/stable';
    if (window.documenterBaseURL) {
        href = window.documenterBaseURL + '/../stable';
    }
    div.innerHTML = 'This is an old version of the documentation. <br> <a href="' + href + '">Go to the newest version</a>.';
    div.appendChild(closer);
    document.body.appendChild(div);
};

if (document.readyState === 'loading') {
    document.addEventListener('DOMContentLoaded', maybeAddWarning);
} else {
    maybeAddWarning();
};
</script></head><body><nav class="toc"><a href="../index.html"><img alt="DifferentialEquations.jl logo" class="logo" src="../assets/logo.png"/></a><h1>DifferentialEquations.jl</h1><select id="version-selector" onchange="window.location.href=this.value" style="visibility: hidden"></select><form action="../search.html" class="search" id="search-form"><input id="search-query" name="q" placeholder="Search docs" type="text"/></form><ul><li><a class="toctext" href="../index.html">Home</a></li><li><span class="toctext">Tutorials</span><ul><li><a class="toctext" href="../tutorials/ode_example.html">Ordinary Differential Equations</a></li><li><a class="toctext" href="../tutorials/sde_example.html">Stochastic Differential Equations</a></li><li><a class="toctext" href="../tutorials/rode_example.html">Random Ordinary Differential Equations</a></li><li><a class="toctext" href="../tutorials/dde_example.html">Delay Differential Equations</a></li><li><a class="toctext" href="../tutorials/dae_example.html">Differential Algebraic Equations</a></li><li><a class="toctext" href="../tutorials/discrete_stochastic_example.html">Discrete Stochastic (Gillespie) Equations</a></li><li><a class="toctext" href="../tutorials/jump_diffusion.html">Jump Diffusion Equations</a></li><li><a class="toctext" href="../tutorials/bvp_example.html">Boundary Value Problems</a></li><li><a class="toctext" href="../tutorials/additional.html">Additional Tutorials</a></li></ul></li><li><span class="toctext">Basics</span><ul><li><a class="toctext" href="../basics/overview.html">Overview of DifferentialEquations.jl</a></li><li><a class="toctext" href="../basics/common_solver_opts.html">Common Solver Options</a></li><li><a class="toctext" href="../basics/solution.html">Solution Handling</a></li><li><a class="toctext" href="../basics/plot.html">Plot Functions</a></li><li><a class="toctext" href="../basics/integrator.html">Integrator Interface</a></li><li><a class="toctext" href="../basics/problem.html">Problem interface</a></li><li><a class="toctext" href="../basics/faq.html">Frequently Asked Questions</a></li><li><a class="toctext" href="../basics/compatibility_chart.html">Solver Compatibility Chart</a></li></ul></li><li><span class="toctext">Problem Types</span><ul><li><a class="toctext" href="../types/discrete_types.html">Discrete Problems</a></li><li><a class="toctext" href="../types/ode_types.html">ODE Problems</a></li><li><a class="toctext" href="../types/dynamical_types.html">Dynamical, Hamiltonian and 2nd Order ODE Problems</a></li><li><a class="toctext" href="../types/split_ode_types.html">Split ODE Problems</a></li><li><a class="toctext" href="../types/steady_state_types.html">Steady State Problems</a></li><li><a class="toctext" href="../types/bvp_types.html">BVP Problems</a></li><li><a class="toctext" href="../types/sde_types.html">SDE Problems</a></li><li><a class="toctext" href="../types/rode_types.html">RODE Problems</a></li><li><a class="toctext" href="../types/dde_types.html">DDE Problems</a></li><li><a class="toctext" href="../types/dae_types.html">DAE Problems</a></li><li><a class="toctext" href="../types/jump_types.html">Jump Problems</a></li></ul></li><li><span class="toctext">Solver Algorithms</span><ul><li><a class="toctext" href="../solvers/discrete_solve.html">Discrete Solvers</a></li><li><a class="toctext" href="../solvers/ode_solve.html">ODE Solvers</a></li><li><a class="toctext" href="../solvers/dynamical_solve.html">Dynamical, Hamiltonian, and 2nd Order ODE Solvers</a></li><li><a class="toctext" href="../solvers/split_ode_solve.html">Split ODE Solvers</a></li><li><a class="toctext" href="../solvers/steady_state_solve.html">Steady State Solvers</a></li><li><a class="toctext" href="../solvers/bvp_solve.html">BVP Solvers</a></li><li><a class="toctext" href="../solvers/jump_solve.html">Jump Problem Solvers</a></li><li><a class="toctext" href="../solvers/sde_solve.html">SDE Solvers</a></li><li><a class="toctext" href="../solvers/rode_solve.html">RODE Solvers</a></li><li><a class="toctext" href="../solvers/dde_solve.html">DDE Solvers</a></li><li><a class="toctext" href="../solvers/dae_solve.html">DAE Solvers</a></li><li><a class="toctext" href="../solvers/benchmarks.html">Solver Benchmarks</a></li></ul></li><li><span class="toctext">Additional Features</span><ul><li><a class="toctext" href="../features/performance_overloads.html">DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types</a></li><li><a class="toctext" href="../features/diffeq_arrays.html">DiffEq-Specific Array Types</a></li><li><a class="toctext" href="../features/diffeq_operator.html">DiffEqOperators</a></li><li><a class="toctext" href="../features/noise_process.html">Noise Processes</a></li><li><a class="toctext" href="../features/linear_nonlinear.html">Specifying (Non)Linear Solvers</a></li><li><a class="toctext" href="../features/callback_functions.html">Event Handling and Callback Functions</a></li><li><a class="toctext" href="../features/callback_library.html">Callback Library</a></li><li><a class="toctext" href="../features/monte_carlo.html">Parallel Monte Carlo Simulations</a></li><li><a class="toctext" href="../features/io.html">I/O: Saving and Loading Solution Data</a></li><li><a class="toctext" href="../features/low_dep.html">Low Dependency Usage</a></li><li><a class="toctext" href="../features/progress_bar.html">Juno Progress Bar Integration</a></li></ul></li><li><span class="toctext">Analysis Tools</span><ul><li><a class="toctext" href="parameterized_functions.html">ParameterizedFunctions</a></li><li><a class="toctext" href="parameter_estimation.html">Parameter Estimation</a></li><li><a class="toctext" href="bifurcation.html">Bifurcation Analysis</a></li><li class="current"><a class="toctext" href="sensitivity.html">Sensitivity Analysis</a><ul class="internal"><li><a class="toctext" href="#Local-Sensitivity-Analysis-1">Local Sensitivity Analysis</a></li><li><a class="toctext" href="#Adjoint-Sensitivity-Analysis-1">Adjoint Sensitivity Analysis</a></li><li><a class="toctext" href="#Global-Sensitivity-Analysis-1">Global Sensitivity Analysis</a></li></ul></li><li><a class="toctext" href="uncertainty_quantification.html">Uncertainty Quantification</a></li><li><a class="toctext" href="dev_and_test.html">Algorithm Development and Testing</a></li></ul></li><li><span class="toctext">Domain Modeling Tools</span><ul><li><a class="toctext" href="../models/multiscale.html">Multi-Scale Models</a></li><li><a class="toctext" href="../models/physical.html">Physical Models</a></li><li><a class="toctext" href="../models/financial.html">Financial Models</a></li><li><a class="toctext" href="../models/biological.html">Chemical Reaction Models</a></li><li><a class="toctext" href="../models/external_modeling.html">External Modeling Packages</a></li></ul></li><li><span class="toctext">Extra Details</span><ul><li><a class="toctext" href="../extras/timestepping.html">Timestepping Method Descriptions</a></li></ul></li></ul></nav><article id="docs"><header><nav><ul><li>Analysis Tools</li><li><a href="sensitivity.html">Sensitivity Analysis</a></li></ul><a class="edit-page" href="https://github.com/JuliaDiffEq/DiffEqDocs.jl/blob/master/docs/src/analysis/sensitivity.md"><span class="fa">ï</span> Edit on GitHub</a></nav><hr/><div id="topbar"><span>Sensitivity Analysis</span><a class="fa fa-bars" href="#"></a></div></header><h1><a class="nav-anchor" href="#Sensitivity-Analysis-1" id="Sensitivity-Analysis-1">Sensitivity Analysis</a></h1><p>Sensitivity analysis for ODE models is provided by the DiffEq suite. The model sensitivities are defined as the derivatives of the solution with respect to the parameters. Sensitivity analysis serves two major purposes. On one hand, the sensitivities are diagnostics of the model which are useful for understand how it will change in accordance to changes in the parameters. But another use is simply because in many cases these derivatives are useful. Sensitivity analysis provides a cheap way to calculate the gradient of the solution which can be used in parameter estimation and other optimization tasks.</p><p>There are three types of sensitivity analysis. Local sensitivity analysis directly gives the gradient of the solution with respect to each parameter along the time series. The computational cost scales like <code>N*M</code>, where <code>N</code> is the number of states and <code>M</code> is the number of parameters. While this gives all of the information, it can be expensive for large models. Instead, adjoint sensitivity analysis solves directly for the gradient of some functional of the solution, such as a cost function or energy functional, in a much cheaper manner. Global Sensitivty Analysis methods are meant to be used for exploring the sensitivity over a larger domain without calculating  derivatives.  </p><h4><a class="nav-anchor" href="#Note-1" id="Note-1">Note</a></h4><p>Currently there are more performance optimizations needed to be done on the adjoint sensitivity method.</p><h2><a class="nav-anchor" href="#Local-Sensitivity-Analysis-1" id="Local-Sensitivity-Analysis-1">Local Sensitivity Analysis</a></h2><p>The local sensitivity of the solution to a parameter is defined by how much the solution would change by changes in the parameter, i.e. the sensitivity of the ith independent variable to the jth parameter is <span>$\frac{\partial y}{\partial p_{j}}$</span>.</p><p>The local sensitivity is computed using the sensitivity ODE:</p><div>\[\frac{d}{dt}\frac{\partial u}{\partial p_{j}}=\frac{\partial f}{\partial y}\frac{\partial y}{\partial p_{j}}+\frac{\partial f}{\partial p_{j}}=J\cdot S_{j}+F_{j}\]</div><p>where</p><div>\[J=\left(\begin{array}{cccc}
\frac{\partial f_{1}}{\partial y_{1}} &amp; \frac{\partial f_{1}}{\partial y_{2}} &amp; \cdots &amp; \frac{\partial f_{1}}{\partial y_{k}}\\
\frac{\partial f_{2}}{\partial y_{1}} &amp; \frac{\partial f_{2}}{\partial y_{2}} &amp; \cdots &amp; \frac{\partial f_{2}}{\partial y_{k}}\\
\cdots &amp; \cdots &amp; \cdots &amp; \cdots\\
\frac{\partial f_{k}}{\partial y_{1}} &amp; \frac{\partial f_{k}}{\partial y_{2}} &amp; \cdots &amp; \frac{\partial f_{k}}{\partial y_{k}}
\end{array}\right)\]</div><p>is the Jacobian of the system,</p><div>\[F_{j}=\left(\begin{array}{c}
\frac{\partial f_{1}}{\partial p_{j}}\\
\frac{\partial f_{2}}{\partial p_{j}}\\
\vdots\\
\frac{\partial f_{k}}{\partial p_{j}}
\end{array}\right)\]</div><p>are the parameter derivatives, and</p><div>\[S_{j}=\left(\begin{array}{c}
\frac{\partial y_{1}}{\partial p_{j}}\\
\frac{\partial y_{2}}{\partial p_{j}}\\
\vdots\\
\frac{\partial y_{k}}{\partial p_{j}}
\end{array}\right)\]</div><p>is the vector of sensitivities. Since this ODE is dependent on the values of the independent variables themselves, this ODE is computed simultaneously with the actual ODE system.</p><h3><a class="nav-anchor" href="#Example-solving-an-ODELocalSensitivityProblem-1" id="Example-solving-an-ODELocalSensitivityProblem-1">Example solving an ODELocalSensitivityProblem</a></h3><p>To define a sensitivity problem, simply use the <code>ODELocalSensitivityProblem</code> type instead of an ODE type. Note that this requires a <a href="https://github.com/JuliaDiffEq/ParameterizedFunctions.jl">ParameterizedFunction</a> with a Jacobian. For example, we generate an ODE with the sensitivity equations attached for the Lotka-Volterra equations by:</p><pre><code class="language-julia">f = @ode_def_nohes LotkaVolterraSensitivity begin
  dx = a*x - b*x*y
  dy = -c*y + x*y
end a b c

p = [1.5,1.0,3.0]
prob = ODELocalSensitivityProblem(f,[1.0;1.0],(0.0,10.0),p)</code></pre><p>This generates a problem which the ODE solvers can solve:</p><pre><code class="language-julia">sol = solve(prob,DP8())</code></pre><p>Note that the solution is the standard ODE system and the sensitivity system combined. We can use the following helper functions to extract the sensitivity information:</p><pre><code class="language-julia">x,dp = extract_local_sensitivities(sol)
x,dp = extract_local_sensitivities(sol,i)
x,dp = extract_local_sensitivities(sol,t)</code></pre><p>In each case, <code>x</code> is the ODE values and <code>dp</code> is the matrix of sensitivities where <code>dp[i]</code> is the gradient of component <code>i</code> by the parameters. The first gives the full timeseries of values. The second returns the <code>i</code>th values, while the third interpolates to calculate the sensitivities at time <code>t</code>. For example, if we do:</p><pre><code class="language-julia">x,dp = extract_local_sensitivities(sol)
da = dp[1]</code></pre><p>then <code>da</code> is the timeseries for <span>$\frac{\partial u(t)}{\partial p}$</span>. We can plot this</p><pre><code class="language-julia">plot(sol.t,da',lw=3)</code></pre><p>transposing so that the rows (the timeseries) is plotted.</p><p><img alt="Local Sensitivity Solution" src="../assets/sensitivityplot.png"/></p><p>Here we see that there is a periodicity to the sensitivity which matches the periodicity of the Lotka-Volterra solutions. However, as time goes on the sensitivity increases. This matches the analysis of Wilkins in Sensitivity Analysis for Oscillating Dynamical Systems.</p><p>We can also quickly see that these values are equivalent to those given by autodifferentiation and numerical differentiation through the ODE solver:</p><pre><code class="language-julia">using ForwardDiff, Calculus
function test_f(p)
  prob = ODEProblem(f,eltype(p).([1.0,1.0]),eltype(p).((0.0,10.0)),p)
  solve(prob,Vern9(),abstol=1e-14,reltol=1e-14,save_everystep=false)[end]
end

p = [1.5,1.0,3.0]
fd_res = ForwardDiff.jacobian(test_f,p)
calc_res = Calculus.finite_difference_jacobian(test_f,p)</code></pre><p>Here we just checked the derivative at the end point.</p><h4><a class="nav-anchor" href="#Internal-representation-1" id="Internal-representation-1">Internal representation</a></h4><p>For completeness, we detail the internal representation. Therefore, the solution to the ODE are the first <code>n</code> components of the solution. This means we can grab the matrix of solution values like:</p><pre><code class="language-julia">x = sol[1:sol.prob.indvars,:]</code></pre><p>Since each sensitivity is a vector of derivatives for each function, the sensitivities are each of size <code>sol.prob.indvars</code>. We can pull out the parameter sensitivities from the solution as follows:</p><pre><code class="language-julia">da = sol[sol.prob.indvars+1:sol.prob.indvars*2,:]
db = sol[sol.prob.indvars*2+1:sol.prob.indvars*3,:]
dc = sol[sol.prob.indvars*3+1:sol.prob.indvars*4,:]</code></pre><p>This means that <code>da[1,i]</code> is the derivative of the <code>x(t)</code> by the parameter <code>a</code> at time <code>sol.t[i]</code>. Note that all of the functionality available to ODE solutions is available in this case, including interpolations and plot recipes (the recipes will plot the expanded system).</p><h2><a class="nav-anchor" href="#Adjoint-Sensitivity-Analysis-1" id="Adjoint-Sensitivity-Analysis-1">Adjoint Sensitivity Analysis</a></h2><p>Adjoint sensitivity analysis is used to find the gradient of the solution with respect to some functional of the solution. In many cases this is used in an optimization problem to return the gradient with respect to some cost function.</p><p>The adjoint requires the definition of some scalar functional <span>$g(u,p,t)$</span> where <span>$u$</span> is the (numerical) solution to the differential equation. Adjoint sensitivity analysis finds the gradient of</p><div>\[G(u,p)=G(u(p))=\int_{0}^{T}g(u(t,p))dt\]</div><p>some integral of the solution. It does so by solving the adjoint problem</p><div>\[\frac{d\lambda^{\star}}{dt}=g_{u}(t)-\lambda^{\star}(t)f_{u}(t),\thinspace\thinspace\thinspace\lambda^{\star}=0\]</div><p>where <span>$f_u$</span> is the Jacobian of the system with respect to the state <code>u</code> while <span>$f_p$</span> is the Jacobian with respect to the parameters. The adjoint problem's solution gives the sensitivities through the integral:</p><div>\[\frac{dG}{dp}=-\int_{0}^{T}\lambda^{\star}(t)f_{p}(t)dt-\lambda^{T}(0)u_{p}(t)\]</div><p>Notice that since the adjoints require the Jacobian of the system at the state, it requires the ability to evaluate the state at any point in time. Thus it requires the continuous forward solution in order to solve the adjoint solution, and the adjoint solution is required to be continuous in order to calculate the resulting integral.</p><p>There is one extra detail to consider. In many cases we would like to calculate the adjoint sensitivity of some discontinuous functional of the solution. One canonical function is the L2 loss against some data points, that is:</p><div>\[L(u,p,t)=\sum_{i=1}^{n}\Vert\tilde{u}(t_{i})-u(t_{i},p)\Vert^{2}\]</div><p>In this case, we can reinterpret our summation as the distribution integral:</p><div>\[G(u,p)=\int_{0}^{T}\sum_{i=1}^{n}\Vert\tilde{u}(t_{i})-u(t_{i},p)\Vert^{2}\delta(t_{i}-t)dt\]</div><p>where <span>$Î´$</span> is the Dirac distribution. In this case, the integral is continuous except at finitely many points. Thus it can be calculated between each <span>$t_i$</span>. At a given <span>$t_i$</span>, given that the <span>$t_i$</span> are unique, we have that</p><div>\[g_{y}(t_{i})=2\left(\tilde{u}(t_{i})-u(t_{i},p)\right)\]</div><p>Thus the adjoint solution is given by integrating between the integrals and applying the jump function <span>$g_y$</span> at every data point.</p><h3><a class="nav-anchor" href="#Syntax-1" id="Syntax-1">Syntax</a></h3><p>There are two forms. For discrete adjoints, the form is:</p><pre><code class="language-julia">s = adjoint_sensitivities(sol,alg,dg,ts;kwargs...)</code></pre><p>where <code>alg</code> is the ODE algorithm to solve the adjoint problem, <code>dg</code> is the jump function, and <code>ts</code> is the time points for data. <code>dg</code> is given by:</p><pre><code class="language-julia">dg(out,u,p,t,i)</code></pre><p>which is the in-place gradient of the cost functional <code>g</code> at time point <code>ts[i]</code> with <code>u=u(t)</code>.</p><p>For continuous functionals, the form is:</p><pre><code class="language-julia">s = adjoint_sensitivities(sol,alg,g,nothing,dg;kwargs...)</code></pre><p>for the cost functional</p><pre><code class="language-julia">g(u,p,t)</code></pre><p>with in-place gradient</p><pre><code class="language-julia">dg(out,u,p,t)</code></pre><p>Currently, the gradient is required. Note that the keyword arguments are passed to the internal ODE solver for solving the adjoint problem. Two special keyword arguments are <code>iabstol</code> and <code>ireltol</code> which are the tolerances for the internal quadrature via QuadGK for the resulting functional.</p><h3><a class="nav-anchor" href="#Example-discrete-adjoints-on-a-cost-function-1" id="Example-discrete-adjoints-on-a-cost-function-1">Example discrete adjoints on a cost function</a></h3><p>In this example we will show solving for the adjoint sensitivities of a discrete cost functional. First let's solve the ODE and get a high quality continuous solution:</p><pre><code class="language-julia">f = @ode_def_nohes LotkaVolterra begin
  dx = a*x - b*x*y
  dy = -c*y + x*y
end a b c

p = [1.5,1.0,3.0]
prob = ODEProblem(f,[1.0;1.0],(0.0,10.0),p)
sol = solve(prob,Vern9(),abstol=1e-10,reltol=1e-10)</code></pre><p>Now let's calculate the sensitivity of the L2 error against 1 at evenly spaced points in time, that is:</p><div>\[L(u,p,t)=\sum_{i=1}^{n}\frac{\Vert1-u(t_{i},p)\Vert^{2}}{2}\]</div><p>for <span>$t_i = 0.5i$</span>. This is the assumption that the data is <code>data[i]=1.0</code>. For this function, notice we have that:</p><div>\[\begin{align}
dg_{1}&amp;=1-u_{1} \\
dg_{2}&amp;=1-u_{2}
\end{align}\]</div><p>and thus:</p><pre><code class="language-julia">dg(out,u,i) = (out.=1.0.-u)</code></pre><p>If we had data, we'd just replace <code>1.0</code> with <code>data[i]</code>. To get the adjoint sensitivities, call:</p><pre><code class="language-julia">res = adjoint_sensitivities(sol,Vern9(),dg,t,abstol=1e-14,
                            reltol=1e-14,iabstol=1e-14,ireltol=1e-12)</code></pre><p>This is super high accuracy. As always, there's a tradeoff between accuracy and computation time. We can check this almost exactly matches the autodifferentiation and numerical differentiation results:</p><pre><code class="language-julia">using ForwardDiff,Calculus
function G(p)
  tmp_prob = problem_new_parameters(prob,p)
  sol = solve(tmp_prob,Vern9(),abstol=1e-14,reltol=1e-14,saveat=t)
  A = convert(Array,sol)
  sum(((1-A).^2)./2)
end
G([1.5,1.0,3.0])
res2 = ForwardDiff.gradient(G,[1.5,1.0,3.0])
res3 = Calculus.gradient(G,[1.5,1.0,3.0])</code></pre><p>and see this gives the same values.</p><h3><a class="nav-anchor" href="#Example-continuous-adjoints-on-an-energy-functional-1" id="Example-continuous-adjoints-on-an-energy-functional-1">Example continuous adjoints on an energy functional</a></h3><p>In this case we'd like to calculate the adjoint sensitivity of the scalar energy functional</p><div>\[G(u,p)=\int_{0}^{T}\frac{\sum_{i=1}^{n}u_{i}^{2}(t)}{2}dt\]</div><p>which is</p><pre><code class="language-julia">g(u,p,t) = (sum(u).^2) ./ 2</code></pre><p>Notice that the gradient of this function with respect to the state <code>u</code> is:</p><pre><code class="language-julia">function dg(out,u,p,t)
  out[1]= u[1] + u[2]
  out[2]= u[1] + u[2]
end</code></pre><p>To get the adjoint sensitivities, we call:</p><pre><code class="language-julia">res = adjoint_sensitivities(sol,Vern9(),g,nothing,dg,abstol=1e-8,
                                 reltol=1e-8,iabstol=1e-8,ireltol=1e-8)</code></pre><p>Notice that we can check this against autodifferentiation and numerical differentiation as follows:</p><pre><code class="language-julia">function G(p)
  tmp_prob = problem_new_parameters(prob,p)
  sol = solve(tmp_prob,Vern9(),abstol=1e-14,reltol=1e-14)
  res,err = quadgk((t)-&gt; (sum(sol(t)).^2)./2,0.0,10.0,abstol=1e-14,reltol=1e-10)
  res
end
res2 = ForwardDiff.gradient(G,[1.5,1.0,3.0])
res3 = Calculus.gradient(G,[1.5,1.0,3.0])</code></pre><h2><a class="nav-anchor" href="#Global-Sensitivity-Analysis-1" id="Global-Sensitivity-Analysis-1">Global Sensitivity Analysis</a></h2><p>Global Sensitivity Analysis methods are used to quantify the uncertainity in  output of a model w.r.t. the parameters, their individual contributions or the  contribution of their interactions. The type of GSA method to use depends on  the interest of the user, below we describe the methods available in the suite at the moment (some more are already in development) and explain what is  the output of each of the methods and what it represents. </p><h3><a class="nav-anchor" href="#Morris-Method-1" id="Morris-Method-1">Morris Method</a></h3><p>The Morris method also known as Morrisâs OAT method where OAT stands for  One At a Time can be described in the following steps:</p><p>We calculate local sensitivity measures known as âelementary effectsâ,  which are calculated by measuring the perturbation in the output of the  model on changing one parameter. </p>$<p>EE_i = \frac{f(x_1,x_2,..x_i+ \Delta,..x_k) - y}{\Delta} $</p><p>These are evaluated at various points in the input chosen such that a wide  âspreadâ of the parameter space is explored and considered in the analysis,  to provide an approximate global importance measure. The mean and variance of  these elementary effects is computed. A high value of the mean implies that  a parameter is important, a high variance implies that its effects are  non-linear or the result of interactions with other inputs. This method  does not evaluate separately the contribution from the  interaction and the contribution of the parameters individually and gives the  effects for each parameter which takes into cpnsideration all the interactions and its individual contribution. </p><p><code>morris_effects = morris_sensitivity(f,param_range,param_steps;relative_scale=false,kwargs...)</code></p><p><code>morris_effects = morris_sensitivity(prob::DiffEqBase.DEProblem,alg,t,param_range,param_steps;kwargs...)</code></p><p>Here, <code>f</code> is just the model (as a julia function or a <code>DEProblem</code>) you want to  run the analysis on, <code>param_range</code> requires an array of 2-tuples with the lower bound  and the upper bound, <code>param_steps</code> decides the value of \Delta in the equation  above and <code>relative_scale</code>, the above equation takes the assumption that  the parameters lie in the range <code>[0,1]</code> but as this is not always the case  scaling is used to get more informative, scaled effects.</p><h3><a class="nav-anchor" href="#Sobol-Method-1" id="Sobol-Method-1">Sobol Method</a></h3><p>Sobol is a variance-based method and it decomposes the variance of the output of  the model or system into fractions which can be attributed to inputs or sets  of inputs. This helps to get not just the individual parameter's sensitivities  but also gives a way to quantify the affect and sensitivity from  the interaction between the parameters. </p>$<p>Y = f_0+ \sum_{i=1}^d f_i(X_i)+ \sum_{i &lt; j}^d f_{ij}(X_i,X_j) ... + f_{1,2...d}(X_1,X_2,..X_d) $</p>$<p>Var(Y) = \sum_{i=1}^d V_i + \sum_{i &lt; j}^d V_{ij} + ... + V_{1,2...,d} $</p><p>The Sobol Indices are "order"ed, the first order indices given by <span>$S_i = \frac{V_i}{Var(Y)}$</span>  the contribution to the output variance of the main effect of $ X_i $, therefore it  measures the effect of varying $ X_i $ alone, but averaged over variations  in other input parameters. It is standardised by the total variance to provide a fractional contribution.  Higher-order interaction indices $ S_{i,j}, S_{i,j,k} $ and so on can be formed  by dividing other terms in the variance decomposition by $ Var(Y) $.</p><p><code>sobol_second_order = sobol_sensitivity(f,param_range,N,order=2)</code></p><p><code>sobol_second_order = sobol_sensitivity(prob::DiffEqBase.DEProblem,alg,t,param_range,N,order=2)</code></p><p>Here <code>f</code> and <code>param_range</code> are the same as Morris's, providing a uniform interface.</p><h3><a class="nav-anchor" href="#Regression-Method-1" id="Regression-Method-1">Regression Method</a></h3><p>If a sample of inputs and outputs $ (X^n, Y^n) = ô°(X^{i}_1, . . . , X^{i}_d, Y_i)_{i=1..n} $ô°  is available, it is possible to fit a linear model explaining the behaviour of Y given the  values of X, provided that the sample size n is sufficiently large (at least n &gt; d).</p><p>The measures provided for this analysis by us in DiffEqSensitivity.jl are</p><p>a) Pearson Correlation Coefficient:</p>$<p>r = \frac{\sum_{i=1}^{n} (x_i - \overline{x})(y_i - \overline{y})} {\sqrt{\sum_{i=1}^{n} (x_i - \overline{x})^2(y_i - \overline{y})^2}} $</p><p>b) Standard Regression Coefficient (SRC):</p>$<p>SRC_j = \beta_{j} \sqrt{\frac{Var(X_j)}{Var(Y)}} $</p><p>where $ \beta_j $ is the linear regression coefficient associated to <span>$X_j$</span>.</p><p>c) Partial Correlation Coefficient (PCC):</p>$<p>PCC_j = \rho(X_j - \hat{X_{-j}},Y_j - \hat{Y_{-j}}) $</p><p>where $ \hat{X_{-j}} $ô° is the prediction of the linear model, expressing $ X_{j} $    with respect to the other inputs and $ \hat{Yô°_{-j}} $ is the prediction of the    linear model where $ X_j $ is absent. PCC measures the sensitivity of $ Y $ to    $ X_j $ when the effects of the other inputs have been canceled.</p><p><code>regre_sensitivity = regression_sensitivity(f,param_range,param_fixed,n;coeffs=:rank)</code></p><p><code>regre_sensitivity = regression_sensitivity(prob::DiffEqBase.DEProblem,alg,t,param_range,param_fixed,n;coeffs=:rank)</code></p><p>Again, <code>f</code> and <code>param_range</code> are the same as above. An array of the true parameter values  that lie within the <code>param_range</code> bounds are passed through the <code>param_fixed</code> argument.  <code>n</code> determines the number of simulations of the model run to generate the data points  of the solution and parameter values and the <code>coeffs</code> kwarg lets you decide the coefficients you want.</p><h3><a class="nav-anchor" href="#GSA-example-1" id="GSA-example-1">GSA example</a></h3><p>Let's create the ODE problem to run our GSA on.</p><pre><code class="language-julia">f = @ode_def_nohes LotkaVolterraTest begin
    dx = a*x - b*x*y
    dy = -3*y + x*y
end a b 
u0 = [1.0;1.0]
tspan = (0.0,10.0)
p = [1.5,1.0]
prob = ODEProblem(f,u0,tspan,p)
t = collect(range(0, stop=10, length=200))</code></pre><p>For Morris Method</p><pre><code class="language-julia">m = DiffEqSensitivity.morris_sensitivity(prob,Tsit5(),t,[[1,5],[0.5,5]],[10,10],len_trajectory=1500,total_num_trajectory=1000,num_trajectory=150)</code></pre><p>Let's get the means and variances from the <code>MorrisSensitivity</code> struct.</p><pre><code class="language-julia">m.means

Out[9]: 2-element Array{Array{Float64,2},1}:
 [0.0 0.0513678 â¦ 7.91336 7.93783; 0.0 0.00115769 â¦ 3.66156 3.67284]
 [0.0 0.0488899 â¦ 2.50728 2.359; 0.0 0.00112006 â¦ 2.23431 2.44946]  

m.variances

Out[10]: 2-element Array{Array{Float64,2},1}:
 [0.0 1.94672e-5 â¦ 26.4223 24.8513; 0.0 4.81347e-9 â¦ 37.4061 30.3068]
 [0.0 1.77615e-5 â¦ 17.9555 14.9231; 0.0 4.47931e-9 â¦ 48.074 51.9312] </code></pre><p>This gives the means of the effects and it's variances over the entire timespan and thus we get 200-length  arrays for each paramter and dependent variable pair.</p><p>We can plot the trajectory of the sensitivity with the standard deviation bars.</p><pre><code class="language-julia"># For the first parameter (a)
stdv1 = sqrt.(m.variances[1])
p = plot(m.means[1]', yerror=stdv1)</code></pre><p><img alt="morrisparameter1" src="../assets/morris1.png"/></p><pre><code class="language-julia"># For the second parameter (b)
stdv2 = sqrt.(m.variances[2])
p = plot(m.means[2]', yerror=stdv2)</code></pre><p><img alt="morrisparameter2" src="../assets/morris2.png"/></p><p>For Sobol Method</p><pre><code class="language-julia">
s0 = sobol_sensitivity(prob,Tsit5(),t,[[1,5],[0.5,5]],N,0)
Out[8]: 2-element Array{Array{Float64,2},1}:
 [NaN 0.507831 â¦ 1.00731 1.00436; NaN 1.92336 â¦ 0.732384 0.730945]  
 [NaN 0.47214 â¦ 0.676224 0.681525; NaN -1.68656 â¦ 0.879557 0.877603]

s1 = sobol_sensitivity(prob,Tsit5(),t,[[1,5],[0.5,5]],N,1)
Out[9]: 2-element Array{Array{Float64,2},1}:
 [NaN 0.39537 â¦ 0.341697 0.343645; NaN -2.06101 â¦ 0.10922 0.106976]     
 [NaN 0.652815 â¦ 0.00910675 0.00815206; NaN 5.24832 â¦ 0.296978 0.296639]

s2 = sobol_sensitivity(prob,Tsit5(),t,[[1,5],[0.5,5]],N,2)
Out[10]: 1-element Array{Array{Float64,2},1}:
 [NaN -0.0596478 â¦ 0.652303 0.657847; NaN -1.84504 â¦ 0.645139 0.620036]</code></pre><p>We can decide which order of Sobol Indices we are interested in my passing an argument for it,  by default it gives the second order indices. Again the result is obtained over the entire <code>timespan</code></p><p>We plot the first order and total order Sobol Indices for some timepoints for each of the parameters (<code>a</code> and <code>b</code>).</p><p><span>$julia p1 = bar(["a","b"],[s0[1][end-2],s0[2][end-2]],color=[:red,:blue],title="Total Order Indices at t=9.949748743718592",legend=false) p2 = bar(["a","b"],[s1[1][end-2],s1[2][end-2]],color=[:red,:blue],title="First Order Indices at t=9.949748743718592",legend=false) p3 = bar(["a","b"],[s0[1][3],s0[2][3]],color=[:red,:blue],title="Total Order Indices at t=0.05025125628140704",legend=false) p4 = bar(["a","b"],[s1[1][3],s1[2][3]],color=[:red,:blue],title="First Order Indices at t=0.05025125628140704",legend=false) plo = plot(p1,p2,p3,p4,layout=(4,1),size=(600,500))$</span>` <img alt="sobolplot" src="../assets/sobolbars.png"/></p><p>Here we plot the Sobol indices of first order and the total Sobol indices for the parameters <code>a</code> and <code>b</code>. The plots are obtained by getting the Sobol Indices at the <code>t = 9.949748743718592</code> and the <code>t = 0.05025125628140704</code> time point of the first dependent variable <code>x(t)</code> from the 200-length sensitivities over the entire time span. The length of the bar represents the quantification of the sensitivity of the output to that parameter and hence for the 199th time point you can say that <code>x(t)</code> is more sensitive to <code>b</code>, also you can observe how the relative difference between <code>a</code> and <code>b</code> is larger in the first order than the total order indices, this tells us that most of the contribution of <code>a</code> to <code>x(t)</code> arises from interactions and it's individual non-interaction contribution is significantly lesser than <code>b</code> and vice-versa for <code>b</code> as it's first order plot indicates quite high value.</p><footer><hr/><a class="previous" href="bifurcation.html"><span class="direction">Previous</span><span class="title">Bifurcation Analysis</span></a><a class="next" href="uncertainty_quantification.html"><span class="direction">Next</span><span class="title">Uncertainty Quantification</span></a></footer></article></body></HTML>