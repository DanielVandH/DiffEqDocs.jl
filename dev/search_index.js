var documenterSearchIndex = {"docs":
[{"location":"types/dynamical_types/#dynamical_prob","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"","category":"section"},{"location":"types/dynamical_types/","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"Dynamical ordinary differential equations, such as those arising from the definition of a Hamiltonian system or a second order ODE, have a special structure that can be utilized in the solution of the differential equation. On this page we describe how to define second order differential equations for their efficient numerical solution.","category":"page"},{"location":"types/dynamical_types/#Mathematical-Specification-of-a-Dynamical-ODE-Problem","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Mathematical Specification of a Dynamical ODE Problem","text":"","category":"section"},{"location":"types/dynamical_types/","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"These algorithms require a Partitioned ODE of the form:","category":"page"},{"location":"types/dynamical_types/","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"fracdvdt = f_1(ut) \nfracdudt = f_2(v) ","category":"page"},{"location":"types/dynamical_types/","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"This is a Partitioned ODE partitioned into two groups, so the functions should be specified as f1(dv,v,u,p,t) and f2(du,v,u,p,t) (in the inplace form), where f1 is independent of v (unless specified by the solver), and f2 is independent of u and t. This includes discretizations arising from SecondOrderODEProblems where the velocity is not used in the acceleration function, and Hamiltonians where the potential is (or can be) time-dependent but the kinetic energy is only dependent on v.","category":"page"},{"location":"types/dynamical_types/","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"Note that some methods assume that the integral of f2 is a quadratic form. That means that f2=v'*M*v, i.e. int f_2 = frac12 m v^2, giving du = v. This is equivalent to saying that the kinetic energy is related to v^2. The methods which require this assumption will lose accuracy if this assumption is violated. Methods listed make note of this requirement with \"Requires quadratic kinetic energy\".","category":"page"},{"location":"types/dynamical_types/#Constructor","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Constructor","text":"","category":"section"},{"location":"types/dynamical_types/","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"DynamicalODEProblem(f::DynamicalODEFunction,v0,u0,tspan,p=NullParameters();kwargs...)\nDynamicalODEProblem{isinplace}(f1,f2,v0,u0,tspan,p=NullParameters();kwargs...)","category":"page"},{"location":"types/dynamical_types/","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"Defines the ODE with the specified functions. isinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred.","category":"page"},{"location":"types/dynamical_types/","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"Parameters are optional, and if not given then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.","category":"page"},{"location":"types/dynamical_types/#Fields","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Fields","text":"","category":"section"},{"location":"types/dynamical_types/","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"f1 and f2: The functions in the ODE.\nv0 and u0: The initial conditions.\ntspan: The timespan for the problem.\np: The parameters for the problem. Defaults to NullParameters\nkwargs: The keyword arguments passed onto the solves.","category":"page"},{"location":"types/dynamical_types/#Mathematical-Specification-of-a-2nd-Order-ODE-Problem","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Mathematical Specification of a 2nd Order ODE Problem","text":"","category":"section"},{"location":"types/dynamical_types/","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"To define a 2nd Order ODE Problem, you simply need to give the function f and the initial condition u_0 which define an ODE:","category":"page"},{"location":"types/dynamical_types/","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"u = f(uupt)","category":"page"},{"location":"types/dynamical_types/","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"f should be specified as f(du,u,p,t) (or in-place as f(ddu,du,u,p,t)), and u₀ should be an AbstractArray (or number) whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher dimension tensors as well.","category":"page"},{"location":"types/dynamical_types/","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"From this form, a dynamical ODE:","category":"page"},{"location":"types/dynamical_types/","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"v = f(vupt) \nu = v ","category":"page"},{"location":"types/dynamical_types/","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"is generated.","category":"page"},{"location":"types/dynamical_types/#Constructors","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Constructors","text":"","category":"section"},{"location":"types/dynamical_types/","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"SecondOrderODEProblem{isinplace}(f,du0,u0,tspan,callback=CallbackSet())","category":"page"},{"location":"types/dynamical_types/","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"Defines the ODE with the specified functions.","category":"page"},{"location":"types/dynamical_types/#Fields-2","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Fields","text":"","category":"section"},{"location":"types/dynamical_types/","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"f: The function for the second derivative.\ndu0: The initial derivative.\nu0: The initial condition.\ntspan: The timespan for the problem.\ncallback: A callback to be applied to every solver which uses the problem. Defaults to nothing.","category":"page"},{"location":"types/dynamical_types/#Hamiltonian-Problems","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Hamiltonian Problems","text":"","category":"section"},{"location":"types/dynamical_types/","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"HamiltonianProblems are provided by DiffEqPhysics.jl and provide an easy way to define equations of motion from the corresponding Hamiltonian. To define a HamiltonianProblem one only needs to specify the Hamiltonian:","category":"page"},{"location":"types/dynamical_types/","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"H(pq)","category":"page"},{"location":"types/dynamical_types/","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"and autodifferentiation (via ForwardDiff.jl) will create the appropriate equations.","category":"page"},{"location":"types/dynamical_types/#Constructors-2","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Constructors","text":"","category":"section"},{"location":"types/dynamical_types/","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"HamiltonianProblem{T}(H,p0,q0,tspan,param=nothing;kwargs...)","category":"page"},{"location":"types/dynamical_types/#Fields-3","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Fields","text":"","category":"section"},{"location":"types/dynamical_types/","page":"Dynamical, Hamiltonian and 2nd Order ODE Problems","title":"Dynamical, Hamiltonian and 2nd Order ODE Problems","text":"H: The Hamiltonian H(p,q,params) which returns a scalar.\np0: The initial momentums.\nq0: The initial positions.\ntspan: The timespan for the problem.\nparam: Defaults to nothing. param will be passed to H's params. ","category":"page"},{"location":"models/external_modeling/#External-Modeling-Packages","page":"External Modeling Packages","title":"External Modeling Packages","text":"","category":"section"},{"location":"models/external_modeling/","page":"External Modeling Packages","title":"External Modeling Packages","text":"This is a list of modeling packages built upon the JuliaDiffEq ecosystem.","category":"page"},{"location":"models/external_modeling/#DynamicalSystems.jl","page":"External Modeling Packages","title":"DynamicalSystems.jl","text":"","category":"section"},{"location":"models/external_modeling/","page":"External Modeling Packages","title":"External Modeling Packages","text":"DynamicalSystems.jl is a package for the exploration of continuous and discrete dynamical systems, with focus on nonlinear dynamics and chaos.","category":"page"},{"location":"models/external_modeling/","page":"External Modeling Packages","title":"External Modeling Packages","text":"It uses DifferentialEquations.jl for all evolution regarding continuous systems while still retaining unified interface for discrete systems.","category":"page"},{"location":"models/external_modeling/","page":"External Modeling Packages","title":"External Modeling Packages","text":"A quick summary of features: Lyapunov exponents, generalized entropies (Renyi entropy), generalized & fractal dimensions, delay coordinates embedding (reconstruction), chaos detection, Lyapunov exponents of a numerical timeseries, finding periodic orbits of any order for maps.","category":"page"},{"location":"models/external_modeling/#BioEnergeticFoodWebs.jl","page":"External Modeling Packages","title":"BioEnergeticFoodWebs.jl","text":"","category":"section"},{"location":"models/external_modeling/","page":"External Modeling Packages","title":"External Modeling Packages","text":"BioEnergeticFoodWebs.jl is a package for simulations of biomass flows in food webs.","category":"page"},{"location":"models/external_modeling/#SwitchTimeOpt.jl","page":"External Modeling Packages","title":"SwitchTimeOpt.jl","text":"","category":"section"},{"location":"models/external_modeling/","page":"External Modeling Packages","title":"External Modeling Packages","text":"SwitchTimeOpt.jl is a Julia package to easily define and efficiently solve switching time optimization (STO) problems for linear and nonlinear systems.","category":"page"},{"location":"models/external_modeling/#VehicleModels.jl","page":"External Modeling Packages","title":"VehicleModels.jl","text":"","category":"section"},{"location":"models/external_modeling/","page":"External Modeling Packages","title":"External Modeling Packages","text":"VehicleModels.jl is a package for simulating vehicle models.","category":"page"},{"location":"models/external_modeling/#MADS.jl","page":"External Modeling Packages","title":"MADS.jl","text":"","category":"section"},{"location":"models/external_modeling/","page":"External Modeling Packages","title":"External Modeling Packages","text":"MADS.jl is a package data and model analysis. It adds many sensitivity analysis, uncertainty quantification, and model selection routines.","category":"page"},{"location":"models/external_modeling/#QuantumOptics.jl","page":"External Modeling Packages","title":"QuantumOptics.jl","text":"","category":"section"},{"location":"models/external_modeling/","page":"External Modeling Packages","title":"External Modeling Packages","text":"QunatumOptics.jl is a package for simulation of quantum systems.","category":"page"},{"location":"solvers/bvp_solve/#BVP-Solvers","page":"BVP Solvers","title":"BVP Solvers","text":"","category":"section"},{"location":"solvers/bvp_solve/#Recomended-Methods","page":"BVP Solvers","title":"Recomended Methods","text":"","category":"section"},{"location":"solvers/bvp_solve/","page":"BVP Solvers","title":"BVP Solvers","text":"GeneralMIRK4 is a good well-rounded method when the problem is not too large. It uses highly stable trust region methods to solve a 4th order fully implicit Runge-Kutta scheme. As an alternative on general BVProblems, the Shooting method paired with an appropriate integrator for the IVP, such as Shooting(Tsit5()), is a flexible and efficient option. This will allow one to combine callbacks/event handling with the BVP solver, and the high order interpolations can be used to define complex boundary conditions. However, Shooting methods can in some cases be prone to sensitivity of the boundary condition.","category":"page"},{"location":"solvers/bvp_solve/","page":"BVP Solvers","title":"BVP Solvers","text":"When the problem is a large two-point boundary value problem that is sensitive to the boundary conditions, MIRK4 utilizes a sparse Jacobian to greatly improve the efficiency.","category":"page"},{"location":"solvers/bvp_solve/#Full-List-of-Methods","page":"BVP Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"solvers/bvp_solve/#BoundaryValueDiffEq.jl","page":"BVP Solvers","title":"BoundaryValueDiffEq.jl","text":"","category":"section"},{"location":"solvers/bvp_solve/","page":"BVP Solvers","title":"BVP Solvers","text":"Shooting - A wrapper over initial value problem solvers.\nGeneralMIRK4 - A 4th order collocation method using an implicit Runge-Kutta tableau solved using a trust region dogleg method from NLsolve.jl.\nMIRK4 - A 4th order collocation method using an implicit Runge-Kutta tableau with a sparse Jacobian. Compatible only with two-point boundary value problems.","category":"page"},{"location":"tutorials/bvp_example/#Boundary-Value-Problems","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"","category":"section"},{"location":"tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"This tutorial will introduce you to the functionality for solving BVPs. Other introductions can be found by checking out DiffEqTutorials.jl. ","category":"page"},{"location":"tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"note: Note\nThis tutorial assumes you have read the Ordinary Differential Equations tutorial.","category":"page"},{"location":"tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"In this example we will solve the ODE that satisfies the boundary condition in the form of","category":"page"},{"location":"tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"beginaligned\nfracdudt = f(t u) \ng(u) = vec0\nendaligned","category":"page"},{"location":"tutorials/bvp_example/#Example-1:-Simple-Pendulum","page":"Boundary Value Problems","title":"Example 1: Simple Pendulum","text":"","category":"section"},{"location":"tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"The concrete example that we are solving is the simple pendulum ddotu+fracgLsin(u)=0 on the time interval tin0fracpi2. First, we need to define the ODE","category":"page"},{"location":"tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"using BoundaryValueDiffEq\nconst g = 9.81\nL = 1.0\ntspan = (0.0,pi/2)\nfunction simplependulum!(du,u,p,t)\n    θ  = u[1]\n    dθ = u[2]\n    du[1] = dθ\n    du[2] = -(g/L)*sin(θ)\nend","category":"page"},{"location":"tutorials/bvp_example/#Boundary-Condition","page":"Boundary Value Problems","title":"Boundary Condition","text":"","category":"section"},{"location":"tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"There are two problem types available:","category":"page"},{"location":"tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"A problem type for general boundary conditions BVProblem ( including conditions that may be anywhere/ everywhere on the integration interval ).\nA problem type for boundaries that are specified at the beginning and the end of the integration interval TwoPointBVProblem","category":"page"},{"location":"tutorials/bvp_example/#BVProblem","page":"Boundary Value Problems","title":"BVProblem","text":"","category":"section"},{"location":"tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"The boundary conditions are specified by a function that calculates the residual in-place from the problem solution, such that the residual is vec0 when the boundary condition is satisfied.","category":"page"},{"location":"tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"function bc1!(residual, u, p, t)\n    residual[1] = u[end÷2][1] + pi/2 # the solution at the middle of the time span should be -pi/2\n    residual[2] = u[end][1] - pi/2 # the solution at the end of the time span should be pi/2\nend\nbvp1 = BVProblem(simplependulum!, bc1!, [pi/2,pi/2], tspan)\nsol1 = solve(bvp1, GeneralMIRK4(), dt=0.05)\nplot(sol1)","category":"page"},{"location":"tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"(Image: BVP Example Plot1)","category":"page"},{"location":"tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"The third argument of BVProblem  is the initial guess of the solution, which is constant in this example. <!– add examples of more general initial conditions –> We need to use GeneralMIRK4 or Shooting methods to solve BVProblem. GeneralMIRK4 is a collocation method, whereas Shooting treats the problem as an IVP and varies the initial conditions until the boundary conditions are met. If you can have a good initial guess, Shooting method works very well.","category":"page"},{"location":"tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"using OrdinaryDiffEq\nu₀_2 = [-1.6, -1.7] # the initial guess\nfunction bc3!(residual, sol, p, t)\n    residual[1] = sol(pi/4)[1] + pi/2 # use the interpolation here, since indexing will be wrong for adaptive methods\n    residual[2] = sol(pi/2)[1] - pi/2\nend\nbvp3 = BVProblem(simplependulum!, bc3!, u₀_2, tspan)\nsol3 = solve(bvp3, Shooting(Vern7()))","category":"page"},{"location":"tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"The initial guess can also be supplied via a function of t or a previous solution type, this is espacially handy for parameter analysis. We changed u to sol to emphasize the fact that in this case the boundary condition can be written on the solution object. Thus all of the features on the solution type such as interpolations are available when using the Shooting method (i.e. you can have a boundary condition saying that the maximum over the interval is 1 using an optimization function on the continuous output). Note that user has to import the IVP solver before it can be used. Any common interface ODE solver is acceptable.","category":"page"},{"location":"tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"plot(sol3)","category":"page"},{"location":"tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"(Image: BVP Example Plot3)","category":"page"},{"location":"tutorials/bvp_example/#TwoPointBVProblem","page":"Boundary Value Problems","title":"TwoPointBVProblem","text":"","category":"section"},{"location":"tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"Defining a similar problem as TwoPointBVProblem is shown in the following example. At the moment MIRK4 is the only solver for TwoPointBVProblems.","category":"page"},{"location":"tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"function bc2!(residual, u, p, t) # u[1] is the beginning of the time span, and u[end] is the ending\n    residual[1] = u[1][1] + pi/2 # the solution at the beginning of the time span should be -pi/2\n    residual[2] = u[end][1] - pi/2 # the solution at the end of the time span should be pi/2\nend\nbvp2 = TwoPointBVProblem(simplependulum!, bc2!, [pi/2,pi/2], tspan)\nsol2 = solve(bvp2, MIRK4(), dt=0.05) # we need to use the MIRK4 solver for TwoPointBVProblem\nplot(sol2)","category":"page"},{"location":"tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"Note that u is a tuple of ( u[1], u[end] ) just like t is ( t[1], t[end] ) and p holds the parameters of the given problem.","category":"page"},{"location":"tutorials/bvp_example/","page":"Boundary Value Problems","title":"Boundary Value Problems","text":"(Image: BVP Example Plot2)","category":"page"},{"location":"analysis/global_sensitivity/#gsa","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"","category":"section"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"Global Sensitivity Analysis (GSA) methods are used to quantify the uncertainty in output of a model w.r.t. the parameters, their individual contributions, or the contribution of their interactions. The GSA interface allows for utilizing batched functions for parallel computation of GSA quantities.","category":"page"},{"location":"analysis/global_sensitivity/#Installation","page":"Global Sensitivity Analysis","title":"Installation","text":"","category":"section"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"This functionality does not come standard with DifferentialEquations.jl. To use this functionality, you must install DiffEqSensitivity.jl:","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"]add DiffEqSensitivity\nusing DiffEqSensitivity","category":"page"},{"location":"analysis/global_sensitivity/#General-Interface","page":"Global Sensitivity Analysis","title":"General Interface","text":"","category":"section"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"The general interface for calling a global sensitivity analysis is either:","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"effects = gsa(f, method, param_range; N, batch=false)","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"where:","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"y=f(x) is a function that takes in a single vector and spits out a single vector or scalar. If batch=true, then f takes in a matrix where each row is a set of parameters, and returns a matrix where each row is a the output for the corresponding row of parameters.\nmethod is one of the GSA methods below.\nparam_range is a vector of tuples for the upper and lower bound for the given parameter i.\nN is a required keyword argument for the number of samples to take in the trajectories/design.","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"Note that for some methods there is a second interface where one can directly pass the design matrices:","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"effects = gsa(f, method, A, B; batch=false)","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"where A and B are design matrices with each row being a set of parameters. Note that generate_design_matrices from QuasiMonteCarlo.jl can be used to generate the design matrices.","category":"page"},{"location":"analysis/global_sensitivity/#Morris-Method","page":"Global Sensitivity Analysis","title":"Morris Method","text":"","category":"section"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"Morris has the following keyword arguments:","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"p_steps - Vector of Delta for the step sizes in each direction. Required.\nrelative_scale - The elementary effects are calculated with the assumption that the parameters lie in the range [0,1] but as this is not always the case scaling is used to get more informative, scaled effects. Defaults to false.\ntotal_num_trajectory, num_trajectory - The total number of design matrices that are generated out of which num_trajectory matrices with the highest spread are used in calculation.\nlendesignmat` - The size of a design matrix.","category":"page"},{"location":"analysis/global_sensitivity/#Morris-Method-Details","page":"Global Sensitivity Analysis","title":"Morris Method Details","text":"","category":"section"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"The Morris method also known as Morris’s OAT method where OAT stands for One At a Time can be described in the following steps:","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"We calculate local sensitivity measures known as “elementary effects”, which are calculated by measuring the perturbation in the output of the model on changing one parameter.","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"EE_i = fracf(x_1x_2x_i+ Deltax_k) - yDelta","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"These are evaluated at various points in the input chosen such that a wide “spread” of the parameter space is explored and considered in the analysis, to provide an approximate global importance measure. The mean and variance of these elementary effects is computed. A high value of the mean implies that a parameter is important, a high variance implies that its effects are non-linear or the result of interactions with other inputs. This method does not evaluate separately the contribution from the interaction and the contribution of the parameters individually and gives the effects for each parameter which takes into consideration all the interactions and its individual contribution.","category":"page"},{"location":"analysis/global_sensitivity/#Sobol-Method","page":"Global Sensitivity Analysis","title":"Sobol Method","text":"","category":"section"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"The Sobol object has as its fields the order of the indices to be estimated.","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"order - the order of the indices to calculate. Defaults to [0,1], which means the Total and First order indices. Passing 2 enables calculation of the Second order indices as well.\nEi_estimator - Can take :Homma1996, :Sobol2007 and :Jansen1999 for which Monte Carlo estimator is used for the Ei term. Defaults to :Jansen1999.","category":"page"},{"location":"analysis/global_sensitivity/#Sobol-Method-Details","page":"Global Sensitivity Analysis","title":"Sobol Method Details","text":"","category":"section"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"Sobol is a variance-based method and it decomposes the variance of the output of the model or system into fractions which can be attributed to inputs or sets of inputs. This helps to get not just the individual parameter's sensitivities but also gives a way to quantify the affect and sensitivity from the interaction between the parameters.","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":" Y = f_0+ sum_i=1^d f_i(X_i)+ sum_i  j^d f_ij(X_iX_j)  + f_12d(X_1X_2X_d)","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":" Var(Y) = sum_i=1^d V_i + sum_i  j^d V_ij +  + V_12d","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"The Sobol Indices are \"order\"ed, the first order indices given by S_i = fracV_iVar(Y) the contribution to the output variance of the main effect of X_i, therefore it measures the effect of varying X_i alone, but averaged over variations in other input parameters. It is standardised by the total variance to provide a fractional contribution. Higher-order interaction indices S_ij S_ijk and so on can be formed by dividing other terms in the variance decomposition by Var(Y).","category":"page"},{"location":"analysis/global_sensitivity/#eFAST-Method","page":"Global Sensitivity Analysis","title":"eFAST Method","text":"","category":"section"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"eFAST has num_harmonics as the only argument, it is the number of harmonics to sum in  the Fourier series decomposition and defaults to 4.","category":"page"},{"location":"analysis/global_sensitivity/#eFAST-Method-Details","page":"Global Sensitivity Analysis","title":"eFAST Method Details","text":"","category":"section"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"eFAST offers a robust, especially at low sample size, and computationally efficient procedure to  get the first and total order indices as discussed in Sobol. It utilizes monodimensional Fourier decomposition along a curve exploring the parameter space. The curve is defined by a set of parametric equations,","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"x_i(s) = G_i(sin ω_is)  i=12  n","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"where s is a scalar variable varying over the range -  s  +, G_i are transformation functions and ω_i  i=12n is a set of different (angular) frequencies, to be properly selected, associated with each factor. For more details on the transformation used and other implementation details you can go through  A. Saltelli et al..","category":"page"},{"location":"analysis/global_sensitivity/#Regression-Method","page":"Global Sensitivity Analysis","title":"Regression Method","text":"","category":"section"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"RegressionGSA has the following keyword arguments:","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"rank: flag which determines whether to calculate the rank coefficients. Defaults to false.","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"It returns a RegressionGSAResult, which contains the pearson, standard_regression, and partial_correlation coefficients, described below. If rank is true, then it also contains the ranked versions of these coefficients. Note that the ranked version of the pearson coefficient is also known as the Spearman coefficient, which is returned here as the pearson_rank coefficient.","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"For multi-variable models, the coefficient for the X_i input variable relating to the Y_j output variable is given as the [i, j] entry in the corresponding returned matrix.","category":"page"},{"location":"analysis/global_sensitivity/#Regression-Details","page":"Global Sensitivity Analysis","title":"Regression Details","text":"","category":"section"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"It is possible to fit a linear model explaining the behavior of Y given the values of X, provided that the sample size n is sufficiently large (at least n > d).","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"The measures provided for this analysis by us in DiffEqSensitivity.jl are","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"a) Pearson Correlation Coefficient:","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"r = fracsum_i=1^n (x_i - overlinex)(y_i - overliney)sqrtsum_i=1^n (x_i - overlinex)^2(y_i - overliney)^2","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"b) Standard Regression Coefficient (SRC):","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"SRC_j = beta_j sqrtfracVar(X_j)Var(Y)","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"where beta_j is the linear regression coefficient associated to X_j. This is also known as a sigma-normalized derivative.","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"c) Partial Correlation Coefficient (PCC):","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"PCC_j = rho(X_j - hatX_-jY_j - hatY_-j)","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"where hatX_-j is the prediction of the linear model, expressing X_j with respect to the other inputs and hatY_-j is the prediction of the linear model where X_j is absent. PCC measures the sensitivity of Y to X_j when the effects of the other inputs have been canceled.","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"If rank is set to true, then the rank coefficients are also calculated.","category":"page"},{"location":"analysis/global_sensitivity/#GSA-examples","page":"Global Sensitivity Analysis","title":"GSA examples","text":"","category":"section"},{"location":"analysis/global_sensitivity/#Lotka-Volterra-Global-Sensitivities","page":"Global Sensitivity Analysis","title":"Lotka-Volterra Global Sensitivities","text":"","category":"section"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"Let's run GSA on the Lotka-Volterra model to study the sensitivity of the maximum of predator population and the average prey population.","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"using DiffEqSensitivity, Statistics, OrdinaryDiffEq #load packages","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"First let's define our model:","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"function f(du,u,p,t)\n  du[1] = p[1]*u[1] - p[2]*u[1]*u[2] #prey\n  du[2] = -p[3]*u[2] + p[4]*u[1]*u[2] #predator\nend\nu0 = [1.0;1.0]\ntspan = (0.0,10.0)\np = [1.5,1.0,3.0,1.0]\nprob = ODEProblem(f,u0,tspan,p)\nt = collect(range(0, stop=10, length=200))","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"Now let's create a function that takes in a parameter set and calculates the maximum of the predator population and the average of the prey population for those parameter values. To do this, we will make use of the remake function which creates a new ODEProblem, and use the p keyword argument to set the new parameters:","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"f1 = function (p)\n  prob1 = remake(prob;p=p)\n  sol = solve(prob1,Tsit5();saveat=t)\n  [mean(sol[1,:]), maximum(sol[2,:])]\nend","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"Now let's perform a Morris global sensitivity analysis on this model. We specify that the parameter range is [1,5] for each of the parameters, and thus call:","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"m = gsa(f1,Morris(total_num_trajectory=1000,num_trajectory=150),[[1,5],[1,5],[1,5],[1,5]])","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"Let's get the means and variances from the MorrisResult struct.","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"m.means\n2×2 Array{Float64,2}:\n 0.474053  0.114922\n 1.38542   5.26094\n\nm.variances\n2×2 Array{Float64,2}:\n 0.208271    0.0317397\n 3.07475   118.103    ","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"Let's plot the result","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"scatter(m.means[1,:], m.variances[1,:],series_annotations=[:a,:b,:c,:d],color=:gray)\nscatter(m.means[2,:], m.variances[2,:],series_annotations=[:a,:b,:c,:d],color=:gray)","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"For the Sobol method we can similarly do:","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"m = gsa(f1,Sobol(),[[1,5],[1,5],[1,5],[1,5]],N=1000)","category":"page"},{"location":"analysis/global_sensitivity/#Design-Matrices","page":"Global Sensitivity Analysis","title":"Design Matrices","text":"","category":"section"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"For the Sobol Method, we can have more control over the sampled points by generating design matrices. Doing it in this manner lets us directly specify a quasi-Monte Carlo sampling method for the parameter space. Here we use QuasiMonteCarlo.jl to generate the design matrices as follows:","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"N = 10000\nlb = [1.0, 1.0, 1.0, 1.0]\nub = [5.0, 5.0, 5.0, 5.0]\nsampler = SobolSample()\nA,B = QuasiMonteCarlo.generate_design_matrices(N,lb,ub,sampler)","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"and now we tell it to calculate the Sobol indices on these designs:","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"sobol_result = gsa(f1,Sobol(),A,B)","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"We plot the first order and total order Sobol Indices for the parameters (a and b).","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"\np1 = bar([\"a\",\"b\",\"c\",\"d\"],sobol_result.ST[1,:],title=\"Total Order Indices prey\",legend=false)\np2 = bar([\"a\",\"b\",\"c\",\"d\"],sobol_result.S1[1,:],title=\"First Order Indices prey\",legend=false)\np1_ = bar([\"a\",\"b\",\"c\",\"d\"],sobol_result.ST[2,:],title=\"Total Order Indices predator\",legend=false)\np2_ = bar([\"a\",\"b\",\"c\",\"d\"],sobol_result.S1[2,:],title=\"First Order Indices predator\",legend=false)\nplot(p1,p2,p1_,p2_)","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"(Image: sobolplot)","category":"page"},{"location":"analysis/global_sensitivity/#Parallelized-GSA-Example","page":"Global Sensitivity Analysis","title":"Parallelized GSA Example","text":"","category":"section"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"In all of the previous examples, f(p) was calculated serially. However, we can parallelize our computations by using the batch interface. In the batch interface, each column p[:,i] is a set of parameters, and we output a column for each set of parameters. Here we showcase using the Ensemble Interface to use EnsembleGPUArray to perform automatic multithreaded-parallelization of the ODE solves.","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"f1 = function (p)\n  prob_func(prob,i,repeat) = remake(prob;p=p[:,i])\n  ensemble_prob = EnsembleProblem(prob,prob_func=prob_func)\n  sol = solve(ensemble_prob,Tsit5(),EnsembleThreads();saveat=t,trajectories=size(p,2))\n  # Now sol[i] is the solution for the ith set of parameters\n  out = zeros(2,size(p,2))\n  for i in 1:size(p,2)\n    out[1,i] = mean(sol[i][1,:])\n    out[2,i] = maximum(sol[i][2,:])\n  end\n  out\nend","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"And now to do the parallelized calls we simply add the batch=true keyword argument:","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"sobol_result = gsa(f1,Sobol(),A,B,batch=true)","category":"page"},{"location":"analysis/global_sensitivity/","page":"Global Sensitivity Analysis","title":"Global Sensitivity Analysis","text":"This user-side parallelism thus allows you to take control, and thus for example you can use DiffEqGPU.jl for automated GPU-parallelism of the ODE-based global sensitivity analysis!","category":"page"},{"location":"basics/faq/#faq","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"","category":"section"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"This page is a compilation of frequently asked questions and answers.","category":"page"},{"location":"basics/faq/#faq_stability","page":"Frequently Asked Questions","title":"Stability and Divergence of ODE Solves","text":"","category":"section"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"For guidelines on debugging ODE solve issues, see  PSA: How to help yourself debug differential equation solving issues.","category":"page"},{"location":"basics/faq/#My-model-is-reporting-unstable-results.-What-can-I-do?","page":"Frequently Asked Questions","title":"My model is reporting unstable results. What can I do?","text":"","category":"section"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"First of all, don't panic. You may have experienced one of the following warnings:","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"dt <= dtmin. Aborting. There is either an error in your model specification or the true solution is unstable.NaN dt detected. Likely a NaN value in the state, parameters, or derivative value caused this outcome.Instability detected. Aborting","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"These are all pointing to a similar behavior: for some reason or another, the ODE solve is diverging to infinity. As it diverges to infinity, the dt of the integrator will drop (trying to control the speed and error), so it will either hit the minimum dt, hit dt=NaN, or have a value in the ODE hit Inf. Whichever one occurs first will throw the respective warning.","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"How to handle this? 99.99% of the time this has been debugged, it has turned out to be an error in the user's model! A missing minus sign, an incorrect term, etc. There are many other behaviors to watch out for. In some ODEs, increasing a parameter can cause a bifurcation so that the solution diverges. With u'=a*u, if a is negative then it nicely falls to zero, but if a is positive the solution quickly diverges to infinity! This means, double check your parameters are indexed correctly!","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Note: if you see these warnings during a parameter estimation process, this is likely the underlying problem. Simply check sol.retcode != :Success and throw an Inf cost and most optimizers will reject steps in those parameter regimes!","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"There are a few other things to check as well. In many cases, the stability of an ODE solve improves as you decrease the tolerance, so you may want to try a smaller abstol and reltol. One behavior to watch out for is that if your model is a differential-algebraic equation and your DAE is of high index (say index>1), this can impact the numerical solution. In this case you may want to use the ModelingToolkit.jl index reduction tools to improve the numerical stability of a solve. In addition, if it's a highly stiff ODE/DAE that is large and you're using a matrix-free solver (such as GMRES), make sure the tolerance of the GMRES is well-tuned and an appropriate preconditioner is applied. Finally, try other solvers. They all have different stability, so try Tsit5(), Vern7(), QNDF(), Rodas5(), TRBDF2(), KenCarp4(), Sundials.CVODE_BDF(), etc. and see what works.","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"If none of this works out, double check that your ODE truly has the behavior that you believe it should. This is one of the most common issues: your intuition may be deceiving. For example, u' = -sqrt(u) with u(0)=1 cannot hit zero because its derivative shrinks to zero, right? Wrong! It will hit zero in a finite time, after which the solution is undefined and does not have a purely real solution. u' = u^2 - 100u will \"usually\" go to zero, but if u(0)>10 then it will go to infinity. Plot out your diverging solution and see whether the asymtopics are correct: if u[i] gets big, do you equations make u'[i] positive and growing? That's would be a problem!","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Let's say you don't believe you made an error at all and you want to file a bug report. To do so, you'll first want to prove that it's isolated to a solver. If it's a solver issue, then you shouldn't see it happen with every single solver. Do you think it's an issue with the Julia solvers? Well fortunately, DifferentialEquations.jl offers direct unmodified wrappers to almost all previously built solvers, so if you think it's a Julia issue, try running your ODE through:","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Sundials.jl, a wrapper for the C++ SUNDIALS library though CVODE_Adams, CVODE_BDF, IDA, and ARKODE.\nODEInterfaceDiffEq.jl, a wrapper for the classic Hairer Fortran codes like dorpi5, dop853, radau, rodas, etc.\nLSODA.jl, a wrapper for the classic lsoda algorithm.\nMATLABDiffEq.jl, a wrapper for the MATLAB ODE solvers ode45, ode15s, etc.\nSciPyDiffEq.jl, a wrapper for SciPy's odeint (LSODA) and other methods (LSODE, etc.).\ndeSolveDiffEq.jl, a wrapper for the commonly used R library.","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"And many more. Testing this is as simple as changing solve(prob,Tsit5()) to solve(prob,lsoda()), so please give this a try. If you translated your code from another language, like Python or MATLAB, use the direct wrapper to double check the steps are the same. If they are not, then your ODE is not the same, because it's using a direct call to the solvers of those packages!","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"If your ODE diverges to infinity with every ODE solver ever made, the problem is most likely not the ODE solvers. Or rather, to put it in meme form:","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"(Image: )","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Don't be like Patrick. If after trying these ideas your ODE solve still seems to have issues and you haven't narrowed it down, feel free to ask on the Julia Discourse to get some help diagnosing it. If you did find a solver issue, please open an issue on the Github repository.","category":"page"},{"location":"basics/faq/#A-larger-maxiters-seems-to-be-needed,-but-it's-already-high?","page":"Frequently Asked Questions","title":"A larger maxiters seems to be needed, but it's already high?","text":"","category":"section"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"If you see:","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Interrupted. Larger maxiters is needed.","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Note that it could quite possibly arise just from having a very long timespan. If you check sol.t from the returned object and it looks like it's stepping at reasonable lengths, feel free to just pass maxiters=... into solve to bump it up from the default of Int(1e5).","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"But if your maxiters is already high, then the problem is likely that your model is stiff. A stiff ODE requires very small time steps from many explicit solvers, such as Tsit5(), Vern7(), etc., and thus those methods are not appropriate for this kind of problem. You will want to change to a different method, like Rodas5(), Rosenbrock23(), TRBDF2(), KenCarp4(), or QNDF().","category":"page"},{"location":"basics/faq/#My-ODE-goes-negative-but-should-stay-positive,-what-tools-can-help?","page":"Frequently Asked Questions","title":"My ODE goes negative but should stay positive, what tools can help?","text":"","category":"section"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"There are many tools to help! However, let's first focus on one piece first: when you say \"should\" be positive, what do you mean by \"should\"? If you mean \"mathematically you can prove that the ODE with these values and these initial conditions will have a solution that is positive for all time\" then yes, you're looking in the right place. If by \"should\" you mean \"it's a model of biochemical reactions so the concentration should always be positive\", well ask yourself first, did you write down a model where it will always be positive?","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"The following set of tools are designed to accuracy enforce positivity in ODE models which mathematically should be positive in the true solution. If they encounter a model that is actually going negative, they will work really hard to get a positive but correct solution, which is impossible, so they will simply error out. This can be more subtle than you think. Solving u'=-sqrt(u) is not guaranteed to stay positive, even though the derivative goes to zero as u goes to zero (check the analytical solution if you're curious). Similarly, analyzing nonlinear models can showcase all sorts of behavior. A common cause for accidental negativity is Hill functions in systems biology models: just because derivatives go to zero doesn't mean they are going to zero fast enough to keep things positive!","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"With that in mind, let's see the options.","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"The simplest trick is to change the solver tolerance. Reduce abstol (and maybe reltol) a bit. That can help reduce the error and thus keep the solution positive. For some more difficult equations, changing to a stiff ODE solver like Rosenbrock23() QNDF, or TRBDF2() can be helpful.","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"If those don't work, call out the big guns. One of them is isoutofdomain, where you can define a boolean function which will cause step rejections whenever it is not satisfied. For example, isoutofdomain = (u,p,t)->any(x->x<0,u) will make the solver reject any step which cases any variable u to go negative. Now, using any pure-Julia solver with this option, it's impossible to get a negative in the result! One thing you may see though is:","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"dt <= dtmin. Aborting. There is either an error in your model specification or the true solution is unstable.","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"or","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Interrupted. Larger maxiters is needed.","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"What this means is that enforcing positivity is not possible. It keeps rejecting steps that go negative, reducing dt, taking another step, rejecting, reducing, repeat until dt hits dtmin or it hits maxiters. This means that even when trying to solve the problem with the most accurate infinitesimal dt, the solution still goes negative. Are you sure the true solution is supposed to be positive? If you see this, check for issues like a missing minus sign in your equations.","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"If that works but is a little slow, the domain handling callbacks in the callback library are designed to function similarly but in a way that gets better performance. Instead of repeating lots of steps through rejections, it interpolates back to still take a smaller step, always progressing forwards. However, this can be a bit less stable, so its applicability depends on the equation, and once again this requires that the solution is truly positive. If the true solution goes negative, it will repeatedly try interpolating backwards until it can no longer and end with a dtmin issue.","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Finally, note that ODE solvers will not be more correct than tolerance, and so one should expect that if the solution is supposed to be positive but abstol=1e-12, you may end up with u[i]=-1e-12. That is okay, that is expected behavior of numerical solvers, the ODE solver is still doing its job. If this is a major issue for your application, you may want to write your model to be robust to this behavior, such as changing sqrt(u[i]) to sqrt(max(0,u[i])). You should also consider transforming your values, like solving for u^2 or exp(u) instead of u, which mathematically can only be positive. Look into using a tool like ModelingToolkit.jl for automatically transforming your equations.","category":"page"},{"location":"basics/faq/#faq_performance","page":"Frequently Asked Questions","title":"Performance","text":"","category":"section"},{"location":"basics/faq/#GPUs,-multithreading-and-distributed-computation-support","page":"Frequently Asked Questions","title":"GPUs, multithreading and distributed computation support","text":"","category":"section"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Yes. The *DiffEq.jl libraries (OrdinaryDiffEq.jl, StochasticDiffEq.jl, and DelayDiffEq.jl) are all written to be generic to the array and number types. This means they will adopt the implementation that is given by the array type. The in-place algorithms internally utilize Julia's broadcast (with some exceptions due to a Julia bug for now, see this issue) and Julia's mul! in-place matrix multiplication function. The out-of-place algorithms utilize standard arithmetical functions. Both additionally utilize the user's norm specified via the common interface options and, if a stiff solver, ForwardDiff/DiffEqDiffTools for the Jacobian calculation, and Base linear factorizations for the linear solve. For your type, you may likely need to give a better form of the norm, Jacobian, or linear solve calculations to fully utilize parallelism.","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"GPUArrays.jl (CuArrays.jl), ArrayFire.jl, DistributedArrays.jl have been tested and work in various forms, where the last one is still not recommended for common use yet.","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"The next question is whether it matters. Generally, your system has to be large for parallelism to matter. Using a multithreaded array for broadcast we find helpful around N>1000, though the Sundials manual says N>100,000. For high order Runge-Kutta methods it's likely lower than the Sundials estimate because of more operations packed into each internal step, but as always that will need more benchmarks to be precise and will depend on the problem being solved. GPUs generally require some intensive parallel operation in the user's f function to be viable, for example a matrix multiplication for a stencil computation in a PDE. If you're simply solving some ODE element-wise on a big array it likely won't do much or it will slow things down just due to how GPUs work. DistributedArrays require parallel linear solves to really matter, and thus are only recommended when you have a problem that cannot fit into memory or are using a stiff solver with a Krylov method for the linear solves.","category":"page"},{"location":"basics/faq/#My-ODE-is-solving-really-slow","page":"Frequently Asked Questions","title":"My ODE is solving really slow","text":"","category":"section"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"First, check for bugs. These solvers go through a ton of convergence tests and so if there's a solver issue, it's either just something to do with how numerical methods work or it's a user-error (generally the latter, though check the later part of the FAQ on normal numerical errors). User-errors in the f function causing a divergence of the solution is the most common reason for reported slow codes.","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"If you have no bugs, great! The standard tricks for optimizing Julia code then apply. Take a look at the Optimizing DiffEq Code tutorial for some tips and pointers.","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"What you want to do first is make sure your function does not allocate. If your system is small (<=100 ODEs/SDEs/DDEs/DAEs?), then you should set your system up to use StaticArrays.jl. This is demonstrated in the ODE tutorial with static matrices. Static vectors/arrays are stack-allocated, and thus creating new arrays is free and the compiler doesn't have to heap-allocate any of the temporaries (that's the expensive part!). These have specialized super fast dispatches for arithmetic operations and extra things like LU-factorizations, and thus they are preferred when possible. However, they lose efficiency if they grow too large.","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"For anything larger, you should use the in-place syntax f(du,u,p,t) and make sure that your function doesn't allocate. Assuming you know of a u0, you should be able to do:","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"du = similar(u0)\n@time f(du,u0,p,t)","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"and see close to zero allocations and close to zero memory allocated. If you see more, then you might have a type-instability or have temporary arrays. To find type-instabilities, you should do:","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"@code_warntype f(du,u,p,t)","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"and read the printout to see if there's any types that aren't inferred by the compiler, and fix them. If you have any global variables, you should make them const. As for allocations, some common things that allocate are:","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Array slicing, like u[1:5]. Instead, use @view u[1:5]\nMatrix multiplication with *. Instead of A*b, use mul!(c,A,b) for some pre-allocated cache vector c.\nNon-broadcasted expressions. Every expression on arrays should .= into another array, or it should be re-written to loop and do computations with scalar (or static array) values.","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"For an example of optimizing a function resulting from a PDE discretization, see this blog post.","category":"page"},{"location":"basics/faq/#The-stiff-solver-takes-forever-to-take-steps-for-my-PDE-discretization","page":"Frequently Asked Questions","title":"The stiff solver takes forever to take steps for my PDE discretization","text":"","category":"section"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"The solvers for stiff solvers require solving a nonlinear equation each step. In order to do so, they have to do a few Newton steps. By default, these methods assume that the Jacobian is dense, automatically calculate the Jacobian for you, and do a dense factorization. However, in many cases you may want to use alternatives that are more tuned for your problem.","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"First of all, when available, it's recommended that you pass a function for computing your Jacobian. This is discussed in the performance overloads section. Jacobians are especially helpful for Rosenbrock methods.","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Secondly, if your Jacobian isn't dense, you shouldn't use a dense Jacobian! Instead, if you're using  a *DiffEq library you should specify a linear solver and/or a jac_prototype for the matrix form, and for Sundials.jl, you should change the linear_solver option. See the ODE solve Sundials portion for details on that.","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Right now, QNDF is the recommended method for stiff problems with large sparse Jacobians. You should specify jac_prototype as a special matrix, such as a banded or tridiagonal matrix, if it satisfies a special structure. If you only know the Jacobian is sparse, using automated sparsity detection can help with identifying the sparsity pattern. See the stiff ODE tutorial for more details. Lastly, using LinSolveGMRES() can help if a sparsity pattern cannot be obtained but the matrix is large, or if the sparsity cannot fit into memory. Once again, a good reference for how to handle PDE discretizations can be found at this blog post.","category":"page"},{"location":"basics/faq/#My-Problem-Has-Discontinuities-and-is-Unstable-/-Slow","page":"Frequently Asked Questions","title":"My Problem Has Discontinuities and is Unstable / Slow","text":"","category":"section"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"This Discourse post goes into detail for how to handle discontinuities in your ODE function and how to use that extra information to speed up the solver.","category":"page"},{"location":"basics/faq/#Complicated-Models","page":"Frequently Asked Questions","title":"Complicated Models","text":"","category":"section"},{"location":"basics/faq/#Switching-ODE-functions-in-the-middle-of-integration","page":"Frequently Asked Questions","title":"Switching ODE functions in the middle of integration","text":"","category":"section"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"There are a few ways to do this. The simplest way is to just have a parameter to switch between the two. For example:","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"function f(du,u,p,t)\n  if p == 0\n    du[1] = 2u[1]\n  else\n    du[1] = -2u[1]\n  end\n  du[2] = -u[2]\nend","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Then in a callback you can make the affect! function modify integrator.prob.p. For example, we can make it change when u[2]<0.5 via:","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"condition(t,u,integrator) = u[2] - 0.5\naffect!(integrator) = integrator.prob.p = 1","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Then it will change betweeen the two ODE choices for du1 at that moment. Another way to do this is to make the ODE functions all be the same type via FunctionWrappers.jl, but that is unnecessary. With the way that modern processors work, there exists branch prediction and thus execution of a conditional is free if it's predictable which branch will be taken. In this case, almost every call to f takes the p==0 route until the callback, at which point it is almost always the else route. Therefore the processor will effectively get rid of the computational cost associated with this, so you're likely over-optimizing if you're going further (unless this change happens every step, but even then this is probably the cheapest part of the computation...).","category":"page"},{"location":"basics/faq/#Numerical-Error","page":"Frequently Asked Questions","title":"Numerical Error","text":"","category":"section"},{"location":"basics/faq/#What-does-tolerance-mean-and-how-much-error-should-I-expect","page":"Frequently Asked Questions","title":"What does tolerance mean and how much error should I expect","text":"","category":"section"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"The most useful options are the tolerances abstol and reltol. These tell the internal adaptive time stepping engine how precise of a solution you want. Generally, reltol is the relative accuracy while abstol is the accuracy when u is near zero. These tolerances are local tolerances and thus are not global guarantees. However, a good rule of thumb is that the total solution accuracy is 1-2 digits less than the relative tolerances. Thus for the defaults abstol=1e-6 and reltol=1e-3, you can expect a global accuracy of about 1-2 digits. This is standard across the board and applies to the native Julia methods, the wrapped Fortran and C++ methods, the calls to MATLAB/Python/R, etc.","category":"page"},{"location":"basics/faq/#The-solver-doesn't-obey-physical-law-X-(e.g.-conservation-of-energy)","page":"Frequently Asked Questions","title":"The solver doesn't obey physical law X (e.g. conservation of energy)","text":"","category":"section"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Yes, this is because the numerical solution of the ODE is not the exact solution. There are a few ways that you can handle this problem. One way is to get a more exact solution. Thus instead of","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"sol = solve(prob,alg)","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"use","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"sol = solve(prob,alg,abstol=1e-10,reltol=1e-10)","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Of course, there's always a tradeoff between accuracy and efficiency, so play around to find out what's right for your problem.","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Another thing you can do is use a callback. There are some premade callbacks in the callback library which handle these sorts of things like projecting to manifolds and preserving positivity.","category":"page"},{"location":"basics/faq/#Symplectic-integrators-don't-conserve-energy","page":"Frequently Asked Questions","title":"Symplectic integrators don't conserve energy","text":"","category":"section"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Yes, symplectic integrators do not exactly conserve energy. It is a common misconception that they do. What symplectic integrators actually do is solve for a trajectory which rests on a symplectic manifold that is perturbed from the true solution's manifold by the truncation error. This means that symplectic integrators do not experience (very much) long time drift, but their orbit is not exactly the same as the true solution in phase space and thus you will see differences in energy that tend to look periodic. There is a small drift which grows linearly and is related to floating point error, but this drift is much less than standard methods. This is why symplectic methods are recommended for long time integration.","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"For conserving energy, there are a few things you can do. First of all, the energy error is related to the integration error, so simply solving with higher accuracy will reduce the error. The results in the DiffEqBenchmarks show that using a DPRKN method with low tolerance can be a great choice. Another thing you can do is use the ManifoldProjection callback from the callback library.","category":"page"},{"location":"basics/faq/#How-to-get-to-zero-error","page":"Frequently Asked Questions","title":"How to get to zero error","text":"","category":"section"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"You can't. For floating point numbers, you shouldn't use below abstol=1e-14 and reltol=1e-14. If you need lower than that, use arbitrary precision numbers like BigFloats or ArbFloats.jl.","category":"page"},{"location":"basics/faq/#Autodifferentiation-and-Dual-Numbers","page":"Frequently Asked Questions","title":"Autodifferentiation and Dual Numbers","text":"","category":"section"},{"location":"basics/faq/#Native-Julia-solvers-compatibility-with-autodifferentiation","page":"Frequently Asked Questions","title":"Native Julia solvers compatibility with autodifferentiation","text":"","category":"section"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Yes, they are compatible with automatic differentiation! Take a look at the sensitivity analysis page for more details.","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"If the algorithm does not have differentiation of parameter-dependent events, then you simply need to make the initial condition have elements of Dual numbers. If the algorithm uses Dual numbers, you need to make sure that time is also given by Dual numbers.","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"To show this in action, let's say we want to find the Jacobian of solution of the Lotka-Volterra equation at t=10 with respect to the parameters.","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"function func(du,u,p,t)\n  du[1] = p[1] * u[1] - p[2] * u[1]*u[2]\n  du[2] = -3 * u[2] + u[1]*u[2]\nend\nfunction f(p)\n  prob = ODEProblem(func,eltype(p).([1.0,1.0]),(0.0,10.0),p)\n  # Lower tolerances to show the methods converge to the same value\n  solve(prob,Tsit5(),save_everystep=false,abstol=1e-12,reltol=1e-12)[end]\nend","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"This function takes in new parameters and spits out the solution at the end. We make the inital condition eltype(p).([1.0,1.0]) so that way it's typed to be Dual numbers whenever p is an array of Dual numbers, and we do the same for the timespan just to show what you'd do if there was parameters-dependent events. Then we can take the Jacobian via ForwardDiff.jl:","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"using ForwardDiff\nForwardDiff.jacobian(f,[1.5,1.0])\n\n2×2 Array{Float64,2}:\n  2.16056   0.188569\n -6.25677  -0.697978","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"and compare it to Calculus.jl:","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Calculus.jacobian(f,[1.5,1.0],:central)\n\n2×2 Array{Float64,2}:\n  2.16056   0.188569\n -6.25677  -0.697978","category":"page"},{"location":"basics/faq/#I-get-Dual-number-errors-when-I-solve-my-ODE-with-Rosenbrock-or-SDIRK-methods","page":"Frequently Asked Questions","title":"I get Dual number errors when I solve my ODE with Rosenbrock or SDIRK methods","text":"","category":"section"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"This is because you're using a cache which is not compatible with autodifferentiaion via ForwardDiff.jl. For example, if we use the ODE function:","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"using LinearAlgebra, OrdinaryDiffEq\nfunction foo(du, u, (A, tmp), t)\n    mul!(tmp, A, u)\n    @. du = u + tmp\n    nothing\nend\nprob = ODEProblem(foo, ones(5, 5), (0., 1.0), (ones(5,5), zeros(5,5)))\nsolve(prob, Rosenbrock23())","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"Here we use a cached temporary array in order to avoid the allocations of matrix multiplication. When autodifferentiation occurs, the element type of u is Dual numbers, so A*u produces Dual numbers, so the error arises when it tries to write into tmp. There are two ways to avoid this. The first way, the easy way, is to just turn off autodifferentiation with the autodiff=false option in the solver. Every solver which uses autodifferentiation has this option. Thus we'd solve this with:","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"prob = ODEProblem(f,rand(4),(0.0,1.0))\nsol = solve(prob,Rosenbrock23(autodiff=false))","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"and it will use a numerical differentiation fallback (DiffEqDiffTools.jl) to calculate Jacobians.","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"We could use get_tmp and dualcache functions from  PreallocationTools.jl  to solve this issue, e.g.,","category":"page"},{"location":"basics/faq/","page":"Frequently Asked Questions","title":"Frequently Asked Questions","text":"using LinearAlgebra, OrdinaryDiffEq, PreallocationTools\nfunction foo(du, u, (A, tmp), t)\n    tmp = get_tmp(tmp, first(u)*t)\n    mul!(tmp, A, u)\n    @. du = u + tmp\n    nothing\nend\nprob = ODEProblem(foo, ones(5, 5), (0., 1.0), (ones(5,5), DiffEqBase.dualcache(zeros(5,5))))\nsolve(prob, TRBDF2()","category":"page"},{"location":"basics/problem/#Problem-Interface","page":"Problem Interface","title":"Problem Interface","text":"","category":"section"},{"location":"basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"This page defines the common problem interface. There are certain rules that can be applied to any function definition, and this page defines those behaviors.","category":"page"},{"location":"basics/problem/#In-place-vs-Out-of-Place-Function-Definition-Forms","page":"Problem Interface","title":"In-place vs Out-of-Place Function Definition Forms","text":"","category":"section"},{"location":"basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"Every problem definition has an in-place and out-of-place form, commonly referred throughout DiffEq as IIP (isinplace) and OOP (out of place). The in-place form is a mutating form. For example, on ODEs, we have that f!(du,u,p,t) is the in-place form which, as its output, mutates du. Whatever is returned is simply ignored. Similarly, for OOP we have the form du=f(u,p,t) which uses the return.","category":"page"},{"location":"basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"Each of the problem types have that the first argument is the option mutating argument. The SciMLBase system will automatically determine the functional form and place a specifier isinplace on the function to carry as type information whether the function defined for this DEProblem is in-place. However, every constructor allows for manually specifying the in-placeness of the function. For example, this can be done at the problem level like:","category":"page"},{"location":"basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"ODEProblem{true}(f,u0,tspan,p)","category":"page"},{"location":"basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"which declares that isinplace=true. Similarly this can be done at the DEFunction level. For example:","category":"page"},{"location":"basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"ODEFunction{true}(f,jac=myjac)","category":"page"},{"location":"basics/problem/#Type-Specifications","page":"Problem Interface","title":"Type Specifications","text":"","category":"section"},{"location":"basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"Throughout DifferentialEquations.jl, the types that are given in a problem are the types used for the solution. If an initial value u0 is needed for a problem, then the state variable u will match the type of that u0. Similarly, if time exists in a problem the type for t will be derived from the types of the tspan. Parameters p can be any type and the type will be matching how it's defined in the problem.","category":"page"},{"location":"basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"For internal matrices, such as Jacobians and Brownian caches, these also match the type specified by the user. jac_prototype and rand_prototype can thus be any Julia matrix type which is compatible with the operations that will be performed.","category":"page"},{"location":"basics/problem/#Functional-and-Condensed-Problem-Inputs","page":"Problem Interface","title":"Functional and Condensed Problem Inputs","text":"","category":"section"},{"location":"basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"Note that the initial condition can be written as a function of parameters and initial time:","category":"page"},{"location":"basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"u0(p,t0)","category":"page"},{"location":"basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"and be resolved before going to the solver. Additionally, the initial condition can be a distribution from Distributions.jl, in which case a sample initial condition will be taken each time init or solve is called.","category":"page"},{"location":"basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"In addition, tspan supports the following forms. The single value form t is equivalent to (zero(t),t). The functional form is allowed:","category":"page"},{"location":"basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"tspan(p)","category":"page"},{"location":"basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"which outputs a tuple.","category":"page"},{"location":"basics/problem/#Examples","page":"Problem Interface","title":"Examples","text":"","category":"section"},{"location":"basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"prob = ODEProblem((u,p,t)->u,(p,t0)->p[1],(p)->(0.0,p[2]),(2.0,1.0))\nusing Distributions\nprob = ODEProblem((u,p,t)->u,(p,t)->Normal(p,1),(0.0,1.0),1.0)","category":"page"},{"location":"basics/problem/#Lower-Level-__init-and-__solve","page":"Problem Interface","title":"Lower Level __init and __solve","text":"","category":"section"},{"location":"basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"At the high level, known problematic problems will emit warnings before entering the solver to better clarify the error to the user. The following cases are checked if the solver is adaptive:","category":"page"},{"location":"basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"Integer times warn\nDual numbers must be in the initial conditions and timespans\nMeasurements.jl values must be in the initial conditions and timespans","category":"page"},{"location":"basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"If there is an exception to these rules, please file an issue. If one wants to go around the high level solve interface and its warnings, one can call __init or __solve instead.","category":"page"},{"location":"basics/problem/#Modification-of-problem-types","page":"Problem Interface","title":"Modification of problem types","text":"","category":"section"},{"location":"basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"Problem-related types in DifferentialEquations.jl are immutable.  This helps, e.g., parallel solvers to efficiently handle problem types.","category":"page"},{"location":"basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"However, you may want to modify the problem after it is created.  For example, to simulate it for longer timespan.  It can be done by the remake function:","category":"page"},{"location":"basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"prob1 = ODEProblem((u,p,t) -> u/2, 1.0, (0.0,1.0))\nprob2 = remake(prob1; tspan=(0.0,2.0))","category":"page"},{"location":"basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"A general syntax of remake is","category":"page"},{"location":"basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"modified_problem = remake(original_problem;\n  field_1 = value_1,\n  field_2 = value_2,\n  ...\n)","category":"page"},{"location":"basics/problem/","page":"Problem Interface","title":"Problem Interface","text":"where field_N and value_N are renamed to appropriate field names and new desired values.","category":"page"},{"location":"solvers/discrete_solve/#Discrete-Solvers","page":"Discrete Solvers","title":"Discrete Solvers","text":"","category":"section"},{"location":"solvers/discrete_solve/#DiscreteProblems","page":"Discrete Solvers","title":"DiscreteProblems","text":"","category":"section"},{"location":"solvers/discrete_solve/","page":"Discrete Solvers","title":"Discrete Solvers","text":"solve(prob::DiscreteProblem,alg;kwargs)","category":"page"},{"location":"solvers/discrete_solve/","page":"Discrete Solvers","title":"Discrete Solvers","text":"Solves the discrete function map defined by prob using the algorithm alg. If no algorithm is given, a default algorithm will be chosen.","category":"page"},{"location":"solvers/discrete_solve/#Recommended-Methods","page":"Discrete Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"solvers/discrete_solve/","page":"Discrete Solvers","title":"Discrete Solvers","text":"The implementation for solving discrete equations is the FunctionMap algorithm in OrdinaryDiffEq.jl. It allows the full common interface (including events/callbacks) to solve function maps, along with everything else like plot recipes, while completely ignoring the ODE functionality related to continuous equations (except for a tiny bit of initialization). However, the SimpleFunctionMap from SimpleDiffEq.jl can be more efficient if the mapping function is sufficiently cheap, but it doesn't have all of the extras like callbacks and saving support (but does have an integrator interface).","category":"page"},{"location":"solvers/discrete_solve/#Full-List-of-Methods","page":"Discrete Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"solvers/discrete_solve/#OrdinaryDiffEq.jl","page":"Discrete Solvers","title":"OrdinaryDiffEq.jl","text":"","category":"section"},{"location":"solvers/discrete_solve/","page":"Discrete Solvers","title":"Discrete Solvers","text":"FunctionMap: A basic function map which implements the full common interface.","category":"page"},{"location":"solvers/discrete_solve/","page":"Discrete Solvers","title":"Discrete Solvers","text":"OrdinaryDiffEq.jl also contains the FunctionMap algorithm which lets you  It has a piecewise constant interpolation and allows for all of the  callback/event handling capabilities (of course, with rootfind=false. If a  ContinuousCallback is given, it's always assumed rootfind=false).","category":"page"},{"location":"solvers/discrete_solve/","page":"Discrete Solvers","title":"Discrete Solvers","text":"The constructor is:","category":"page"},{"location":"solvers/discrete_solve/","page":"Discrete Solvers","title":"Discrete Solvers","text":"FunctionMap()\nFunctionMap{scale_by_time}()","category":"page"},{"location":"solvers/discrete_solve/","page":"Discrete Solvers","title":"Discrete Solvers","text":"Every step is the update","category":"page"},{"location":"solvers/discrete_solve/","page":"Discrete Solvers","title":"Discrete Solvers","text":"u_n+1 = f(t_n+1u_n)","category":"page"},{"location":"solvers/discrete_solve/","page":"Discrete Solvers","title":"Discrete Solvers","text":"If in addition scale_by_time is marked true (default is false),  then every step is the update:","category":"page"},{"location":"solvers/discrete_solve/","page":"Discrete Solvers","title":"Discrete Solvers","text":"u_n+1 = u_n + dtf(t_n+1u_n)","category":"page"},{"location":"solvers/discrete_solve/","page":"Discrete Solvers","title":"Discrete Solvers","text":"Notice that this is the same as updates from the Euler method, except in this case we assume that its a discrete change and thus the interpolation is piecewise constant.","category":"page"},{"location":"solvers/discrete_solve/#SimpleDiffEq.jl","page":"Discrete Solvers","title":"SimpleDiffEq.jl","text":"","category":"section"},{"location":"solvers/discrete_solve/","page":"Discrete Solvers","title":"Discrete Solvers","text":"SimpleFunctionMap: A barebones implementation of a function map. Is optimally-efficient and has an integrator interface version, but does not support callbacks or saving controls.","category":"page"},{"location":"analysis/sensitivity/#sensitivity","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"","category":"section"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"Sensitivity analysis, or automatic differentiation of the solver, is provided by the DiffEq suite. The model sensitivities are the derivatives of the solution u(t) with respect to the parameters. Specifically, the local sensitivity of the solution to a parameter is defined by how much the solution would change by changes in the parameter, i.e. the sensitivity of the ith independent variable to the jth parameter is fracpartial u_ipartial p_j.","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"Sensitivity analysis serves two major purposes. On one hand, the sensitivities are diagnostics of the model which are useful for understand how it will change in accordance to changes in the parameters. But another use is simply because in many cases these derivatives are useful. Sensitivity analysis provides a cheap way to calculate the gradient of the solution which can be used in parameter estimation and other optimization tasks.","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"There are three types of sensitivity analysis. Local forward sensitivity analysis directly gives the gradient of the solution with respect to each parameter along the time series. The computational cost scales like N*M, where N is the number of states and M is the number of parameters. While this gives all of the information, it can be expensive for models with large numbers of parameters. Local adjoint sensitivity analysis solves directly for the gradient of some functional of the solution, such as a cost function or energy functional, in a manner that is cheaper when the number of parameters is large. Global Sensitivity Analysis methods are meant to be used for exploring the sensitivity over a larger domain without calculating derivatives and are covered on a different page.","category":"page"},{"location":"analysis/sensitivity/#Installation","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Installation","text":"","category":"section"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"This functionality does not come standard with DifferentialEquations.jl. To use this functionality, you must install DiffEqSensitivity.jl:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"]add DiffEqSensitivity\nusing DiffEqSensitivity","category":"page"},{"location":"analysis/sensitivity/#High-Level-Interface:-sensealg","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"High Level Interface: sensealg","text":"","category":"section"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"The highest level interface is provided by the function solve:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"solve(prob,args...;sensealg=InterpolatingAdjoint(),\n      checkpoints=sol.t,kwargs...)","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"solve is fully compatible with automatic differentiation libraries like:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"Zygote.jl\nReverseDiff.jl\nTracker.jl\nForwardDiff.jl","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"and will automatically replace any calculations of the solution's derivative with a fast method. The keyword argument sensealg controls the dispatch to the AbstractSensitivityAlgorithm used for the sensitivity calculation. Note that solve in an AD context does not allow higher order interpolations unless sensealg=DiffEqBase.SensitivityADPassThrough() is used, i.e. going back to the AD mechanism.","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"note: Note\nForwardDiff.jl only does forward differentiation pass through.","category":"page"},{"location":"analysis/sensitivity/#solve-Differentiation-Examples","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"solve Differentiation Examples","text":"","category":"section"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"using DiffEqSensitivity, OrdinaryDiffEq, ForwardDiff, Zygote\n\nfunction fiip(du,u,p,t)\n  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]\n  du[2] = dy = -p[3]*u[2] + p[4]*u[1]*u[2]\nend\np = [1.5,1.0,3.0,1.0]; u0 = [1.0;1.0]\nprob = ODEProblem(fiip,u0,(0.0,10.0),p)\nsol = solve(prob,Tsit5(),rtol=1e-6,atol=1e-6)","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"note: Note\nSince the global error is 1-2 orders of magnitude higher than the local error, we use accuracies of 1e-6 (instead of the default 1e-3) to get reasonable sensitivities","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"But if we want to perturb u0 and p in a gradient calculation then we can do forward-mode:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"function sum_of_solution(x)\n    _prob = remake(prob,u0=x[1:2],p=x[3:end])\n    sum(solve(_prob,Tsit5(),rtol=1e-6,atol=1e-6,saveat=0.1))\nend\ndx = ForwardDiff.gradient(sum_of_solution,[u0;p])","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"or reverse-mode:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"function sum_of_solution(u0,p)\n  _prob = remake(prob,u0=u0,p=p)\n  sum(solve(_prob,Tsit5(),rtol=1e-6,atol=1e-6,saveat=0.1,sensealg=QuadratureAdjoint()))\nend\ndu01,dp1 = Zygote.gradient(sum_of_solution,u0,p)","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"Or we can use the u0 and p keyword argument short hands to tell it to replace u0 and p by the inputs:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"du01,dp1 = Zygote.gradient((u0,p)->sum(solve(prob,Tsit5(),u0=u0,p=p,saveat=0.1,sensealg=QuadratureAdjoint())),u0,p)","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"Here this computes the derivative of the output with respect to the initial condition and the the derivative with respect to the parameters respectively using the QuadratureAdjoint().","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"When Zygote.jl is used in a larger context, these gradients are implicitly calculated and utilized. For example, the Flux.jl deep learning package uses Zygote.jl in its training loop, so if we use solve in a likelihood of a Flux training loop then the derivative choice we make will be used in the optimization:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"using Flux, Plots\n\np = [2.2, 1.0, 2.0, 0.4] # Initial Parameter Vector\nfunction predict_adjoint() # Our 1-layer neural network\n  Array(solve(prob,Tsit5(),rtol=1e-6,atol=1e-6,p=p,saveat=0.0:0.1:10.0,sensealg=BacksolveAdjoint())) # Concretize to a matrix\nend\nloss_adjoint() = sum(abs2,x-1 for x in predict_adjoint())\n\ndata = Iterators.repeated((), 100)\nopt = ADAM(0.1)\ncb = function () #callback function to observe training\n  display(loss_adjoint())\n  # using `remake` to re-create our `prob` with current parameters `p`\n  display(plot(solve(remake(prob,p=p),Tsit5(),saveat=0.0:0.1:10.0),ylim=(0,6)))\nend\n\n# Display the ODE with the initial parameter values.\ncb()\n\nFlux.train!(loss_adjoint, Flux.params(p), data, opt, cb = cb)","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"This optimizes the parameters from a starting point p where the gradients are calculated using the BacksolveAdjoint method.","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"Using this technique, we can define and mix neural networks into the differential equation:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"using DiffEqFlux, Flux, OrdinaryDiffEq, DiffEqSensitivity\n\nu0 = Float32[0.0; 1.1]\ntspan = (0.0f0,1.0f0)\n\nann = Chain(Dense(2,10,tanh), Dense(10,1))\n\np1,re = Flux.destructure(ann)\np2 = Float32[-0.5,-0.5]\np3 = [p1;p2]\nps = Flux.params(p3,u0)\n\nfunction dudt_(du,u,p,t)\n    x, y = u\n    du[1] = re(p[1:41])(u)[1]\n    du[2] = p[end-1]*y + p[end]*x\nend\nprob = ODEProblem(dudt_,u0,tspan,p3)\n\nfunction predict_adjoint()\n  Array(solve(prob,Tsit5(),u0=u0,p=p3,saveat=0.0:0.1:1.0,abstol=1e-8,\n              reltol=1e-6,sensealg=InterpolatingAdjoint(checkpointing=true)))\n  # ^ wrapped this in Array as done in the previous example\nend\nloss_adjoint() = sum(abs2,x-1 for x in predict_adjoint())\n\ndata = Iterators.repeated((), 100)\nopt = ADAM(0.1)\ncb = function ()\n  display(loss_adjoint())\n  #display(plot(solve(remake(prob,p=p3,u0=u0),Tsit5(),saveat=0.1),ylim=(0,6)))\nend\n\n# Display the ODE with the current parameter values.\ncb()\n\nFlux.train!(loss_adjoint, ps, data, opt, cb = cb)","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"For more details and helper function for using DifferentialEquations.jl with neural networks, see the DiffEqFlux.jl repository.","category":"page"},{"location":"analysis/sensitivity/#Sensitivity-Algorithms","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Sensitivity Algorithms","text":"","category":"section"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"The following algorithm choices exist for sensealg. See the sensitivity mathematics page for more details on the definition of the methods.","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"ForwardSensitivity(;ADKwargs...): An implementation of continuous forward sensitivity analysis for propagating derivatives by solving the extended ODE. Only supports ODEs.\nForwardDiffSensitivity(;chunk_size=0,convert_tspan=true): An implementation of discrete forward sensitivity analysis through ForwardDiff.jl. This algorithm can differentiate code with callbacks when convert_tspan=true, but will be faster when convert_tspan=false.\nBacksolveAdjoint(;checkpointing=true,ADKwargs...): An implementation of adjoint sensitivity analysis using a backwards solution of the ODE. By default this algorithm will use the values from the forward pass to perturb the backwards solution to the correct spot, allowing reduced memory with stabilization. Only supports ODEs and SDEs.\nInterpolatingAdjoint(;checkpointing=false;ADKwargs...): The default. An implementation of adjoint sensitivity analysis which uses the interpolation of the forward solution for the reverse solve vector-Jacobian products. By default it requires a dense solution of the forward pass and will internally ignore saving arguments during the gradient calculation. When checkpointing is enabled it will only require the memory to interpolate between checkpoints. Only supports ODEs and SDEs.\nQuadratureAdjoint(;abstol=1e-6,reltol=1e-3,compile=false,ADKwargs...): An implementation of adjoint sensitivity analysis which develops a full continuous solution of the reverse solve in order to perform a post-ODE quadrature. This method requires the the dense solution and will ignore saving arguments during the gradient calculation. The tolerances in the constructor control the inner quadrature. The inner quadrature uses a ReverseDiff vjp if autojacvec, and compile=false by default but can compile the tape under the same circumstances as ReverseDiffVJP. Only supports ODEs.\nReverseDiffAdjoint(): An implementation of discrete adjoint sensitivity analysis using the ReverseDiff.jl tracing-based AD. Supports in-place functions through an Array of Structs formulation, and supports out of place through struct of arrays.\nTrackerAdjoint(): An implementation of discrete adjoint sensitivity analysis using the Tracker.jl tracing-based AD. Supports in-place functions through an Array of Structs formulation, and supports out of place through struct of arrays.\nZygoteAdjoint(): An implementation of discrete adjoint sensitivity analysis using the Zygote.jl source-to-source AD directly on the differential equation solver. Currently fails.\nSensitivityADPassThrough(): Ignores all adjoint definitions and proceeds to do standard AD through the solve functions.\nForwardLSS(), AdjointLSS(), and NILSS(nseg,nstep): Implementation of shadowing methods for chaotic systems with a long-time averaged objective. See the sensitivity analysis for chaotic systems (shadowing methods) section for more details.","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"The ReverseDiffAdjoint(), TrackerAdjoint(), ZygoteAdjoint(), and SensitivityADPassThrough() algorithms all offer differentiate-through-the-solver adjoints, each based on their respective automatic differentiation packages. If you're not sure which to use, ReverseDiffAdjoint() is generally a stable and performant best if using the CPU, while TrackerAdjoint() is required if you need GPUs. Note that SensitivityADPassThrough() is more or less an internal implementation detail. For example, ReverseDiffAdjoint() is implemented by invoking ReverseDiff's AD functionality on solve(...; sensealg=SensitivityADPassThrough()).","category":"page"},{"location":"analysis/sensitivity/#Internal-Automatic-Differentiation-Options-(ADKwargs)","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Internal Automatic Differentiation Options (ADKwargs)","text":"","category":"section"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"Many sensitivity algorithms share the same options for controlling internal use of automatic differentiation. The following arguments constitute the ADKwargs:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"autodiff: Use automatic differentiation in the internal sensitivity algorithm computations. Default is true.\nchunk_size: Chunk size for forward mode differentiation if full Jacobians are built (autojacvec=false and autodiff=true). Default is 0 for automatic choice of chunk size.\nautojacvec: Calculate the Jacobian-vector (forward sensitivity) or vector-Jacobian (adjoint sensitivity analysis) product via automatic differentiation with special seeding. For adjoint methods this option requires autodiff=true. If autojacvec=false, then a full Jacobian has to be computed, and this will default to using a f.jac function provided by the user from the problem of the forward pass. Otherwise, if autodiff=true and autojacvec=false then it will use forward-mode AD for the Jacobian, otherwise it will fall back to using a numerical approximation to the Jacobian. Additionally, if the method is an adjoint method, there are three choices which can be made explicitly. The default vjp choice is a polyalgorithm that uses a compiler analysis to choose the most efficient vjp for a given code.\nTrackerVJP: Uses Tracker.jl for the vjp.\nZygoteVJP: Uses Zygote.jl for the vjp.\nEnzymeVJP: Uses Enzyme.jl for the vjp. \nReverseDiffVJP(compile=false): Uses ReverseDiff.jl for the vjp. compile is a boolean for whether to precompile the tape, which should only be done if there are no branches (if or while statements) in the f function. When applicable, ReverseDiffVJP(true) is the fastest method, and then ReverseDiffVJP(false) is the second fastest, but this method is not compatible with third party libraries like Flux.jl, FFTW.jl, etc. (only linear algebra and basic mathematics is supported) so it should be swapped in only as an optimization.","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"Note that the Jacobian-vector products and vector-Jacobian products can be directly specified by the user using the performance overloads.","category":"page"},{"location":"analysis/sensitivity/#Choosing-a-Sensitivity-Algorithm","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Choosing a Sensitivity Algorithm","text":"","category":"section"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"For an analysis of which methods will be most efficient for computing the solution derivatives for a given problem, consult our analysis in this arxiv paper. A general rule of thumb is:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"ForwardDiffSensitivity is the fastest for differential equations with small numbers of parameters (<100) and can be used on any differential equation solver that is native Julia.\nAdjoint senstivity analysis is the fastest when the number of parameters is sufficiently large. There are three configurations of note. Using QuadratureAdjoint is the fastest for small systems, BacksolveAdjoint uses the least memory but on very stiff problems it may be unstable and require a lot of checkpoints, while InterpolatingAdjoint is in the middle, allowing checkpointing to control total memory use.\nThe methods which use automatic differentiation (ReverseDiffAdjoint, TrackerAdjoint, ForwardDiffSensitivity, and ZygoteAdjoint) support the full range of DifferentialEquations.jl features (SDEs, DDEs, events, etc.), but only work on native Julia solvers. The methods which utilize altered differential equation systems only work on ODEs (without events), but work on any ODE solver.\nFor non-ODEs with large numbers of parameters, TrackerAdjoint in out-of-place form may be the best performer.\nTrackerAdjoint is able to use a TrackedArray form with out-of-place functions du = f(u,p,t) but requires an Array{TrackedReal} form for f(du,u,p,t) mutating du. The latter has much more overhead, and should be avoided if possible. Thus if solving non-ODEs with lots of parameters, using TrackerAdjoint with an out-of-place definition may be the current best option.","category":"page"},{"location":"analysis/sensitivity/#Lower-Level-Sensitivity-Analysis-Interfaces","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Lower Level Sensitivity Analysis Interfaces","text":"","category":"section"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"While the high level interface is sufficient for interfacing with automatic differentiation, for example allowing compatibility with neural network libraries, in some cases one may want more control over the way the sensitivities are calculated in order to squeeze out every ounce of optimization. If you're that user, then this section of the docs is for you.","category":"page"},{"location":"analysis/sensitivity/#Local-Forward-Sensitivity-Analysis-via-ODEForwardSensitivityProblem","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Forward Sensitivity Analysis via ODEForwardSensitivityProblem","text":"","category":"section"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"Local forward sensitivity analysis gives a solution along with a timeseries of the sensitivities. Thus if one wishes to have a derivative at every possible time point, directly utilizing the ODEForwardSensitivityProblem can be more efficient.","category":"page"},{"location":"analysis/sensitivity/#ODEForwardSensitivityProblem-Syntax","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"ODEForwardSensitivityProblem Syntax","text":"","category":"section"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"ODEForwardSensitivityProblem is similar to an ODEProblem, but takes an AbstractForwardSensitivityAlgorithm that describes how to append the forward sensitivity equation calculation to the time evolution to simultaneously compute the derivative of the solution with respect to parameters.","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"ODEForwardSensitivityProblem(f::SciMLBase.AbstractODEFunction,u0,\n                             tspan,p=nothing,\n                             sensealg::AbstractForwardSensitivityAlgorithm = ForwardSensitivity();\n                             kwargs...)","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"Once constructed, this problem can be used in solve. The solution can be deconstructed into the ODE solution and sensitivities parts using the extract_local_sensitivities function, with the following dispatches:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"extract_local_sensitivities(sol, asmatrix::Val=Val(false)) # Decompose the entire time series\nextract_local_sensitivities(sol, i::Integer, asmatrix::Val=Val(false)) # Decompose sol[i]\nextract_local_sensitivities(sol, t::Union{Number,AbstractVector}, asmatrix::Val=Val(false)) # Decompose sol(t)","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"For information on the mathematics behind these calculations, consult the sensitivity math page","category":"page"},{"location":"analysis/sensitivity/#Example-using-an-ODEForwardSensitivityProblem","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Example using an ODEForwardSensitivityProblem","text":"","category":"section"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"To define a sensitivity problem, simply use the ODEForwardSensitivityProblem type instead of an ODE type. For example, we generate an ODE with the sensitivity equations attached for the Lotka-Volterra equations by:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"function f(du,u,p,t)\n  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]\n  du[2] = dy = -p[3]*u[2] + u[1]*u[2]\nend\n\np = [1.5,1.0,3.0]\nprob = ODEForwardSensitivityProblem(f,[1.0;1.0],(0.0,10.0),p)","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"This generates a problem which the ODE solvers can solve:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"sol = solve(prob,DP8())","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"Note that the solution is the standard ODE system and the sensitivity system combined. We can use the following helper functions to extract the sensitivity information:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"x,dp = extract_local_sensitivities(sol)\nx,dp = extract_local_sensitivities(sol,i)\nx,dp = extract_local_sensitivities(sol,t)","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"In each case, x is the ODE values and dp is the matrix of sensitivities The first gives the full timeseries of values and dp[i] contains the time series of the sensitivities of all components of the ODE with respect to ith parameter. The second returns the ith time step, while the third interpolates to calculate the sensitivities at time t. For example, if we do:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"x,dp = extract_local_sensitivities(sol)\nda = dp[1]","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"then da is the timeseries for fracpartial u(t)partial p. We can plot this","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"plot(sol.t,da',lw=3)","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"transposing so that the rows (the timeseries) is plotted.","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"(Image: Local Sensitivity Solution)","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"Here we see that there is a periodicity to the sensitivity which matches the periodicity of the Lotka-Volterra solutions. However, as time goes on the sensitivity increases. This matches the analysis of Wilkins in Sensitivity Analysis for Oscillating Dynamical Systems.","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"We can also quickly see that these values are equivalent to those given by automatic differentiation and numerical differentiation through the ODE solver:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"using ForwardDiff, Calculus\nfunction test_f(p)\n  prob = ODEProblem(f,eltype(p).([1.0,1.0]),eltype(p).((0.0,10.0)),p)\n  solve(prob,Vern9(),abstol=1e-14,reltol=1e-14,save_everystep=false)[end]\nend\n\np = [1.5,1.0,3.0]\nfd_res = ForwardDiff.jacobian(test_f,p)\ncalc_res = Calculus.finite_difference_jacobian(test_f,p)","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"Here we just checked the derivative at the end point.","category":"page"},{"location":"analysis/sensitivity/#Internal-representation","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Internal representation","text":"","category":"section"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"For completeness, we detail the internal representation. When using ForwardDiffSensitivity, the representation is with Dual numbers under the standard interpretation. The values for the ODE's solution at time i are the ForwardDiff.value.(sol[i]) portions, and the derivative with respect to parameter j is given by ForwardDiff.partials.(sol[i])[j].","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"When using ForwardSensitivity, the solution to the ODE are the first n components of the solution. This means we can grab the matrix of solution values like:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"x = sol[1:sol.prob.indvars,:]","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"Since each sensitivity is a vector of derivatives for each function, the sensitivities are each of size sol.prob.indvars. We can pull out the parameter sensitivities from the solution as follows:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"da = sol[sol.prob.indvars+1:sol.prob.indvars*2,:]\ndb = sol[sol.prob.indvars*2+1:sol.prob.indvars*3,:]\ndc = sol[sol.prob.indvars*3+1:sol.prob.indvars*4,:]","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"This means that da[1,i] is the derivative of the x(t) by the parameter a at time sol.t[i]. Note that all of the functionality available to ODE solutions is available in this case, including interpolations and plot recipes (the recipes will plot the expanded system).","category":"page"},{"location":"analysis/sensitivity/#Adjoint-Sensitivity-Analysis-via-adjoint_sensitivities-(Backpropogation)","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Adjoint Sensitivity Analysis via adjoint_sensitivities (Backpropogation)","text":"","category":"section"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"Adjoint sensitivity analysis is used to find the gradient of the solution with respect to some functional of the solution. In many cases this is used in an optimization problem to return the gradient with respect to some cost function. It is equivalent to \"backpropagation\" or reverse-mode automatic differentiation of a differential equation.","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"Using adjoint_sensitivities directly let's you do three things. One it can allow you to be more efficient, since the sensitivity calculation can be done directly on a cost function, avoiding the overhead of building the derivative of the full concretized solution. It can also allow you to be more efficient by directly controlling the forward solve that is then reversed over. Lastly, it allows one to define a continuous cost function on the continuous solution, instead of just at discrete data points.","category":"page"},{"location":"analysis/sensitivity/#Syntax","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Syntax","text":"","category":"section"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"There are two forms. For discrete adjoints, the form is:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"du0,dp = adjoint_sensitivities(sol,alg,dg,ts;sensealg=InterpolatingAdjoint(),\n                               checkpoints=sol.t,kwargs...)","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"where alg is the ODE algorithm to solve the adjoint problem, dg is the jump function, sensealg is the sensitivity algorithm, and ts is the time points for data. dg is given by:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"dg(out,u,p,t,i)","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"which is the in-place gradient of the cost functional g at time point ts[i] with u=u(t).","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"For continuous functionals, the form is:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"du0,dp = adjoint_sensitivities(sol,alg,g,nothing,(dgdu,dgdp);sensealg=InterpolatingAdjoint(),\n                               checkpoints=sol.t,,kwargs...)","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"for the cost functional","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"g(u,p,t)","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"with in-place gradient","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"dgdu(out,u,p,t)\ndgdp(out,u,p,t)","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"If the gradient is omitted, i.e.","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"du0,dp = adjoint_sensitivities(sol,alg,g,nothing;kwargs...)","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"then we assume dgdp is zero and dgdu will be computed automatically using ForwardDiff or finite differencing, depending on the autodiff setting in the AbstractSensitivityAlgorithm. Note that the keyword arguments are passed to the internal ODE solver for solving the adjoint problem.","category":"page"},{"location":"analysis/sensitivity/#Example-discrete-adjoints-on-a-cost-function","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Example discrete adjoints on a cost function","text":"","category":"section"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"In this example we will show solving for the adjoint sensitivities of a discrete cost functional. First let's solve the ODE and get a high quality continuous solution:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"function f(du,u,p,t)\n  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]\n  du[2] = dy = -p[3]*u[2] + u[1]*u[2]\nend\n\np = [1.5,1.0,3.0]\nprob = ODEProblem(f,[1.0;1.0],(0.0,10.0),p)\nsol = solve(prob,Vern9(),abstol=1e-10,reltol=1e-10)","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"Now let's calculate the sensitivity of the ell_2 error against 1 at evenly spaced points in time, that is:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"L(upt)=sum_i=1^nfracVert1-u(t_ip)Vert^22","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"for t_i = 05i. This is the assumption that the data is data[i]=1.0. For this function, notice we have that:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"beginaligned\ndg_1=1-u_1 \ndg_2=1-u_2 \n quad vdots\nendaligned","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"and thus:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"dg(out,u,p,t,i) = (out.=1.0.-u)","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"Also, we can omit dgdp, because the cost function doesn't dependent on p. If we had data, we'd just replace 1.0 with data[i]. To get the adjoint sensitivities, call:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"ts = 0:0.5:10\nres = adjoint_sensitivities(sol,Vern9(),dg,ts,abstol=1e-14,\n                            reltol=1e-14)","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"This is super high accuracy. As always, there's a tradeoff between accuracy and computation time. We can check this almost exactly matches the autodifferentiation and numerical differentiation results:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"using ForwardDiff,Calculus,Tracker\nfunction G(p)\n  tmp_prob = remake(prob,u0=convert.(eltype(p),prob.u0),p=p)\n  sol = solve(tmp_prob,Vern9(),abstol=1e-14,reltol=1e-14,saveat=ts,\n              sensealg=SensitivityADPassThrough())\n  A = convert(Array,sol)\n  sum(((1 .- A).^2)./2)\nend\nG([1.5,1.0,3.0])\nres2 = ForwardDiff.gradient(G,[1.5,1.0,3.0])\nres3 = Calculus.gradient(G,[1.5,1.0,3.0])\nres4 = Tracker.gradient(G,[1.5,1.0,3.0])\nres5 = ReverseDiff.gradient(G,[1.5,1.0,3.0])","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"and see this gives the same values.","category":"page"},{"location":"analysis/sensitivity/#Example-controlling-adjoint-method-choices-and-checkpointing","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Example controlling adjoint method choices and checkpointing","text":"","category":"section"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"In the previous examples, all calculations were done using the interpolating method. This maximizes speed but at a cost of requiring a dense sol. If it is not possible to hold a dense forward solution in memory, then one can use checkpointing. For example:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"ts = [0.0,0.2,0.5,0.7]\nsol = solve(prob,Vern9(),saveat=ts)","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"Creates a non-dense solution with checkpoints at [0.0,0.2,0.5,0.7]. Now we can do:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"res = adjoint_sensitivities(sol,Vern9(),dg,ts,\n                            sensealg=InterpolatingAdjoint(checkpointing=true))","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"When grabbing a Jacobian value during the backwards solution, it will no longer interpolate to get the value. Instead, it will start a forward solution at the nearest checkpoint to build local interpolants in a way that conserves memory. By default the checkpoints are at sol.t, but we can override this:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"res = adjoint_sensitivities(sol,Vern9(),dg,ts,\n                            sensealg=InterpolatingAdjoint(checkpointing=true),\n                            checkpoints = [0.0,0.5])","category":"page"},{"location":"analysis/sensitivity/#Example-continuous-adjoints-on-an-energy-functional","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Example continuous adjoints on an energy functional","text":"","category":"section"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"In this case we'd like to calculate the adjoint sensitivity of the scalar energy functional:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"G(up)=int_0^Tfracsum_i=1^nu_i^2(t)2dt","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"which is:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"g(u,p,t) = (sum(u).^2) ./ 2","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"Notice that the gradient of this function with respect to the state u is:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"function dg(out,u,p,t)\n  out[1]= u[1] + u[2]\n  out[2]= u[1] + u[2]\nend","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"To get the adjoint sensitivities, we call:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"res = adjoint_sensitivities(sol,Vern9(),g,nothing,dg,abstol=1e-8,\n                                 reltol=1e-8,iabstol=1e-8,ireltol=1e-8)","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"Notice that we can check this against autodifferentiation and numerical differentiation as follows:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"using QuadGK\nfunction G(p)\n  tmp_prob = remake(prob,p=p)\n  sol = solve(tmp_prob,Vern9(),abstol=1e-14,reltol=1e-14)\n  res,err = quadgk((t)-> (sum(sol(t)).^2)./2,0.0,10.0,atol=1e-14,rtol=1e-10)\n  res\nend\nres2 = ForwardDiff.gradient(G,[1.5,1.0,3.0])\nres3 = Calculus.gradient(G,[1.5,1.0,3.0])","category":"page"},{"location":"analysis/sensitivity/#Applicability-of-Backsolve-and-Caution","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Applicability of Backsolve and Caution","text":"","category":"section"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"When BacksolveAdjoint is applicable it is a fast method and requires the least memory. However, one must be cautious because not all ODEs are stable under backwards integration by the majority of ODE solvers. An example of such an equation is the Lorenz equation. Notice that if one solves the Lorenz equation forward and then in reverse with any adaptive time step and non-reversible integrator, then the backwards solution diverges from the forward solution. As a quick demonstration:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"using Sundials\nfunction lorenz(du,u,p,t)\n du[1] = 10.0*(u[2]-u[1])\n du[2] = u[1]*(28.0-u[3]) - u[2]\n du[3] = u[1]*u[2] - (8/3)*u[3]\nend\nu0 = [1.0;0.0;0.0]\ntspan = (0.0,100.0)\nprob = ODEProblem(lorenz,u0,tspan)\nsol = solve(prob,Tsit5(),reltol=1e-12,abstol=1e-12)\nprob2 = ODEProblem(lorenz,sol[end],(100.0,0.0))\nsol = solve(prob,Tsit5(),reltol=1e-12,abstol=1e-12)\n@show sol[end]-u0 #[-3.22091, -1.49394, 21.3435]","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"Thus one should check the stability of the backsolve on their type of problem before enabling this method. Additionally, using checkpointing with backsolve can be a low memory way to stabilize it.","category":"page"},{"location":"analysis/sensitivity/#Sensitivity-analysis-for-chaotic-systems-(shadowing-methods)(@id-shadowing_methods)","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Sensitivity analysis for chaotic systems (shadowing methods)(@id shadowing_methods)","text":"","category":"section"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"Let us define the instantaneous objective g(up) which depends on the state u and the parameter p of the differential equation. Then, if the objective is a long-time average quantity","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"langle g rangle_ = lim_T rightarrow  langle g rangle_T","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"where","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"langle g rangle_T = frac1T int_0^T g(us) textdt","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"under the assumption of ergodicity, langle g rangle_ only depends on p.","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"In the case of chaotic systems, the trajectories diverge with O(1) error and one finds that the average derivative is not the derivative of the average. \"Traditional\" forward and adjoint sensitivity methods also diverge because the tangent space diverges with a rate given by the Lyapunov exponent.","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"Shadowing methods use renormalization to accurately compute derivatives w.r.t. the long-time average quantities. The following sensealg choices exist","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"ForwardLSS(;alpha=CosWindowing(),ADKwargs...): An implementation of the forward least square shadowing method. For alpha, one can choose between two different windowing options, CosWindowing (default) and Cos2Windowing, and alpha::Number which corresponds to the weight of the time dilation term in ForwardLSS.\nAdjointLSS(;alpha=10.0,ADKwargs...): An implementation of the adjoint-mode least square shadowing method. alpha controls the weight of the time dilation term in AdjointLSS.\nNILSS(nseg, nstep; rng = Xorshifts.Xoroshiro128Plus(rand(UInt64)), ADKwargs...):   An implementation of the non-intrusive least squares shadowing method. nseg is the number of segments. nstep is the number of steps per segment.","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"Recommendation: Since the computational and memory costs of NILSS() scale with the number of positive (unstable) Lyapunov, it is typically less expensive than ForwardLSS(). AdjointLSS() is favorable for a large number of system parameters.","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"As an example, for the Lorenz system with g(u,p,t) = u[3], i.e., the z coordinate, as the instantaneous objective, we can use the direct interface by passing ForwardLSS as the sensealg:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"function lorenz!(du,u,p,t)\n  du[1] = p[1]*(u[2]-u[1])\n  du[2] = u[1]*(p[2]-u[3]) - u[2]\n  du[3] = u[1]*u[2] - p[3]*u[3]\nend\n\np = [10.0, 28.0, 8/3]\n\ntspan_init = (0.0,30.0)\ntspan_attractor = (30.0,50.0)\nu0 = rand(3)\nprob_init = ODEProblem(lorenz!,u0,tspan_init,p)\nsol_init = solve(prob_init,Tsit5())\nprob_attractor = ODEProblem(lorenz!,sol_init[end],tspan_attractor,p)\n\ng(u,p,t) = u[end]\n\nfunction G(p)\n  _prob = remake(prob_attractor,p=p)\n  _sol = solve(_prob,Vern9(),abstol=1e-14,reltol=1e-14,saveat=0.01,sensealg=ForwardLSS(alpha=10),g=g)\n  sum(getindex.(_sol.u,3))\nend\ndp1 = Zygote.gradient(p->G(p),p)","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"Alternatively, we can define the ForwardLSSProblem and solve it via shadow_forward as follows:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"lss_problem = ForwardLSSProblem(sol_attractor, ForwardLSS(alpha=10), g)\nresfw = shadow_forward(lss_problem)\n@test res ≈ dp1[1] atol=1e-10","category":"page"},{"location":"analysis/sensitivity/#Second-Order-Sensitivity-Analysis-via-second_order_sensitivities","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Second Order Sensitivity Analysis via second_order_sensitivities","text":"","category":"section"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"Second order sensitivity analysis is used for the fast calculation of Hessian matrices. Currently there are two functions available. The first, second_order_sensitivities, calculates the Hessian of the solution to a differential equation with respect to a loss function on the solution loss(sol). The second calculates Hessian-vector products, i.e. H*v, with respect to such a loss. The syntax is:","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"H  = second_order_sensitivities(loss,prob,alg,args...;kwargs...)\nHv  = second_order_sensitivity_product(loss,v,prob,alg,args...;kwargs...)","category":"page"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"These methods utilize what is known as forward-over-reverse to mix a forward-mode sensitivity analysis with an adjoint sensitivity analysis for a fast computation.","category":"page"},{"location":"analysis/sensitivity/#Example-second-order-sensitivity-analysis-calculation","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Example second order sensitivity analysis calculation","text":"","category":"section"},{"location":"analysis/sensitivity/","page":"Local Sensitivity Analysis (Automatic Differentiation)","title":"Local Sensitivity Analysis (Automatic Differentiation)","text":"using DiffEqSensitivity, OrdinaryDiffEq, ForwardDiff\nusing Test\n\nfunction lotka!(du,u,p,t)\n  du[1] = dx = p[1]*u[1] - p[2]*u[1]*u[2]\n  du[2] = dy = -p[3]*u[2] + p[4]*u[1]*u[2]\nend\n\np = [1.5,1.0,3.0,1.0]; u0 = [1.0;1.0]\nprob = ODEProblem(lotka!,u0,(0.0,10.0),p)\nloss(sol) = sum(sol)\nv = ones(4)\n\nH  = second_order_sensitivities(loss,prob,Vern9(),saveat=0.1,abstol=1e-12,reltol=1e-12)\nHv = second_order_sensitivity_product(loss,v,prob,Vern9(),saveat=0.1,abstol=1e-12,reltol=1e-12)","category":"page"},{"location":"analysis/parameter_estimation/#parameter_estimation","page":"Parameter Estimation and Bayesian Analysis","title":"Parameter Estimation and Bayesian Analysis","text":"","category":"section"},{"location":"analysis/parameter_estimation/","page":"Parameter Estimation and Bayesian Analysis","title":"Parameter Estimation and Bayesian Analysis","text":"Parameter estimation for differential equation models, also known as dynamic data analysis,  is provided by the DiffEq suite. In this introduction, we briefly present the relevant packages that facilitate parameter estimation, namely:","category":"page"},{"location":"analysis/parameter_estimation/","page":"Parameter Estimation and Bayesian Analysis","title":"Parameter Estimation and Bayesian Analysis","text":"DiffEqFlux.jl\nTuring.jl\nDataDrivenDiffEq.jl\nDiffEqParamEstim.jl\nDiffEqBayes.jl","category":"page"},{"location":"analysis/parameter_estimation/","page":"Parameter Estimation and Bayesian Analysis","title":"Parameter Estimation and Bayesian Analysis","text":"We also provide information regarding the respective strengths of these packages so that you can easily decide which one suits your needs best.","category":"page"},{"location":"analysis/parameter_estimation/#DiffEqFlux.jl","page":"Parameter Estimation and Bayesian Analysis","title":"DiffEqFlux.jl","text":"","category":"section"},{"location":"analysis/parameter_estimation/","page":"Parameter Estimation and Bayesian Analysis","title":"Parameter Estimation and Bayesian Analysis","text":"A very versatile and composable package, DiffEqFlux.jl allows for solving a wide range of differential equations, for instance: stiff universal ODEs, universal SDEs, universal PDEs, and other kinds of universal differential equations. As regards probabilistic programming, DiffEqFlux.jl works in conjunction with Turing.jl (see below). It is the most flexible and high performance parameter estimation system.","category":"page"},{"location":"analysis/parameter_estimation/#Turing.jl","page":"Parameter Estimation and Bayesian Analysis","title":"Turing.jl","text":"","category":"section"},{"location":"analysis/parameter_estimation/","page":"Parameter Estimation and Bayesian Analysis","title":"Parameter Estimation and Bayesian Analysis","text":"In the context of differential equations and parameter estimation, Turing.jl allows for a Bayesian estimation of differential equations (used in conjunction with the high-level package DiffEqBayes.jl). For more examples on combining Turing.jl with DiffEqBayes.jl, see the documentation below. It is important to note that Turing.jl can also perform Bayesian estimation without relying on DiffEqBayes.jl (for an example, consult this tutorial).","category":"page"},{"location":"analysis/parameter_estimation/#DataDrivenDiffEq.jl","page":"Parameter Estimation and Bayesian Analysis","title":"DataDrivenDiffEq.jl","text":"","category":"section"},{"location":"analysis/parameter_estimation/","page":"Parameter Estimation and Bayesian Analysis","title":"Parameter Estimation and Bayesian Analysis","text":"The distinguishing feature of this package is that its ultimate goal is to identify the differential equation model that generated the input data. Depending on the user's needs, the package can provide structural identification of a given differential equation (output in a symbolic form) or structural estimation (output as a function for prediction purposes).","category":"page"},{"location":"analysis/parameter_estimation/#DiffEqParamEstim.jl","page":"Parameter Estimation and Bayesian Analysis","title":"DiffEqParamEstim.jl","text":"","category":"section"},{"location":"analysis/parameter_estimation/","page":"Parameter Estimation and Bayesian Analysis","title":"Parameter Estimation and Bayesian Analysis","text":"This package is for simplified parameter estimation. While not as flexible of a system like DiffEqFlux.jl, it provides ready-made functions for doing standard optmization procedures like L2 fitting and MAP estimates. Among other features, it allows for the optimization of parameters in ODEs, stochastic problems, and delay differential equations.","category":"page"},{"location":"analysis/parameter_estimation/#DiffEqBayes.jl","page":"Parameter Estimation and Bayesian Analysis","title":"DiffEqBayes.jl","text":"","category":"section"},{"location":"analysis/parameter_estimation/","page":"Parameter Estimation and Bayesian Analysis","title":"Parameter Estimation and Bayesian Analysis","text":"As the name suggests, this package has been designed to provide the estimation of differential equations parameters by means of Bayesian methods. It works in conjunction with Turing.jl,  CmdStan.jl,  DynamicHMC.jl, and  ApproxBayes.jl. While not as flexible as direct usage of DiffEqFlux.jl or Turing.jl, DiffEqBayes.jl can be an approachable interface for those not familiar with Bayesian estimation, and provides a nice way to use Stan from pure Julia.","category":"page"},{"location":"types/split_ode_types/#split_ode_prob","page":"Split ODE Problems","title":"Split ODE Problems","text":"","category":"section"},{"location":"types/split_ode_types/#Mathematical-Specification-of-a-Split-ODE-Problem","page":"Split ODE Problems","title":"Mathematical Specification of a Split ODE Problem","text":"","category":"section"},{"location":"types/split_ode_types/","page":"Split ODE Problems","title":"Split ODE Problems","text":"To define a SplitODEProblem, you simply need to give a two functions functions f_1 and f_2 along with an initial condition u₀ which define an ODE:","category":"page"},{"location":"types/split_ode_types/","page":"Split ODE Problems","title":"Split ODE Problems","text":"fracdudt =  f_1(upt) + f_2(upt)","category":"page"},{"location":"types/split_ode_types/","page":"Split ODE Problems","title":"Split ODE Problems","text":"f should be specified as f(u,p,t) (or in-place as f(du,u,p,t)), and u₀ should be an AbstractArray (or number) whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher dimension tensors as well.","category":"page"},{"location":"types/split_ode_types/","page":"Split ODE Problems","title":"Split ODE Problems","text":"Many splits are at least partially linear. That is the equation:","category":"page"},{"location":"types/split_ode_types/","page":"Split ODE Problems","title":"Split ODE Problems","text":"fracdudt =  Au + f_2(upt)","category":"page"},{"location":"types/split_ode_types/","page":"Split ODE Problems","title":"Split ODE Problems","text":"For how to define a linear function A, see the documentation for the DiffEqOperators.","category":"page"},{"location":"types/split_ode_types/#Constructors","page":"Split ODE Problems","title":"Constructors","text":"","category":"section"},{"location":"types/split_ode_types/","page":"Split ODE Problems","title":"Split ODE Problems","text":"SplitODEProblem(f::SplitFunction,u0,tspan,p=NullParameters();kwargs...)\nSplitODEProblem{isinplace}(f1,f2,u0,tspan,p=NullParameters();kwargs...)","category":"page"},{"location":"types/split_ode_types/","page":"Split ODE Problems","title":"Split ODE Problems","text":"The isinplace parameter can be omitted and will be determined using the signature of f2. Note that both f1 and f2 should support the in-place style if isinplace is true or they should both support the out-of-place style if isinplace is false. You cannot mix up the two styles.","category":"page"},{"location":"types/split_ode_types/","page":"Split ODE Problems","title":"Split ODE Problems","text":"Parameters are optional, and if not given then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.","category":"page"},{"location":"types/split_ode_types/","page":"Split ODE Problems","title":"Split ODE Problems","text":"Under the hood, a SplitODEProblem is just a regular ODEProblem whose f is a SplitFunction. Therefore you can solve a SplitODEProblem using the same solvers for ODEProblem. For solvers dedicated to split problems, see Split ODE Solvers.","category":"page"},{"location":"types/split_ode_types/","page":"Split ODE Problems","title":"Split ODE Problems","text":"For specifying Jacobians and mass matrices, see the DiffEqFunctions page.","category":"page"},{"location":"types/split_ode_types/#Fields","page":"Split ODE Problems","title":"Fields","text":"","category":"section"},{"location":"types/split_ode_types/","page":"Split ODE Problems","title":"Split ODE Problems","text":"f1, f2: The functions in the ODE.\nu0: The initial condition.\ntspan: The timespan for the problem.\np: The parameters for the problem. Defaults to NullParameters\nkwargs: The keyword arguments passed onto the solves.","category":"page"},{"location":"solvers/rode_solve/#RODE-Solvers","page":"RODE Solvers","title":"RODE Solvers","text":"","category":"section"},{"location":"solvers/rode_solve/#Recommended-Methods","page":"RODE Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"solvers/rode_solve/","page":"RODE Solvers","title":"RODE Solvers","text":"Currently, the only implemented method is the RandomEM method in StochasticDiffEq.jl. It is strong order alpha for a alpha-Holder continuous noise process.","category":"page"},{"location":"solvers/rode_solve/#Full-List-of-Methods","page":"RODE Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"solvers/rode_solve/#StochasticDiffEq.jl","page":"RODE Solvers","title":"StochasticDiffEq.jl","text":"","category":"section"},{"location":"solvers/rode_solve/","page":"RODE Solvers","title":"RODE Solvers","text":"Each of the StochasticDiffEq.jl solvers come with a linear interpolation.","category":"page"},{"location":"solvers/rode_solve/","page":"RODE Solvers","title":"RODE Solvers","text":"RandomEM- The Euler-Maruyama method for RODEs. Strong order matching Holder continuity.","category":"page"},{"location":"solvers/rode_solve/","page":"RODE Solvers","title":"RODE Solvers","text":"Example usage:","category":"page"},{"location":"solvers/rode_solve/","page":"RODE Solvers","title":"RODE Solvers","text":"sol = solve(prob,RandomEM())","category":"page"},{"location":"solvers/dde_solve/#DDE-Solvers","page":"DDE Solvers","title":"DDE Solvers","text":"","category":"section"},{"location":"solvers/dde_solve/","page":"DDE Solvers","title":"DDE Solvers","text":"solve(prob::AbstractDDEProblem, alg; kwargs)","category":"page"},{"location":"solvers/dde_solve/","page":"DDE Solvers","title":"DDE Solvers","text":"Solves the DDE defined by prob using the algorithm alg. If no algorithm is given, a default algorithm will be chosen.","category":"page"},{"location":"solvers/dde_solve/#Recommended-Methods","page":"DDE Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"solvers/dde_solve/","page":"DDE Solvers","title":"DDE Solvers","text":"The recommended method for DDE problems are the MethodOfSteps algorithms. These are constructed from an OrdinaryDiffEq.jl algorithm as follows:","category":"page"},{"location":"solvers/dde_solve/","page":"DDE Solvers","title":"DDE Solvers","text":"MethodOfSteps(alg; constrained=false, fpsolve=NLFunctional(; max_iter=10))","category":"page"},{"location":"solvers/dde_solve/","page":"DDE Solvers","title":"DDE Solvers","text":"where alg is an OrdinaryDiffEq.jl algorithm. Most algorithms should work.","category":"page"},{"location":"solvers/dde_solve/#Nonstiff-DDEs","page":"DDE Solvers","title":"Nonstiff DDEs","text":"","category":"section"},{"location":"solvers/dde_solve/","page":"DDE Solvers","title":"DDE Solvers","text":"The standard algorithm choice is MethodOfSteps(Tsit5()). This is a highly efficient FSAL 5th order algorithm with free interpolants which should handle most problems. For fast solving where non-strict error control is needed, choosing MethodOfSteps(BS3()) can do well. Using BS3 is similar to the MATLAB dde23. For algorithms where strict error control is needed, it is recommended that one uses MethodOfSteps(Vern6()). Benchmarks show that going to higher order methods like MethodOfSteps(DP8()) may not be beneficial.","category":"page"},{"location":"solvers/dde_solve/#Stiff-DDEs-and-Differential-Algebraic-Delay-Equations-(DADEs)","page":"DDE Solvers","title":"Stiff DDEs and Differential-Algebraic Delay Equations (DADEs)","text":"","category":"section"},{"location":"solvers/dde_solve/","page":"DDE Solvers","title":"DDE Solvers","text":"For stiff DDEs, the SDIRK and Rosenbrock methods are very efficient as they will reuse the Jacobian in the unconstrained stepping iterations. One should choose from the methods which have stiff-aware interpolants for better stability. MethodOfSteps(Rosenbrock23()) is a good low order method choice. Additionally, the Rodas methods like MethodOfSteps(Rodas4()) are good choices because of their higher order stiff-aware interpolant.","category":"page"},{"location":"solvers/dde_solve/","page":"DDE Solvers","title":"DDE Solvers","text":"Additionally, DADEs can be solved by specifying the problem in mass matrix form. The Rosenbrock methods are good choices in these situations.","category":"page"},{"location":"solvers/dde_solve/#Lag-Handling","page":"DDE Solvers","title":"Lag Handling","text":"","category":"section"},{"location":"solvers/dde_solve/","page":"DDE Solvers","title":"DDE Solvers","text":"Lags are declared separately from their use. One can use any lag by simply using the interpolant of h at that point. However, one should use caution in order to achieve the best accuracy. When lags are declared, the solvers can more efficiently be more accurate. Constant delays are propagated until the order is higher than the order of the integrator. If state-dependent delays are declared, the algorithm will detect discontinuities arising from these delays and adjust the step size such that these discontinuities are included in the mesh, if steps are rejected. This way, all discontinuities are treated exactly.","category":"page"},{"location":"solvers/dde_solve/","page":"DDE Solvers","title":"DDE Solvers","text":"If there are undeclared lags, the discontinuities due to delays are not tracked. In this case, one should only use residual control methods like MethodOfSteps(RK4()), which is the current best choice, as these will step more accurately. Still, residual control is an error-prone method. We recommend setting the tolerances lower in order to get accurate results, though this may be costly since it will use a rejection-based approach to adapt to the delay discontinuities.","category":"page"},{"location":"solvers/dde_solve/#Special-Keyword-Arguments","page":"DDE Solvers","title":"Special Keyword Arguments","text":"","category":"section"},{"location":"solvers/dde_solve/","page":"DDE Solvers","title":"DDE Solvers","text":"discontinuity_interp_points - Number of interpolation points used to track discontinuities arising from dependent delays. Defaults to 10. Only relevant if dependent delays are declared.\ndiscontinuity_abstol and discontinuity_reltol - These are absolute and relative tolerances used by the check whether the time point at the beginning of the current step is a discontinuity arising from dependent delays. Defaults to 1/10^12 and 0. Only relevant if dependent delays are declared.","category":"page"},{"location":"solvers/dde_solve/#Note","page":"DDE Solvers","title":"Note","text":"","category":"section"},{"location":"solvers/dde_solve/","page":"DDE Solvers","title":"DDE Solvers","text":"If the method is having trouble, one may want to adjust the fixed-point iteration. Decreasing the absolute tolerance and the relative tolerance by specifying the keyword arguments abstol and reltol when solving the DDE problem, and increasing the maximal number of iterations by specifying the keyword argument max_iter in the MethodOfSteps algorithm, can help ensure that the steps are correct. If the problem still is not correctly converging, one should lower dtmax. For problems with only constant delays, in the worst case scenario, one may need to set constrained = true which will constrain timesteps to at most the size of the minimal lag and hence forces more stability at the cost of smaller timesteps.","category":"page"},{"location":"types/sdde_types/#sdde_prob","page":"SDDE Problems","title":"SDDE Problems","text":"","category":"section"},{"location":"types/sdde_types/#Mathematical-Specification-of-a-Stochastic-Delay-Differential-Equation-(SDDE)-Problem","page":"SDDE Problems","title":"Mathematical Specification of a Stochastic Delay Differential Equation (SDDE) Problem","text":"","category":"section"},{"location":"types/sdde_types/","page":"SDDE Problems","title":"SDDE Problems","text":"To define a SDDE Problem, you simply need to give the drift function f, the diffusion function g, the initial condition u_0 at time point t_0, and the history function h which together define a SDDE:","category":"page"},{"location":"types/sdde_types/","page":"SDDE Problems","title":"SDDE Problems","text":"du = f(uhpt)dt + g(uhpt)dW_t qquad (t geq t_0)","category":"page"},{"location":"types/sdde_types/","page":"SDDE Problems","title":"SDDE Problems","text":"u(t_0) = u_0","category":"page"},{"location":"types/sdde_types/","page":"SDDE Problems","title":"SDDE Problems","text":"u(t) = h(t) qquad (t  t_0)","category":"page"},{"location":"types/sdde_types/","page":"SDDE Problems","title":"SDDE Problems","text":"f should be specified as f(u, h, p, t) (or in-place as f(du, u, h, p, t)) (and g should match). u_0 should be an AbstractArray (or number) whose geometry matches the desired geometry of u, and h should be specified as described below. The history function h is accessed for all delayed values. Note that we are not limited to numbers or vectors for u_0; one is allowed to provide u_0 as arbitrary matrices / higher dimension tensors as well.","category":"page"},{"location":"types/sdde_types/","page":"SDDE Problems","title":"SDDE Problems","text":"Note that this functionality should be considered experimental.","category":"page"},{"location":"types/sdde_types/#Functional-Forms-of-the-History-Function","page":"SDDE Problems","title":"Functional Forms of the History Function","text":"","category":"section"},{"location":"types/sdde_types/","page":"SDDE Problems","title":"SDDE Problems","text":"The history function h can be called in the following ways:","category":"page"},{"location":"types/sdde_types/","page":"SDDE Problems","title":"SDDE Problems","text":"h(p, t): out-of-place calculation\nh(out, p, t): in-place calculation\nh(p, t, deriv::Type{Val{i}}): out-of-place calculation of the ith derivative\nh(out, p, t, deriv::Type{Val{i}}): in-place calculation of the ith derivative\nh(args...; idxs): calculation of h(args...) for indices idxs","category":"page"},{"location":"types/sdde_types/","page":"SDDE Problems","title":"SDDE Problems","text":"Note that a dispatch for the supplied history function of matching form is required for whichever function forms are used in the user derivative function f.","category":"page"},{"location":"types/sdde_types/#Declaring-Lags","page":"SDDE Problems","title":"Declaring Lags","text":"","category":"section"},{"location":"types/sdde_types/","page":"SDDE Problems","title":"SDDE Problems","text":"Lags are declared separately from their use. One can use any lag by simply using the interpolant of h at that point. However, one should use caution in order to achieve the best accuracy. When lags are declared, the solvers can more efficiently be more accurate and thus this is recommended.","category":"page"},{"location":"types/sdde_types/#Neutral,-Retarded,-and-Algebraic-Stochastic-Delay-Differential-Equations","page":"SDDE Problems","title":"Neutral, Retarded, and Algebraic Stochastic Delay Differential Equations","text":"","category":"section"},{"location":"types/sdde_types/","page":"SDDE Problems","title":"SDDE Problems","text":"Note that the history function specification can be used to specify general retarded arguments, i.e. h(p,α(u,t)). Neutral delay differential equations can be specified by using the deriv value in the history interpolation. For example, h(p,t-τ, Val{1}) returns the first derivative of the history values at time t-τ.","category":"page"},{"location":"types/sdde_types/","page":"SDDE Problems","title":"SDDE Problems","text":"Note that algebraic equations can be specified by using a singular mass matrix.","category":"page"},{"location":"types/sdde_types/#Problem-Type","page":"SDDE Problems","title":"Problem Type","text":"","category":"section"},{"location":"types/sdde_types/#Constructors","page":"SDDE Problems","title":"Constructors","text":"","category":"section"},{"location":"types/sdde_types/","page":"SDDE Problems","title":"SDDE Problems","text":"SDDEProblem(f,g[, u0], h, tspan[, p]; <keyword arguments>)\nSDDEProblem{isinplace}(f,g[, u0], h, tspan[, p]; <keyword arguments>)","category":"page"},{"location":"types/sdde_types/","page":"SDDE Problems","title":"SDDE Problems","text":"Parameter isinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred.","category":"page"},{"location":"types/sdde_types/","page":"SDDE Problems","title":"SDDE Problems","text":"Parameters are optional, and if not given then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.","category":"page"},{"location":"types/sdde_types/","page":"SDDE Problems","title":"SDDE Problems","text":"For specifying Jacobians and mass matrices, see the DiffEqFunctions page.","category":"page"},{"location":"types/sdde_types/#Arguments","page":"SDDE Problems","title":"Arguments","text":"","category":"section"},{"location":"types/sdde_types/","page":"SDDE Problems","title":"SDDE Problems","text":"f: The drift function in the SDDE.\ng: The diffusion function in the SDDE.\nu0: The initial condition. Defaults to the value h(p, first(tspan)) of the history function evaluated at the initial time point.\nh: The history function for the DDE before t0.\ntspan: The timespan for the problem.\np: The parameters with which function f is called. Defaults to NullParameters.\nconstant_lags: A collection of constant lags used by the history function h. Defaults to ().\ndependent_lags A tuple of functions (u, p, t) -> lag for the state-dependent lags used by the history function h. Defaults to ().\nneutral: If the DDE is neutral, i.e., if delays appear in derivative terms.\norder_discontinuity_t0: The order of the discontinuity at the initial time point. Defaults to 0 if an initial condition u0 is provided. Otherwise it is forced to be greater or equal than 1.\nkwargs: The keyword arguments passed onto the solves.","category":"page"},{"location":"types/sdae_types/#SDAE-Problems","page":"SDAE Problems","title":"SDAE Problems","text":"","category":"section"},{"location":"types/sdae_types/#Mathematical-Specification-of-a-Stochastic-Differential-Algebraic-Equation-(SDAE)-Problem","page":"SDAE Problems","title":"Mathematical Specification of a Stochastic Differential-Algebraic Equation (SDAE) Problem","text":"","category":"section"},{"location":"types/sdae_types/","page":"SDAE Problems","title":"SDAE Problems","text":"To define an SDAE, you simply define an SDE Problem with the forcing function f, the noise function g, a mass matrix M and the initial condition u₀ which define the SDAE in mass matrix form:","category":"page"},{"location":"types/sdae_types/","page":"SDAE Problems","title":"SDAE Problems","text":"M du = f(upt)dt + Σgᵢ(upt)dWⁱ","category":"page"},{"location":"types/sdae_types/","page":"SDAE Problems","title":"SDAE Problems","text":"f and g should be specified as f(u,p,t) and  g(u,p,t) respectively, and u₀ should be an AbstractArray whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher dimension tensors as well. A vector of gs can also be defined to determine an SDE of higher Ito dimension.","category":"page"},{"location":"types/sdae_types/","page":"SDAE Problems","title":"SDAE Problems","text":"Nonsingular mass matrices correspond to constraint equations and thus a stochastic DAE.","category":"page"},{"location":"types/sdae_types/#Example","page":"SDAE Problems","title":"Example","text":"","category":"section"},{"location":"types/sdae_types/","page":"SDAE Problems","title":"SDAE Problems","text":"const mm_A = [-2.0 1 4\n            4 -2 1\n            0 0 0]\nfunction f!(du,u,p,t)\n  du[1] = u[1]\n  du[2] = u[2]\n  du[3] = u[1] + u[2] + u[3] - 1\nend\n\nfunction g!(du,u,p,t)\n  @. du = 0.1\nend\n\nprob = SDEProblem(SDEFunction(f!,g!;mass_matrix=mm_A),g!,\n                  ones(3),(0.0,1.0))","category":"page"},{"location":"tutorials/additional/#Additional-Tutorials","page":"Additional Tutorials","title":"Additional Tutorials","text":"","category":"section"},{"location":"tutorials/additional/","page":"Additional Tutorials","title":"Additional Tutorials","text":"Additional tutorials can be found at DiffEqTutorials.jl. These include interactive introductions, optimizing code, modeling examples, and deeper examples for extra features.","category":"page"},{"location":"types/rode_types/#RODE-Problems","page":"RODE Problems","title":"RODE Problems","text":"","category":"section"},{"location":"types/rode_types/#Mathematical-Specification-of-a-RODE-Problem","page":"RODE Problems","title":"Mathematical Specification of a RODE Problem","text":"","category":"section"},{"location":"types/rode_types/","page":"RODE Problems","title":"RODE Problems","text":"To define a RODE Problem, you simply need to give the function f and the initial condition u₀ which define an ODE:","category":"page"},{"location":"types/rode_types/","page":"RODE Problems","title":"RODE Problems","text":"fracdudt = f(uptW(t))","category":"page"},{"location":"types/rode_types/","page":"RODE Problems","title":"RODE Problems","text":"where W(t) is a random process. f should be specified as f(u,p,t,W) (or in-place as f(du,u,p,t,W)), and u₀ should be an AbstractArray (or number) whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher dimension tensors as well.","category":"page"},{"location":"types/rode_types/#Constructors","page":"RODE Problems","title":"Constructors","text":"","category":"section"},{"location":"types/rode_types/","page":"RODE Problems","title":"RODE Problems","text":"RODEProblem(f::RODEFunction,u0,tspan,p=NullParameters();noise=WHITE_NOISE,rand_prototype=nothing,callback=nothing)\nRODEProblem{isinplace}(f,u0,tspan,p=NullParameters();noise=WHITE_NOISE,rand_prototype=nothing,callback=nothing,mass_matrix=I) : Defines the RODE with the specified functions. The default noise is WHITE_NOISE. isinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred.","category":"page"},{"location":"types/rode_types/","page":"RODE Problems","title":"RODE Problems","text":"Parameters are optional, and if not given then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.","category":"page"},{"location":"types/rode_types/","page":"RODE Problems","title":"RODE Problems","text":"For specifying Jacobians and mass matrices, see the DiffEqFunctions page.","category":"page"},{"location":"types/rode_types/#Fields","page":"RODE Problems","title":"Fields","text":"","category":"section"},{"location":"types/rode_types/","page":"RODE Problems","title":"RODE Problems","text":"f: The drift function in the SDE.\nu0: The initial condition.\ntspan: The timespan for the problem.\np: The optional parameters for the problem. Defaults to NullParameters.\nnoise: The noise process applied to the noise upon generation. Defaults to Gaussian white noise. For information on defining different noise processes, see the noise process documentation page\nrand_prototype: A prototype type instance for the noise vector. It defaults to nothing, which means the problem should be interpreted as having a noise vector whose size matches u0.\nkwargs: The keyword arguments passed onto the solves.","category":"page"},{"location":"features/callback_functions/#callbacks","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"","category":"section"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"DifferentialEquations.jl allows for using callback functions to inject user code into the solver algorithms. It allows for safely and accurately applying events and discontinuities. Multiple callbacks can be chained together, and these callback types can be used to build libraries of extension behavior.","category":"page"},{"location":"features/callback_functions/#The-Callback-Types","page":"Event Handling and Callback Functions","title":"The Callback Types","text":"","category":"section"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"The callback types are defined as follows. There are three primitive callback types: the ContinuousCallback, DiscreteCallback and the VectorContinuousCallback: ","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"The ContinuousCallback is applied when a given continuous condition function hits zero. This hitting can happen even within an integration step and the solver must be able to detect it and adjust the integration step accordingly. This type of callback implements what is known in other problem solving environments as an Event. \nThe DiscreteCallback is applied when its condition function is true, but the condition is only evaluated at the end of every integration step. \nThe VectorContinuousCallback works like a vector of ContinuousCallbacks and lets the user specify which callback is called when.","category":"page"},{"location":"features/callback_functions/#ContinuousCallback","page":"Event Handling and Callback Functions","title":"ContinuousCallback","text":"","category":"section"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"ContinuousCallback","category":"page"},{"location":"features/callback_functions/#DiffEqBase.ContinuousCallback","page":"Event Handling and Callback Functions","title":"DiffEqBase.ContinuousCallback","text":"ContinuousCallback(condition,affect!,affect_neg!;\n                   initialize = INITIALIZE_DEFAULT,\n                   finalize = FINALIZE_DEFAULT,\n                   idxs = nothing,\n                   rootfind=LeftRootFind,\n                   save_positions=(true,true),\n                   interp_points=10,\n                   abstol=10eps(),reltol=0,repeat_nudge=1//100)\n\nfunction ContinuousCallback(condition,affect!;\n                   initialize = INITIALIZE_DEFAULT,\n                   finalize = FINALIZE_DEFAULT,\n                   idxs = nothing,\n                   rootfind=LeftRootFind,\n                   save_positions=(true,true),\n                   affect_neg! = affect!,\n                   interp_points=10,\n                   abstol=10eps(),reltol=0,repeat_nudge=1//100)\n\nContains a single callback whose condition is a continuous function. The callback is triggered when this function evaluates to 0.\n\nArguments\n\ncondition: This is a function condition(u,t,integrator) for declaring when the callback should be used. A callback is initiated if the condition hits 0 within the time interval. See the Integrator Interface documentation for information about integrator.\naffect!: This is the function affect!(integrator) where one is allowed to modify the current state of the integrator. If you do not pass an affect_neg! function, it is called when condition is found to be 0 (at a root) and the cross is either an upcrossing (from negative to positive) or a downcrossing (from positive to negative). You need to explicitly pass nothing as the affect_neg! argument if it should only be called at upcrossings, e.g. ContinuousCallback(condition, affect!, nothing). For more information on what can be done, see the Integrator Interface manual page. Modifications to u are safe in this function.\naffect_neg!=affect!: This is the function affect_neg!(integrator) where one is allowed to modify the current state of the integrator. This is called when condition is found to be 0 (at a root) and the cross is an downcrossing (from positive to negative). For more information on what can be done, see the Integrator Interface manual page. Modifications to u are safe in this function.\nrootfind=LeftRootFind: This is a flag to specify the type of rootfinding to do for finding event location. If this is set to LeftRootfind, the solution will be backtracked to the point where condition==0 and if the solution isn't exact, the left limit of root is used. If set to RightRootFind, the solution would be set to the right limit of the root. Otherwise the systems and the affect! will occur at t+dt.\ninterp_points=10: The number of interpolated points to check the condition. The condition is found by checking whether any interpolation point / endpoint has a different sign. If interp_points=0, then conditions will only be noticed if the sign of condition is different at t than at t+dt. This behavior is not robust when the solution is oscillatory, and thus it's recommended that one use some interpolation points (they're cheap to compute!). 0 within the time interval.\nsave_positions=(true,true): Boolean tuple for whether to save before and after the affect!. This saving will occur just before and after the event, only at event times, and does not depend on options like saveat, save_everystep, etc. (i.e. if saveat=[1.0,2.0,3.0], this can still add a save point at 2.1 if true). For discontinuous changes like a modification to u to be handled correctly (without error), one should set save_positions=(true,true).\nidxs=nothing: The components which will be interpolated into the condition. Defaults to nothing which means u will be all components.\ninitialize: This is a function (c,u,t,integrator) which can be used to initialize the state of the callback c. It should modify the argument c and the return is ignored.\nfinalize: This is a function (c,u,t,integrator) which can be used to finalize the state of the callback c. It can modify the argument c and the return is ignored.\nabstol=1e-14 & reltol=0: These are used to specify a tolerance from zero for the rootfinder: if the starting condition is less than the tolerance from zero, then no root will be detected. This is to stop repeat events happening just after a previously rootfound event.\nrepeat_nudge = 1//100: This is used to set the next testing point after a previously found zero. Defaults to 1//100, which means after a callback the next sign check will take place at t + dt*1//100 instead of at t to avoid repeats.\n\n\n\n\n\n","category":"type"},{"location":"features/callback_functions/#discrete_callback","page":"Event Handling and Callback Functions","title":"DiscreteCallback","text":"","category":"section"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"DiscreteCallback","category":"page"},{"location":"features/callback_functions/#DiffEqBase.DiscreteCallback","page":"Event Handling and Callback Functions","title":"DiffEqBase.DiscreteCallback","text":"DiscreteCallback(condition,affect!;\n                 initialize = INITIALIZE_DEFAULT,\n                 finalize = FINALIZE_DEFAULT,\n                 save_positions=(true,true))\n\nArguments\n\ncondition: This is a function condition(u,t,integrator) for declaring when the callback should be used. A callback is initiated if the condition evaluates to true. See the Integrator Interface documentation for information about integrator.\naffect!: This is the function affect!(integrator) where one is allowed to\nmodify the current state of the integrator. For more information on what can be done, see the Integrator Interface manual page.\nsave_positions: Boolean tuple for whether to save before and after the affect!. This saving will occur just before and after the event, only at event times, and does not depend on options like saveat, save_everystep, etc. (i.e. if saveat=[1.0,2.0,3.0], this can still add a save point at 2.1 if true). For discontinuous changes like a modification to u to be handled correctly (without error), one should set save_positions=(true,true).\ninitialize: This is a function (c,u,t,integrator) which can be used to initialize the state of the callback c. It should modify the argument c and the return is ignored.\nfinalize: This is a function (c,u,t,integrator) which can be used to finalize the state of the callback c. It should can the argument c and the return is ignored.\n\n\n\n\n\n","category":"type"},{"location":"features/callback_functions/#CallbackSet","page":"Event Handling and Callback Functions","title":"CallbackSet","text":"","category":"section"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"CallbackSet","category":"page"},{"location":"features/callback_functions/#DiffEqBase.CallbackSet","page":"Event Handling and Callback Functions","title":"DiffEqBase.CallbackSet","text":"struct CallbackSet{T1<:Tuple, T2<:Tuple} <: SciMLBase.DECallback\n\nMultiple callbacks can be chained together to form a CallbackSet. A CallbackSet is constructed by passing the constructor ContinuousCallback, DiscreteCallback, VectorContinuousCallback or other CallbackSet instances:\n\nCallbackSet(cb1,cb2,cb3)\n\nYou can pass as many callbacks as you like. When the solvers encounter multiple callbacks, the following rules apply:\n\nContinuousCallbacks and VectorContinuousCallbacks are applied before DiscreteCallbacks. (This is because they often implement event-finding that will backtrack the timestep to smaller than dt).\nFor ContinuousCallbacks and VectorContinuousCallbacks, the event times are found by rootfinding and only the first ContinuousCallback or VectorContinuousCallback affect is applied.\nThe DiscreteCallbacks are then applied in order. Note that the ordering only matters for the conditions: if a previous callback modifies u in such a way that the next callback no longer evaluates condition to true, its affect will not be applied.\n\n\n\n\n\n","category":"type"},{"location":"features/callback_functions/#VectorContinuousCallback","page":"Event Handling and Callback Functions","title":"VectorContinuousCallback","text":"","category":"section"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"VectorContinuousCallback","category":"page"},{"location":"features/callback_functions/#DiffEqBase.VectorContinuousCallback","page":"Event Handling and Callback Functions","title":"DiffEqBase.VectorContinuousCallback","text":"VectorContinuousCallback(condition,affect!,affect_neg!,len;\n                         initialize = INITIALIZE_DEFAULT,\n                         finalize = FINALIZE_DEFAULT,\n                         idxs = nothing,\n                         rootfind=LeftRootFind,\n                         save_positions=(true,true),\n                         interp_points=10,\n                         abstol=10eps(),reltol=0,repeat_nudge = 1//100)\n\nVectorContinuousCallback(condition,affect!,len;\n                   initialize = INITIALIZE_DEFAULT,\n                   finalize = FINALIZE_DEFAULT,\n                   idxs = nothing,\n                   rootfind=LeftRootFind,\n                   save_positions=(true,true),\n                   affect_neg! = affect!,\n                   interp_points=10,\n                   abstol=10eps(),reltol=0,repeat_nudge=1//100)\n\nThis is also a subtype of AbstractContinuousCallback. CallbackSet is not feasible when you have a large number of callbacks, as it doesn't scale well. For this reason, we have VectorContinuousCallback - it allows you to have a single callback for multiple events.\n\nArguments\n\ncondition: This is a function condition(out, u, t, integrator) which should save the condition value in the array out  at the right index. Maximum index of out should be specified in the len property of callback. So this way you can have  a chain of len events, which would cause the ith event to trigger when out[i] = 0.\naffect!: This is a function affect!(integrator, event_index) which lets you modify integrator and it tells you about  which event occured using event_idx i.e. gives you index i for which out[i] came out to be zero.\nlen: Number of callbacks chained. This is compulsory to be specified.\n\nRest of the arguments have the same meaning as in ContinuousCallback.\n\n\n\n\n\n","category":"type"},{"location":"features/callback_functions/#Using-Callbacks","page":"Event Handling and Callback Functions","title":"Using Callbacks","text":"","category":"section"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"The callback type is then sent to the solver (or the integrator) via the callback keyword argument:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"sol = solve(prob,alg,callback=cb)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"You can supply nothing, a single DiscreteCallback or ContinuousCallback or VectorContinuousCallback, or a CallbackSet.","category":"page"},{"location":"features/callback_functions/#Note-About-Saving","page":"Event Handling and Callback Functions","title":"Note About Saving","text":"","category":"section"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"When a callback is supplied, the default saving behavior is turned off. This is because otherwise events would \"double save\" one of the values. To re-enable the standard saving behavior, one must have the first save_positions value be true for at least one callback.","category":"page"},{"location":"features/callback_functions/#Modifying-the-Stepping-Within-A-Callback","page":"Event Handling and Callback Functions","title":"Modifying the Stepping Within A Callback","text":"","category":"section"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"A common issue with callbacks is that they cause a large discontinuous change, and so it may be wise to pull down dt after such a change. To control the timestepping from a callback, please see the timestepping controls in the integrator interface. Specifically, set_proposed_dt! is used to set the next stepsize, and terminate! can be used to cause the simulation to stop.","category":"page"},{"location":"features/callback_functions/#DiscreteCallback-Examples","page":"Event Handling and Callback Functions","title":"DiscreteCallback Examples","text":"","category":"section"},{"location":"features/callback_functions/#Example-1:-Interventions-at-Preset-Times","page":"Event Handling and Callback Functions","title":"Example 1: Interventions at Preset Times","text":"","category":"section"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Assume we have a patient whose internal drug concentration follows exponential decay, i.e. the linear ODE with a negative coefficient:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"using DifferentialEquations\nfunction f(du,u,p,t)\n    du[1] = -u[1]\nend\nu0 = [10.0]\nconst V = 1\nprob = ODEProblem(f,u0,(0.0,10.0))\nsol = solve(prob,Tsit5())\nusing Plots; plot(sol)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: Linear Decay)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Now assume we wish to give the patient a dose of 10 at time t==4. For this, we can use a DiscreteCallback which will only be true at t==4:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"condition(u,t,integrator) = t==4\naffect!(integrator) = integrator.u[1] += 10\ncb = DiscreteCallback(condition,affect!)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"If we then solve with this callback enabled, we see no change:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"sol = solve(prob,Tsit5(),callback=cb)\nplot(sol)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: Linear Decay)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"The reason there is no change is because the DiscreteCallback only applies at a specific time, and the integrator never hit that time. Thus we would like to force the ODE solver to step exactly at t=4 so that the condition can be applied. We can do that with the tstops argument:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"sol = solve(prob,Tsit5(),callback=cb,tstops=[4.0])\nplot(sol)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: Linear Decay Dose)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"and thus we achieve the desired result.","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Performing multiple doses then just requires that we have multiple points which are hit. For example, to dose at time t=4 and t=8, we can do the following:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"dosetimes = [4.0,8.0]\ncondition(u,t,integrator) = t ∈ dosetimes\naffect!(integrator) = integrator.u[1] += 10\ncb = DiscreteCallback(condition,affect!)\nsol = solve(prob,Tsit5(),callback=cb,tstops=dosetimes)\nplot(sol)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: Linear Decay Dose)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"We can then use this mechanism to make the model arbitrarily complex. For example, let's say there's now 3 dose times, but the dose only triggers if the current concentration is below 1.0. Additionally, the dose is now 10t instead of just 10. This model is implemented as simply:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"dosetimes = [4.0,6.0,8.0]\ncondition(u,t,integrator) = t ∈ dosetimes && (u[1] < 1.0)\naffect!(integrator) = integrator.u[1] += 10integrator.t\ncb = DiscreteCallback(condition,affect!)\nsol = solve(prob,Tsit5(),callback=cb,tstops=dosetimes)\nplot(sol)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: Linear Decay Dose)","category":"page"},{"location":"features/callback_functions/#PresetTimeCallback","page":"Event Handling and Callback Functions","title":"PresetTimeCallback","text":"","category":"section"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Because events at preset times is a very common occurrence, DifferentialEquations.jl provides a pre-built callback in the Callback Library. The PresetTimeCallback(tstops,affect!) takes an array of times and an affect! function to apply. Thus to do the simple 2 dose example with this callback, we could do the following:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"dosetimes = [4.0,8.0]\naffect!(integrator) = integrator.u[1] += 10\ncb = PresetTimeCallback(dosetimes,affect!)\nsol = solve(prob,Tsit5(),callback=cb)\nplot(sol)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: Linear Decay Dose)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Notice that this version will automatically set the tstops for you.","category":"page"},{"location":"features/callback_functions/#Example-2:-A-Control-Problem","page":"Event Handling and Callback Functions","title":"Example 2: A Control Problem","text":"","category":"section"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Another example of a DiscreteCallback is a control problem switching parameters. Our parameterized ODE system is as follows:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Our ODE function will use this field as follows:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"function f(du,u,p,t)\n    du[1] = -0.5*u[1] + p\n    du[2] = -0.5*u[2]\nend","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Now we will setup our control mechanism. It will be a simple setup which uses set timepoints at which we will change p. At t=5.0 we will want to increase the value of p, and at t=8.0 we will want to decrease the value of p. Using the DiscreteCallback interface, we code these conditions as follows:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"const tstop1 = [5.]\nconst tstop2 = [8.]\n\n\nfunction condition(u,t,integrator)\n  t in tstop1\nend\n\nfunction condition2(u,t,integrator)\n  t in tstop2\nend","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Now we have to apply an effect when these conditions are reached. When condition is hit (at t=5.0), we will increase p to 1.5. When condition2 is reached, we will decrease p to -1.5. This is done via the functions:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"function affect!(integrator)\n  integrator.p = 1.5\nend\n\nfunction affect2!(integrator)\n  integrator.p = -1.5\nend","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"With these functions we can build our callbacks:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"save_positions = (true,true)\n\ncb = DiscreteCallback(condition, affect!, save_positions=save_positions)\n\nsave_positions = (false,true)\n\ncb2 = DiscreteCallback(condition2, affect2!, save_positions=save_positions)\n\ncbs = CallbackSet(cb,cb2)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Now we define our initial condition. We will start at [10.0;10.0] with p=0.0.","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"u0 = [10.0;10.0]\np = 0.0\nprob = ODEProblem(f,u0,(0.0,10.0),p)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Lastly we solve the problem. Note that we must pass tstop values of 5.0 and 8.0 to ensure the solver hits those timepoints exactly:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"const tstop = [5.;8.]\nsol = solve(prob,Tsit5(),callback = cbs, tstops=tstop)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: data_array_plot)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"It's clear from the plot how the controls affected the outcome.","category":"page"},{"location":"features/callback_functions/#Example-3:-AutoAbstol","page":"Event Handling and Callback Functions","title":"Example 3: AutoAbstol","text":"","category":"section"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"MATLAB's Simulink has the option for an automatic absolute tolerance. In this example we will implement a callback which will add this behavior to any JuliaDiffEq solver which implements the integrator and callback interface.","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"The algorithm is as follows. The default value is set to start at 1e-6, though we will give the user an option for this choice. Then as the simulation progresses, at each step the absolute tolerance is set to the maximum value that has been reached so far times the relative tolerance. This is the behavior that we will implement in affect!.","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Since the effect is supposed to occur every timestep, we use the trivial condition:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"condition = function (u,t,integrator)\n    true\nend","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"which always returns true. For our effect we will overload the call on a type. This type will have a value for the current maximum. By doing it this way, we can store the current state for the running maximum. The code is as follows:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"mutable struct AutoAbstolAffect{T}\n  curmax::T\nend\n# Now make `affect!` for this:\nfunction (p::AutoAbstolAffect)(integrator)\n  p.curmax = max(p.curmax,integrator.u)\n  integrator.opts.abstol = p.curmax * integrator.opts.reltol\n  u_modified!(integrator,false)\nend","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"This makes affect!(integrator) use an internal mutating value curmax to update the absolute tolerance of the integrator as the algorithm states.","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Lastly, we can wrap it in a nice little constructor:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"function AutoAbstol(save=true;init_curmax=1e-6)\n  affect! = AutoAbstolAffect(init_curmax)\n  condtion = (u,t,integrator) -> true\n  save_positions = (save,false)\n  DiscreteCallback(condtion,affect!,save_positions=save_positions)\nend","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"This creates the DiscreteCallback from the affect! and condition functions that we implemented. Now","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"cb = AutoAbstol(save=true;init_curmax=1e-6)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"returns the callback that we created. We can then solve an equation using this by simply passing it with the callback keyword argument. Using the integrator interface rather than the solve interface, we can step through one by one to watch the absolute tolerance increase:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"integrator = init(prob,BS3(),callback=cb)\nat1 = integrator.opts.abstol\nstep!(integrator)\nat2 = integrator.opts.abstol\n@test at1 < at2\nstep!(integrator)\nat3 = integrator.opts.abstol\n@test at2 < at3","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Note that this example is contained in the Callback Library, a library of useful callbacks for JuliaDiffEq solvers.","category":"page"},{"location":"features/callback_functions/#ContinuousCallback-Examples","page":"Event Handling and Callback Functions","title":"ContinuousCallback Examples","text":"","category":"section"},{"location":"features/callback_functions/#Example-1:-Bouncing-Ball","page":"Event Handling and Callback Functions","title":"Example 1: Bouncing Ball","text":"","category":"section"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Let's look at the bouncing ball. Let the first variable y is the height which changes by v the velocity, where the velocity is always changing at -g which is the gravitational constant. This is the equation:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"function f(du,u,p,t)\n  du[1] = u[2]\n  du[2] = -p\nend","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"All we have to do in order to specify the event is to have a function which should always be positive with an event occurring at 0. For now at least that's how it's specified. If a generalization is needed we can talk about this (but it needs to be \"root-findable\"). For here it's clear that we just want to check if the ball's height ever hits zero:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"function condition(u,t,integrator) # Event when event_f(u,t) == 0\n  u[1]\nend","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Notice that here we used the values u instead of the value from the integrator. This is because the values u,t will be appropriately modified at the interpolation points, allowing for the rootfinding behavior to occur.","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Now we have to say what to do when the event occurs. In this case we just flip the velocity (the second variable)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"function affect!(integrator)\n  integrator.u[2] = -integrator.u[2]\nend","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"The callback is thus specified by:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"cb = ContinuousCallback(condition,affect!)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Then you can solve and plot:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"u0 = [50.0,0.0]\ntspan = (0.0,15.0)\np = 9.8\nprob = ODEProblem(f,u0,tspan,p)\nsol = solve(prob,Tsit5(),callback=cb)\nplot(sol)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: BallBounce)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"As you can see from the resulting image, DifferentialEquations.jl is smart enough to use the interpolation to hone in on the time of the event and apply the event back at the correct time. Thus one does not have to worry about the adaptive timestepping \"overshooting\" the event as this is handled for you. Notice that the event macro will save the value(s) at the discontinuity.","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"The callback is robust to having multiple discontinuities occur. For example, we can integrate for long time periods and get the desired behavior:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"u0 = [50.0,0.0]\ntspan = (0.0,100.0)\nprob = ODEProblem(f,u0,tspan,p)\nsol = solve(prob,Tsit5(),callback=cb)\nplot(sol,plotdensity=10000)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: bounce_long)","category":"page"},{"location":"features/callback_functions/#Handling-Changing-Dynamics-and-Exactness","page":"Event Handling and Callback Functions","title":"Handling Changing Dynamics and Exactness","text":"","category":"section"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Let's make a version of the bouncing ball where the ball sticks to the ground. We can do this by introducing a parameter p to send the velocity to zero on the bounce. This looks as follows:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"function dynamics!(du, u, p, t)\n\tdu[1] = u[2]\n\tdu[2] = p[1] * -9.8\nend\nfunction floor_aff!(int)\n    int.p[1] = 0\n    int.u[2] = 0\n    @show int.u[1], int.t\nend\nfloor_event = ContinuousCallback(floor_cond, floor_aff!)\nu0 = [1.0,0.0]\np = [1.0]\nprob = ODEProblem{true}(dynamics!, u0, (0., 1.75), p)\nsol = solve(prob, Tsit5(), callback=floor_event)\nplot(sol)\n@show sol[end] # [4.329177480185359e-16, 0.0]","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: sticky bounce)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Notice that at the end, the ball is not at 0.0 like the condition would let you believe, but instead it's at 4.329177480185359e-16. From the printing inside of the affect function, we can see that this is the value it had at the event time t=0.4517539514526232. Why did the event handling not make it exactly zero? If you instead would have run the simulation to nextfloat(0.4517539514526232) = 0.45175395145262326, we would see that the value of u[1] = -1.2647055847076505e-15. You can see this by changing the rootfind argument of the callback:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"floor_event = ContinuousCallback(floor_cond, floor_aff!,rootfind=DiffEqBase.RightRootFind)\nu0 = [1.0,0.0]\np = [1.0]\nprob = ODEProblem{true}(dynamics!, u0, (0., 1.75), p)\nsol = solve(prob, Tsit5(), callback=floor_event)\n@show sol[end] # [-1.2647055847076505e-15, 0.0]","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"What this means is that there is not 64-bit floating point number t such that the condition is zero! By default, if there is no t such that condition=0, then rootfinder defaults to choosing the floating point number exactly before the exactly before the event (LeftRootFind). This way manifold constraints are preserved by default (i.e. the ball never goes below the floor). However, if you require that the condition is exactly satisfied after the event, you will want to add such a change to the affect! function. For example, the error correction in this case is to add int.u[1] = 0 to the affect!, i.e.:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"function floor_aff!(int)\n    int.p[1] = 0\n    int.u[1] = 0\n    int.u[2] = 0\n    @show int.u[1], int.t\nend\nfloor_event = ContinuousCallback(floor_cond, floor_aff!)\nu0 = [1.0,0.0]\np = [1.0]\nprob = ODEProblem{true}(dynamics!, u0, (0., 1.75), p)\nsol = solve(prob, Tsit5(), callback=floor_event)\n@show sol[end] # [0.0,0.0]","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"and now the sticky behavior is perfect to the floating point.","category":"page"},{"location":"features/callback_functions/#Handling-Accumulation-Points","page":"Event Handling and Callback Functions","title":"Handling Accumulation Points","text":"","category":"section"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Now let's take a look at the bouncing ball with friction. After the bounce, we will send the velocity to -v/2. Since the velocity is halving each time, we should have Zeno-like behavior and see an accumulation point of bounces. We will use some extra parameters to count the number of bounces (to infinity) and find the accumulation point. Let's watch!","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"function dynamics!(du, u, p, t)\n\tdu[1] = u[2]\n\tdu[2] = p[1] * -9.8\nend\nfloor_cond(u, t, int) = u[1]\nfunction floor_aff!(int)\n    int.u[2] *= -0.5\n    int.p[1] += 1\n    int.p[2] = int.t\nend\nfloor_event = ContinuousCallback(floor_cond, floor_aff!)\nu0 = [1.0,0.0]\np = [0.0,0.0]\nprob = ODEProblem{true}(dynamics!, u0, (0., 2.), p)\nsol = solve(prob, Tsit5(), callback=floor_event)\nplot(sol)\n@show p # [8.0, 1.3482031881786312]","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: ball floor)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"From the readout we can see the ball only bounced 8 times before it went below the floor, what happened? What happened is floating point error. Because one cannot guarantee that floating point numbers exist to make the condition=0, a heuristic is used to ensure that a zero is not accidentally detected at nextfloat(t) after the simulation restarts (otherwise it would repeatly find the same event!). However, sooner or later the ability to detect minute floating point differences will crash, and what should be infinitely many bounces finally misses a bounce.","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"This leads to two questions:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"How can you improve the accuracy of an accumulation calculation?\nHow can you make it gracefully continue?","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"For (1), note that floating point accuracy is dependent on the current dt. If you know that an accumulation point is coming, one can use set_proposed_dt! to shrink the dt value and help find the next bounce point. You can use t - tprev to know the length of the previous interval for this calculation. For this example, we can set the proposed dt to (t - tprev)/10 to ensure an ever increasing accuracy of the check.","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"However, at some point we will hit machine epsilon, the value where t + eps(t) == t, so we cannot measure infinitely many bounces and instead will be limited by the floating point accuracy of our number representation. Using alternative number types like ArbFloats.jl can allow for this to be done at very high accuracy, but still not infinite. Thus what we need to do is determine a tolerance after which we assume the accumulation has been reached and define the exit behavior. In this case we will say when the dt<1e-12, we are almost at the edge of Float64 accuracy (eps(1.0) = 2.220446049250313e-16), so we will change the position and velocity to exactly zero.","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"With these floating point corrections in mind, the accumulation calculations looks as follows:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"function dynamics!(du, u, p, t)\n\tdu[1] = u[2]\n\tdu[2] = p[1] * -9.8\nend\nfloor_cond(u, t, int) = u[1]\nfunction floor_aff!(int)\n    int.u[2] *= -0.5\n    if int.dt > 1e-12\n        set_proposed_dt!(int,(int.t-int.tprev)/100)\n    else\n        int.u[1] = 0\n        int.u[2] = 0\n        int.p[1] = 0\n    end\n    int.p[2] += 1\n    int.p[3] = int.t\nend\nfloor_event = ContinuousCallback(floor_cond, floor_aff!)\nu0 = [1.0,0.0]\np = [1.0,0.0,0.0]\nprob = ODEProblem{true}(dynamics!, u0, (0., 2.), p)\nsol = solve(prob, Tsit5(), callback=floor_event)\nplot(sol)\n@show p # [0.0, 41.0, 1.355261854357056]","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: bounce accumulation)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"With this corrected version, we see that after 41 bounces the accumulation point is reached at t = 1.355261854357056. To really see the accumulation, let's zoom in:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"p1 = plot(sol,vars=1,tspan=(1.25,1.40))\np2 = plot(sol,vars=1,tspan=(1.35,1.36))\np3 = plot(sol,vars=1,tspan=(1.354,1.35526))\np4 = plot(sol,vars=1,tspan=(1.35526,1.35526185))\nplot(p1,p2,p3,p4)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: bounce accumulation zoom)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"I think Zeno would be proud of our solution.","category":"page"},{"location":"features/callback_functions/#Example-2:-Terminating-an-Integration","page":"Event Handling and Callback Functions","title":"Example 2: Terminating an Integration","text":"","category":"section"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"In many cases you might want to terminate an integration when some condition is satisfied. To terminate an integration, use terminate!(integrator) as the affect! in a callback.","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"In this example we will solve the differential equation:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"u0 = [1.,0.]\nfunction fun2(du,u,p,t)\n   du[2] = -u[1]\n   du[1] = u[2]\nend\ntspan = (0.0,10.0)\nprob = ODEProblem(fun2,u0,tspan)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"which has cosine and -sine as the solutions respectively. We wish to solve until the sine part, u[2] becomes positive. There are two things we may be looking for.","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"A DiscreteCallback will cause this to halt at the first step such that the condition is satisfied. For example, we could use:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"condition(u,t,integrator) = u[2]>0\naffect!(integrator) = terminate!(integrator)\ncb = DiscreteCallback(condition,affect!)\nsol = solve(prob,Tsit5(),callback=cb)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: discrete_terminate)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"However, in many cases we wish to halt exactly at the point of time that the condition is satisfied. To do that, we use a continuous callback. The condition must thus be a function which is zero at the point we want to halt. Thus we use the following:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"condition(u,t,integrator) = u[2]\naffect!(integrator) = terminate!(integrator)\ncb = ContinuousCallback(condition,affect!)\nsol = solve(prob,Tsit5(),callback=cb)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: simple_terminate)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Note that this uses rootfinding to approximate the \"exact\" moment of the crossing. Analytically we know the value is pi, and here the integration terminates at","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"sol.t[end] # 3.1415902502224307","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Using a more accurate integration increases the accuracy of this prediction:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"sol = solve(prob,Vern8(),callback=cb,reltol=1e-12,abstol=1e-12)\nsol.t[end] # 3.1415926535896035\n#π = 3.141592653589703...","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Now say we wish to find the when the first period is over, i.e. we want to ignore the upcrossing and only stop on the downcrossing. We do this by ignoring the affect! and only passing an affect! for the second:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"condition(u,t,integrator) = u[2]\naffect!(integrator) = terminate!(integrator)\ncb = ContinuousCallback(condition,nothing,affect!)\nsol = solve(prob,Tsit5(),callback=cb)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: downcrossing_terminate)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Notice that passing only one affect! is the same as ContinuousCallback(condition,affect!,affect!), i.e. both upcrossings and downcrossings will activate the event. Using ContinuousCallback(condition,affect!,nothing)will thus be the same as above because the first event is an upcrossing.","category":"page"},{"location":"features/callback_functions/#Example-3:-Growing-Cell-Population","page":"Event Handling and Callback Functions","title":"Example 3: Growing Cell Population","text":"","category":"section"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Another interesting issue is with models of changing sizes. The ability to handle such events is a unique feature of DifferentialEquations.jl! The problem we would like to tackle here is a cell population. We start with 1 cell with a protein X which increases linearly with time with rate parameter α. Since we are going to be changing the size of the population, we write the model in the general form:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"const α = 0.3\nfunction f(du,u,p,t)\n  for i in 1:length(u)\n    du[i] = α*u[i]\n  end\nend","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Our model is that, whenever the protein X gets to a concentration of 1, it triggers a cell division. So we check to see if any concentrations hit 1:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"function condition(u,t,integrator) # Event when event_f(u,t) == 0\n  1-maximum(u)\nend","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Again, recall that this function finds events as when condition==0, so 1-maximum(u) is positive until a cell has a concentration of X which is 1, which then triggers the event. At the event, we have that the cell splits into two cells, giving a random amount of protein to each one. We can do this by resizing the cache (adding 1 to the length of all of the caches) and setting the values of these two cells at the time of the event:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"function affect!(integrator)\n  u = integrator.u\n  resize!(integrator,length(u)+1)\n  maxidx = findmax(u)[2]\n  Θ = rand()\n  u[maxidx] = Θ\n  u[end] = 1-Θ\n  nothing\nend","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"As noted in the Integrator Interface, resize!(integrator,length(integrator.u)+1) is used to change the length of all of the internal caches (which includes u) to be their current length + 1, growing the ODE system. Then the following code sets the new protein concentrations. Now we can solve:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"callback = ContinuousCallback(condition,affect!)\nu0 = [0.2]\ntspan = (0.0,10.0)\nprob = ODEProblem(f,u0,tspan)\nsol = solve(prob,callback=callback)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"The plot recipes do not have a way of handling the changing size, but we can plot from the solution object directly. For example, let's make a plot of how many cells there are at each time. Since these are discrete values, we calculate and plot them directly:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"plot(sol.t,map((x)->length(x),sol[:]),lw=3,\n     ylabel=\"Number of Cells\",xlabel=\"Time\")","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: NumberOfCells)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Now let's check-in on a cell. We can still use the interpolation to get a nice plot of the concentration of cell 1 over time. This is done with the command:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"ts = range(0, stop=10, length=100)\nplot(ts,map((x)->x[1],sol.(ts)),lw=3,\n     ylabel=\"Amount of X in Cell 1\",xlabel=\"Time\")","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: Cell1)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Notice that every time it hits 1 the cell divides, giving cell 1 a random amount of X which then grows until the next division.","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Note that one macro which was not shown in this example is deleteat! on the caches. For example, to delete the second cell, we could use:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"deleteat!(integrator,2)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"This allows you to build sophisticated models of populations with births and deaths.","category":"page"},{"location":"features/callback_functions/#VectorContinuousCallback-Example","page":"Event Handling and Callback Functions","title":"VectorContinuousCallback Example","text":"","category":"section"},{"location":"features/callback_functions/#Example-1:-Bouncing-Ball-with-multiple-walls","page":"Event Handling and Callback Functions","title":"Example 1: Bouncing Ball with multiple walls","text":"","category":"section"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"This is similar to the above Bouncing Ball example, but now we have two more vertical walls, at x = 0 and x = 10.0. We have our ODEFunction as -","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"function f(du,u,p,t)\n  du[1] = u[2]\n  du[2] = -p\n  du[3] = u[4]\n  du[4] = 0.0\nend","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"where u[1] denotes y-coordinate, u[2] denotes velocity in y-direction, u[3] denotes x-coordinate and u[4] denotes velocity in x-direction. We will make a VectorContinuousCallback of length 2 - one for x axis collision, one for walls parallel to y axis.","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"function condition(out,u,t,integrator) # Event when event_f(u,t) == 0\n  out[1] = u[1]\n  out[2] = (u[3] - 10.0)u[3]\nend\n\nfunction affect!(integrator, idx)\n  if idx == 1\n    integrator.u[2] = -0.9integrator.u[2]\n  elseif idx == 2\n    integrator.u[4] = -0.9integrator.u[4]\n  end\nend\n\ncb = VectorContinuousCallback(condition,affect!,2)","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"It is evident that out[2] will be zero when u[3] (x-coordinate) is either 0.0 or 10.0. And when that happens, we flip the velocity with some coefficient of restitution (0.9).","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"Completeting rest of the code-","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"u0 = [50.0,0.0,0.0,2.0]\ntspan = (0.0,15.0)\np = 9.8\nprob = ODEProblem(f,u0,tspan,p)\nsol = solve(prob,Tsit5(),callback=cb,dt=1e-3,adaptive=false)\nplot(sol,vars=(1,3))","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"And you get the following output:","category":"page"},{"location":"features/callback_functions/","page":"Event Handling and Callback Functions","title":"Event Handling and Callback Functions","text":"(Image: Cell1)","category":"page"},{"location":"solvers/ode_solve/#ode_solve","page":"ODE Solvers","title":"ODE Solvers","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"solve(prob::ODEProblem,alg;kwargs)","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Solves the ODE defined by prob using the algorithm alg. If no algorithm is given, a default algorithm will be chosen.","category":"page"},{"location":"solvers/ode_solve/#Recommended-Methods","page":"ODE Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"It is suggested that you try choosing an algorithm using the alg_hints keyword argument. However, in some cases you may want something specific, or you may just be curious. This guide is to help you choose the right algorithm.","category":"page"},{"location":"solvers/ode_solve/#Unknown-Stiffness-Problems","page":"ODE Solvers","title":"Unknown Stiffness Problems","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"When the stiffness of the problem is unknown, it is recommended you use a stiffness detection and auto-switching algorithm. These methods are multi-paradigm and allow for efficient solution of both stiff and non-stiff problems. The cost for auto-switching is very minimal but the choices are restrained and so they are a good go-to method when applicable.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"For default tolerances, AutoTsit5(Rosenbrock23()) is a good choice. For lower tolerances, using AutoVern7 or AutoVern9 with Rodas4, KenCarp4, or Rodas5 can all be good choices depending on the problem. For very large systems (>1000 ODEs?), consider using lsoda.","category":"page"},{"location":"solvers/ode_solve/#Non-Stiff-Problems","page":"ODE Solvers","title":"Non-Stiff Problems","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"For non-stiff problems, the native OrdinaryDiffEq.jl algorithms are vastly more efficient than the other choices. For most non-stiff problems, we recommend Tsit5. When more robust error control is required, BS5 is a good choice. If at moderate tolerances and the interpolation error is very important, consider the OwrenZen5 method. For fast solving at higher tolerances, we recommend BS3 (or OwrenZen3 if the interpolation error is important). For high accuracy but with the range of Float64 (~1e-8-1e-12), we recommend Vern6, Vern7, or Vern8 as efficient choices.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"For high accuracy non-stiff solving (BigFloat and tolerances like <1e-12), we recommend the Vern9 method. If a high-order method is needed with a high order interpolant, then you should choose Vern9 which is Order 9 with an Order 9 interpolant. If you need extremely high accuracy (<1e-30?) and do not need an interpolant, try the Feagin12 or Feagin14 methods. Note that the Feagin methods are the only high-order optimized methods which do not include a high-order interpolant (they do include a 3rd order Hermite interpolation if needed). Note that these high order RK methods are more robust than the high order Adams-Bashforth methods to discontinuities and achieve very high precision, and are much more efficient than the extrapolation methods. However, the VCABM method can be a good choice for high accuracy when the system of equations is very large (>1,000 ODEs?), the function calculation is very expensive, or the solution is very smooth.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"If strict error bounds are needed, then adaptive methods with defect controls are required. Defect controls use an error measurement on the interpolating polynomial to make the error estimate better capture the error over the full interval. For medium accuracy calculations, RK4 is a good choice.","category":"page"},{"location":"solvers/ode_solve/#Stiff-Problems","page":"ODE Solvers","title":"Stiff Problems","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"For stiff problems at high tolerances (>1e-2?) it is recommended that you use Rosenbrock23 or TRBDF2. These are robust to oscillations and massive stiffness, though are only efficient when low accuracy is needed. Rosenbrock23 is more efficient for small systems where re-evaluating and re-factorizing the Jacobian is not too costly, and for sufficiently large systems TRBDF2 will be more efficient. ABDF2 can be the most efficient the largest systems or most expensive f.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"At medium tolerances (>1e-8?) it is recommended you use Rodas5, Rodas4P (the former is more efficient but the later is more reliable), Kvaerno5, or KenCarp4. As native DifferentialEquations.jl solvers, many Julia numeric types (such as BigFloats, ArbFloats, or DecFP) will work. When the equation is defined via the @ode_def macro, these will be the most efficient.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"For faster solving at low tolerances (<1e-9) but when Vector{Float64} is used, use radau.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"For asymptotically large systems of ODEs (N>1000?) where f is very costly and the complex eigenvalues are minimal (low oscillations), in that case QNDF will be the most efficient but requires Vector{Float64}. QNDF will also do surprisingly well if the solution is smooth. However, this method can handle less stiffness than other methods and its Newton iterations may fail at low accuracy situations. Other choices to consider in this regime are CVODE_BDF and lsoda.","category":"page"},{"location":"solvers/ode_solve/#Special-Properties-of-Stiff-Integrators","page":"ODE Solvers","title":"Special Properties of Stiff Integrators","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"ImplicitMidpoint is a symmetric and symplectic integrator. Trapezoid is a symmetric (almost symplectic) integrator with adaptive timestepping. ImplicitEuler is an extension to the common algorithm with adaptive timestepping and efficient quasi-Newton Jacobian re-usage which is fully strong-stability preserving (SSP) for hyperbolic PDEs.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Notice that Rodas4 loses accuracy on discretizations of nonlinear parabolic PDEs, and thus it's suggested you replace it with Rodas4P in those situations which is 3rd order. ROS3P is only third order and achieves 3rd order on such problems and can thus be more efficient in this case.","category":"page"},{"location":"solvers/ode_solve/#Translations-from-MATLAB/Python/R","page":"ODE Solvers","title":"Translations from MATLAB/Python/R","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"For users familiar with MATLAB/Python/R, good translations of the standard library methods are as follows:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"ode23 –> BS3()\node45/dopri5 –> DP5(), though in most cases Tsit5() is more efficient\node23s –> Rosenbrock23(), though in most cases Rodas4() is more efficient\node113 –> VCABM(), though in many cases Vern7() is more efficient\ndop853 –> DP8(), though in most cases Vern7() is more efficient\node15s/vode –> QNDF(), though in many cases Rodas4(), KenCarp4(), TRBDF2(), or RadauIIA5() are more efficient\node23t –> Trapezoid()\node23tb –> TRBDF2()\nlsoda –> lsoda(), though AutoTsit5(Rosenbrock23()) or AutoVern7(Rodas5()) may be more efficient. Note that lsoda() requires the LSODA.jl extension, which can be added via ]add LSODA; using LSODA.\node15i –> IDA(), though in many cases Rodas4() can handle the DAE and is significantly more efficient","category":"page"},{"location":"solvers/ode_solve/#Full-List-of-Methods","page":"ODE Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"solvers/ode_solve/#OrdinaryDiffEq.jl-for-Non-Stiff-Equations","page":"ODE Solvers","title":"OrdinaryDiffEq.jl for Non-Stiff Equations","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Unless otherwise specified, the OrdinaryDiffEq algorithms all come with a 3rd order Hermite polynomial interpolation. The algorithms denoted as having a \"free\" interpolation means that no extra steps are required for the interpolation. For the non-free higher order interpolating functions, the extra steps are computed lazily (i.e. not during the solve).","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The OrdinaryDiffEq.jl algorithms achieve the highest performance for non-stiff equations while being the most generic: accepting the most Julia-based types, allow for sophisticated event handling, etc. On stiff ODEs these algorithms again consistently among the top. OrdinaryDiffEq.jl is recommended for most ODE problems.","category":"page"},{"location":"solvers/ode_solve/#Explicit-Runge-Kutta-Methods","page":"ODE Solvers","title":"Explicit Runge-Kutta Methods","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Euler- The canonical forward Euler method. Fixed timestep only.\nMidpoint - The second order midpoint method. Uses embedded Euler method for adaptivity.\nHeun - The second order Heun's method. Uses embedded Euler method for adaptivity.\nRalston - The optimized second order midpoint method. Uses embedded Euler. method for adaptivity.\nRK4 - The canonical Runge-Kutta Order 4 method. Uses a defect control for adaptive stepping using maximum error over the whole interval.\nBS3 - Bogacki-Shampine 3/2 method.\nOwrenZen3 - Owren-Zennaro optimized interpolantion 3/2 method (free 3th order interpolant).\nOwrenZen4 - Owren-Zennaro optimized interpolantion 4/3 method (free 4th order interpolant).\nOwrenZen5 - Owren-Zennaro optimized interpolantion 5/4 method (free 5th order interpolant).\nDP5 - Dormand-Prince's 5/4 Runge-Kutta method. (free 4th order interpolant).\nTsit5 - Tsitouras 5/4 Runge-Kutta method. (free 4th order interpolant).\nAnas5(w) - 4th order Runge-Kutta method designed for periodic problems. Requires a periodicity estimate w which when accurate the method becomes 5th order (and is otherwise 4th order with less error for better estimates).\nFRK65(w=0) - Zero Dissipation Runge-Kutta of 6th order. Takes an optional argument w to for the periodicity phase, in which case this method results in zero numerical dissipation.\nPFRK87(w=0) - Phase-fitted Runge-Kutta Runge-Kutta of 8th order. Takes an optional argument w to for the periodicity phase, in which case this method results in zero numerical dissipation.\nRKO65 - Tsitouras' Runge-Kutta-Oliver 6 stage 5th order method. This method is robust on problems which have a singularity at t=0.\nTanYam7 - Tanaka-Yamashita 7 Runge-Kutta method.\nDP8 - Hairer's 8/5/3 adaption of the Dormand-Prince Runge-Kutta method. (7th order interpolant).\nTsitPap8 - Tsitouras-Papakostas 8/7 Runge-Kutta method.\nFeagin10 - Feagin's 10th-order Runge-Kutta method.\nFeagin12 - Feagin's 12th-order Runge-Kutta method.\nFeagin14 - Feagin's 14th-order Runge-Kutta method.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Example usage:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"alg = Tsit5()\nsolve(prob,alg)","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Additionally, the following algorithms have a lazy interpolant:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"BS5 - Bogacki-Shampine 5/4 Runge-Kutta method. (lazy 5th order interpolant).\nVern6 - Verner's \"Most Efficient\" 6/5 Runge-Kutta method. (lazy 6th order interpolant).\nVern7 - Verner's \"Most Efficient\" 7/6 Runge-Kutta method. (lazy 7th order interpolant).\nVern8 - Verner's \"Most Efficient\" 8/7 Runge-Kutta method. (lazy 8th order interpolant)\nVern9 - Verner's \"Most Efficient\" 9/8 Runge-Kutta method. (lazy 9th order interpolant)","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"These methods require a few extra steps in order to compute the high order interpolation, but these steps are only taken when the interpolation is used. These methods when lazy assume that the parameter vector p will be unchanged between the moment of the interval solving and the interpolation. If p is changed in a ContinuousCallback, or in a DiscreteCallback and the continuous solution is used after the full solution, then set lazy=false.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Example:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"solve(prob,Vern7()) # lazy by default\nsolve(prob,Vern7(lazy=false))","category":"page"},{"location":"solvers/ode_solve/#Parallel-Explicit-Runge-Kutta-Methods","page":"ODE Solvers","title":"Parallel Explicit Runge-Kutta Methods","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"KuttaPRK2p5 - A 5 parallel, 2 processor explicit Runge-Kutta method of 5th order.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"These methods utilize multithreading on the f calls to parallelize the problem. This requires that simultaneous calls to f are thread-safe.","category":"page"},{"location":"solvers/ode_solve/#Explicit-Strong-Stability-Preserving-Runge-Kutta-Methods-for-Hyperbolic-PDEs-(Conservation-Laws)","page":"ODE Solvers","title":"Explicit Strong-Stability Preserving Runge-Kutta Methods for Hyperbolic PDEs (Conservation Laws)","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"SSPRK22 - The two-stage, second order strong stability preserving (SSP) method of Shu and Osher (SSP coefficient 1, free 2nd order SSP interpolant). Fixed timestep only.\nSSPRK33 - The three-stage, third order strong stability preserving (SSP) method of Shu and Osher (SSP coefficient 1, free 2nd order SSP interpolant). Fixed timestep only.\nSSPRK53 - The five-stage, third order strong stability preserving (SSP) method of Ruuth (SSP coefficient 2.65, free 3rd order Hermite interpolant). Fixed timestep only.\nSSPRK63 - The six-stage, third order strong stability preserving (SSP) method of Ruuth (SSP coefficient 3.518, free 3rd order Hermite interpolant). Fixed timestep only.\nSSPRK73 - The seven-stage, third order strong stability preserving (SSP) method of Ruuth (SSP coefficient 4.2879, free 3rd order Hermite interpolant). Fixed timestep only.\nSSPRK83 - The eight-stage, third order strong stability preserving (SSP) method of Ruuth (SSP coefficient 5.107, free 3rd order Hermite interpolant). Fixed timestep only.\nSSPRK432 - A  3/2 adaptive strong stability preserving (SSP) method with five stages (SSP coefficient 2, free 2nd order SSP interpolant).\nSSPRK43 - A  3/2 adaptive strong stability preserving (SSP) method with five stages (SSP coefficient 2, free 2nd order SSP interpolant). The main method is the same as SSPRK432, but the embedded method has a larger stability region.\nSSPRK932 - A  3/2 adaptive strong stability preserving (SSP) method with nine stages (SSP coefficient 6, free 3rd order Hermite interpolant).\nSSPRK54 - The five-stage, fourth order strong stability preserving (SSP) method of Spiteri and Ruuth (SSP coefficient 1.508, 3rd order Hermite interpolant). Fixed timestep only.\nSSPRK104 - The ten-stage, fourth order strong stability preserving method of Ketcheson (SSP coefficient 6, free 3rd order Hermite interpolant). Fixed timestep only.\nSSPRKMSVS32 - 3-stage, 2nd order SSP-optimal linear multistep method. (SSP coefficent 0.5, 3rd order Hermite interpolant). Fixed timestep only.\nSSPRKMSVS43 - 4-stage, 3rd order SSP-optimal linear multistep method. (SSP coefficent 0.33, 3rd order Hermite interpolant). Fixed timestep only.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The SSP coefficients of the methods can be queried as ssp_coefficient(alg). All explicit SSP methods take two optional arguments SSPXY(stage_limiter!, step_limiter!), where stage_limiter! and step_limiter are functions taking arguments of the form limiter!(u, integrator, p, t). Here, u is the new solution value (updated inplace) after an explicit Euler stage / the whole time step , integrator the ODE integrator, and t the current time. These limiters can be used to enforce physical constraints, e.g. the positivity preserving limiters of Zhang and Shu (Zhang, Xiangxiong, and Chi-Wang Shu. \"Maximum-principle-satisfying and positivity-preserving high-order schemes for conservation laws: survey and new developments.\" Proceedings of the Royal Society of London A: Mathematical, Physical and Engineering Sciences. The Royal Society, 2011.).","category":"page"},{"location":"solvers/ode_solve/#Low-Storage-Methods","page":"ODE Solvers","title":"Low-Storage Methods","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"ORK256 - 5-stage, second order low-storage method for wave propogation equations. Fixed timestep only. Like SSPRK methods, ORK256 also takes optional arguments stage_limiter!, step_limiter!, where stage_limiter! and step_limiter! are functions of the form limiter!(u, integrator, p, t).\nSSPRK53_2N1 and SSPRK53_2N2 - 5-stage, third order low-storage methods with large SSP coefficients. (SSP coefficient 2.18 and 2.15, free 3rd order Hermite interpolant). Fixed timestep only.\nCarpenterKennedy2N54 - The five-stage, fourth order low-storage method of Carpenter and Kennedy (free 3rd order Hermite interpolant). Fixed timestep only. Designed for hyperbolic PDEs (stability properties). Like SSPRK methods, CarpenterKennedy2N54 also takes optional arguments stage_limiter!, step_limiter!.\nNDBLSRK124 - 12-stage, fourth order low-storage method with optimized stability regions for advection-dominated problems. Fixed timestep only. Like SSPRK methods, NDBLSRK124 also takes optional arguments stage_limiter!, step_limiter!.\nNDBLSRK134 - 13-stage, fourth order low-storage method with optimized stability regions for advection-dominated problems. Fixed timestep only. Like SSPRK methods, NDBLSRK134 also takes optional arguments stage_limiter!, step_limiter!.\nNDBLSRK144 - 14-stage, fourth order low-storage method with optimized stability regions for advection-dominated problems. Fixed timestep only.  Like SSPRK methods, NDBLSRK144 also takes optional arguments stage_limiter!, step_limiter!.\nCFRLDDRK64 - 6-stage, fourth order low-storage, low-dissipation, low-dispersion scheme. Fixed timestep only.\nTSLDDRK74 - 7-stage, fourth order low-storage low-dissipation, low-dispersion scheme with maximal accuracy and stability limit along the imaginary axes. Fixed timestep only.\nDGLDDRK73_C - 7-stage, third order low-storage low-dissipation, low-dispersion scheme for discontinuous Galerkin space discretizations applied to wave propagation problems, optimized for PDE discretizations when maximum spatial step is small due to geometric features of computational domain. Fixed timestep only. Like SSPRK methods, DGLDDRK73_C also takes optional arguments stage_limiter!, step_limiter!.\nDGLDDRK84_C - 8-stage, fourth order low-storage low-dissipation, low-dispersion scheme for discontinuous Galerkin space discretizations applied to wave propagation problems, optimized for PDE discretizations when maximum spatial step is small due to geometric features of computational domain. Fixed timestep only. Like SSPRK methods, DGLDDRK84_C also takes optional arguments stage_limiter!, step_limiter!.\nDGLDDRK84_F - 8-stage, fourth order low-storage low-dissipation, low-dispersion scheme for discontinuous Galerkin space discretizations applied to wave propagation problems, optimized for PDE discretizations when the maximum spatial step size is not constrained. Fixed timestep only. Like SSPRK methods, DGLDDRK84_F also takes optional arguments stage_limiter!, step_limiter!.\nSHLDDRK64 - 6-stage, fourth order low-stage, low-dissipation, low-dispersion scheme. Fixed timestep only. Like SSPRK methods, SHLDDRK64 also takes optional arguments stage_limiter!, step_limiter!.\nRK46NL - 6-stage, fourth order low-stage, low-dissipation, low-dispersion scheme. Fixed timestep only.\nParsaniKetchesonDeconinck3S32 - 3-stage, second order (3S) low-storage scheme, optimised for for the spectral difference method applied to wave propagation problems.\nParsaniKetchesonDeconinck3S82 - 8-stage, second order (3S) low-storage scheme, optimised for for the spectral difference method applied to wave propagation problems.\nParsaniKetchesonDeconinck3S53 - 5-stage, third order (3S) low-storage scheme, optimised for for the spectral difference method applied to wave propagation problems.\nParsaniKetchesonDeconinck3S173 - 17-stage, third order (3S) low-storage scheme, optimised for for the spectral difference method applied to wave propagation problems.\nParsaniKetchesonDeconinck3S94 - 9-stage, fourth order (3S) low-storage scheme, optimised for for the spectral difference method applied to wave propagation problems.\nParsaniKetchesonDeconinck3S184 - 18-stage, fourth order (3S) low-storage scheme, optimised for for the spectral difference method applied to wave propagation problems.\nParsaniKetchesonDeconinck3S105 - 10-stage, fifth order (3S) low-storage scheme, optimised for for the spectral difference method applied to wave propagation problems.\nParsaniKetchesonDeconinck3S205 - 20-stage, fifth order (3S) low-storage scheme, optimised for for the spectral difference method applied to wave propagation problems.\nCKLLSRK43_2 - 4-stage, third order low-storage scheme, optimised for compressible Navier–Stokes equations..\nCKLLSRK54_3C - 5-stage, fourth order low-storage scheme, optimised for compressible Navier–Stokes equations.\nCKLLSRK95_4S - 9-stage, fifth order low-storage scheme, optimised for compressible Navier–Stokes equations.\nCKLLSRK95_4C - 9-stage, fifth order low-storage scheme, optimised for compressible Navier–Stokes equations.\nCKLLSRK95_4M - 9-stage, fifth order low-storage scheme, optimised for compressible Navier–Stokes equations.\nCKLLSRK54_3C_3R - 5-stage, fourth order low-storage scheme, optimised for compressible Navier–Stokes equations.\nCKLLSRK54_3M_3R - 5-stage, fourth order low-storage scheme, optimised for compressible Navier–Stokes equations.\nCKLLSRK54_3N_3R - 5-stage, fourth order low-storage scheme, optimised for compressible Navier–Stokes equations.\nCKLLSRK85_4C_3R - 8-stage, fifth order low-storage scheme, optimised for compressible Navier–Stokes equations.\nCKLLSRK85_4M_3R - 8-stage, fifth order low-storage scheme, optimised for compressible Navier–Stokes equations.\nCKLLSRK85_4P_3R - 8-stage, fifth order low-storage scheme, optimised for compressible Navier–Stokes equations.\nCKLLSRK54_3N_4R - 5-stage, fourth order low-storage scheme, optimised for compressible Navier–Stokes equations.\nCKLLSRK54_3M_4R - 5-stage, fourth order low-storage scheme, optimised for compressible Navier–Stokes equations.\nCKLLSRK65_4M_4R - 6-stage, fifth order low-storage scheme, optimised for compressible Navier–Stokes equations.\nCKLLSRK85_4FM_4R - 8-stage, fifth order low-storage scheme, optimised for compressible Navier–Stokes equations.\nCKLLSRK75_4M_5R - 7-stage, fifth order low-storage scheme, optimised for compressible Navier–Stokes equations.\nRDPK3Sp35 - 5-stage, third order low-storage scheme with embedded error estimator, optimized for compressible fluid mechanics. Like SSPRK methods, this method also takes optional arguments stage_limiter! and step_limiter!.\nRDPK3SpFSAL35 - 5-stage, third order low-storage scheme with embedded error estimator, optimized for compressible fluid mechanics. Like SSPRK methods, this method also takes optional arguments stage_limiter! and step_limiter!.\nRDPK3Sp49 - 9-stage, fourth order low-storage scheme with embedded error estimator, optimized for compressible fluid mechanics. Like SSPRK methods, this method also takes optional arguments stage_limiter! and step_limiter!.\nRDPK3SpFSAL49 - 9-stage, fourth order low-storage scheme with embedded error estimator, optimized for compressible fluid mechanics. Like SSPRK methods, this method also takes optional arguments stage_limiter! and step_limiter!.\nRDPK3Sp510 - 10-stage, fifth order low-storage scheme with embedded error estimator, optimized for compressible fluid mechanics. Like SSPRK methods, this method also takes optional arguments stage_limiter! and step_limiter!.\nRDPK3SpFSAL510 - 10-stage, fifth order low-storage scheme with embedded error estimator, optimized for compressible fluid mechanics. Like SSPRK methods, this method also takes optional arguments stage_limiter! and step_limiter!.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"NOTE: All the 2N Methods (ORK256, CarpenterKennedy2N54, NDBLSRK124, NDBLSRK134, NDBLSRK144, DGLDDRK73_C, DGLDDRK84_C, DGLDDRK84_F and SHLDDRK64) work on the basic principle of being able to perform step S1 = S1 + F(S2) in just 2 registers. Certain optimizations have been done to achieve this theoritical limit (when alias_u0 is set) but have a limitation that du should always be on the left hand side (assignments only) in the implementation.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Example - This is an invalid implementation for 2N methods:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"function f(du,u,p,t)\n  du[1] = u[1] * u[2]\n  du[2] = du[1] * u[2] # du appears on the RHS\nend","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"If you don't wish to have the optimization and have to use du on the RHS, please set the keyword argument williamson_condition to false in the algorithm (by default it is set to true). In this case 3 registers worth memory would be needed instead.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Example :","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"alg = CarpenterKennedy2N54(;williamson_condition=false)","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"So the above implementation of f becomes valid.","category":"page"},{"location":"solvers/ode_solve/#Parallelized-Explicit-Extrapolation-Methods","page":"ODE Solvers","title":"Parallelized Explicit Extrapolation Methods","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The following are adaptive order, adaptive step size extrapolation methods:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"AitkenNeville - Euler extrapolation using Aitken-Neville with the Romberg Sequence.\nExtrapolationMidpointDeuflhard - Midpoint extrapolation using Barycentric coordinates\nExtrapolationMidpointHairerWanner - Midpoint extrapolation using Barycentric coordinates, following Hairer's ODEX in the adaptivity behavior.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"These methods have arguments for max_order, min_order, and init_order on the adaptive order algorithm. The sequence_factor denotes which even multiple of sequence to take while evaluating internal discretisations. threading denotes whether to automatically multithread the f evaluations, allowing for a high degree of within-method parallelism. The defaults are:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"max_order=10\nmin_order=1 except for ExtrapolationMidpointHairerWanner it's 2.\ninit_order=5\nthreading=true\nseqeunce_factor = 2","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Additionally, the ExtrapolationMidpointDeuflhard and ExtrapolationMidpointHairerWanner methods have the additional argument:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"sequence: the step-number sequences, also called the subdividing","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"sequence. Possible values are :harmonic, :romberg or :bulirsch. Default  is :harmonic.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"To override, utilize the keyword arguments. For example:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"alg = ExtrapolationMidpointDeuflhard(max_order=7,min_order=4,init_order=4,sequence=:bulirsch,threading=false)\nsolve(prob,alg)","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that the order that is referred to is the extrapolation order. For AitkenNeville this is the order of the method, for the others an extrapolation order of n gives an order 2(n+1) method.","category":"page"},{"location":"solvers/ode_solve/#Explicit-Multistep-Methods","page":"ODE Solvers","title":"Explicit Multistep Methods","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Methods using the approximation at more than one previous mesh point to determine the approximation at the next point are called multistep methods. These methods tend to be more efficient as the size of the system or the cost of f increases.","category":"page"},{"location":"solvers/ode_solve/#Adams-Bashforth-Explicit-Methods","page":"ODE Solvers","title":"Adams-Bashforth Explicit Methods","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"These methods require a choice of dt.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"AB3 - The 3-step third order multistep method. Ralston's Second Order Method is used to calculate starting values.\nAB4 - The 4-step fourth order multistep method. Runge-Kutta method of order 4 is used to calculate starting values.\nAB5 - The 5-step fifth order multistep method. Runge-Kutta method of order 4 is used to calculate starting values.\nABM32 - It is third order method. In ABM32, AB3 works as predictor and Adams Moulton 2-steps method works as Corrector. Ralston's Second Order Method is used to calculate starting values.\nABM43 - It is fourth order method. In ABM43, AB4 works as predictor and Adams Moulton 3-steps method works as Corrector. Runge-Kutta method of order 4 is used to calculate starting values.\nABM54 - It is fifth order method. In ABM54, AB5 works as predictor and Adams Moulton 4-steps method works as Corrector. Runge-Kutta method of order 4 is used to calculate starting values.","category":"page"},{"location":"solvers/ode_solve/#Adaptive-step-size-Adams-explicit-Methods","page":"ODE Solvers","title":"Adaptive step size Adams explicit Methods","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"VCAB3 - The 3rd order Adams method. Bogacki-Shampine 3/2 method is used to calculate starting values.\nVCAB4 - The 4th order Adams method. Runge-Kutta 4 is used to calculate starting values.\nVCAB5 - The 5th order Adams method. Runge-Kutta 4 is used to calculate starting values.\nVCABM3 - The 3rd order Adams-Moulton method. Bogacki-Shampine 3/2 method is used to calculate starting values.\nVCABM4 - The 4th order Adams-Moulton method. Runge-Kutta 4 is used to calculate starting values.\nVCABM5 - The 5th order Adams-Moulton method. Runge-Kutta 4 is used to calculate starting values.\nVCABM - An adaptive order adaptive time Adams Moulton method. It uses an order adaptivity algorithm is derived from Shampine's DDEABM.\nAN5 - An adaptive 5th order fixed-leading coefficient Adams method in Nordsieck form.\nJVODE_Adams - An adaptive time adaptive order fixed-leading coefficient Adams method in Nordsieck form. The order adaptivity algorithm is derived from Sundials' CVODE_Adams. In development.","category":"page"},{"location":"solvers/ode_solve/#OrdinaryDiffEq.jl-for-Stiff-Equations","page":"ODE Solvers","title":"OrdinaryDiffEq.jl for Stiff Equations","text":"","category":"section"},{"location":"solvers/ode_solve/#SDIRK-Methods","page":"ODE Solvers","title":"SDIRK Methods","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"ImplicitEuler - A 1st order implicit solver. A-B-L-stable. Adaptive timestepping through a divided differences estimate via memory. Strong-stability preserving (SSP).\nImplicitMidpoint - A second order A-stable symplectic and symmetric implicit solver. Good for highly stiff equations which need symplectic integration.\nTrapezoid - A second order A-stable symmetric ESDIRK method. \"Almost symplectic\" without numerical dampening. Also known as Crank-Nicolson when applied to PDEs. Adaptive timestepping via divided differences on the memory. Good for highly stiff equations which are non-oscillatory.\nTRBDF2 - A second order A-B-L-S-stable one-step ESDIRK method. Includes stiffness-robust error estimates for accurate adaptive timestepping, smoothed derivatives for highly stiff and oscillatory problems.\nSDIRK2 - An A-B-L stable 2nd order SDIRK method\nKvaerno3 - An A-L stable stiffly-accurate 3rd order ESDIRK method\nKenCarp3 - An A-L stable stiffly-accurate 3rd order ESDIRK method with splitting\nCash4 - An A-L stable 4th order SDIRK method\nHairer4 - An A-L stable 4th order SDIRK method\nHairer42 - An A-L stable 4th order SDIRK method\nKvaerno4 - An A-L stable stiffly-accurate 4th order ESDIRK method\nKenCarp4 - An A-L stable stiffly-accurate 4th order ESDIRK method with splitting\nKenCarp47 - An A-L stable stiffly-accurate 4th order seven-stage ESDIRK method with splitting\nKvaerno5 - An A-L stable stiffly-accurate 5th order ESDIRK method\nKenCarp5 - An A-L stable stiffly-accurate 5th order ESDIRK method with splitting\nKenCarp58 - An A-L stable stiffly-accurate 5th order eight-stage ESDIRK method with splitting","category":"page"},{"location":"solvers/ode_solve/#Fully-Implicit-Runge-Kutta-Methods-(FIRK)","page":"ODE Solvers","title":"Fully-Implicit Runge-Kutta Methods (FIRK)","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"RadauIIA3 - An A-B-L stable fully implicit Runge-Kutta method with internal tableau complex basis transform for efficiency.\nRadauIIA5 - An A-B-L stable fully implicit Runge-Kutta method with internal tableau complex basis transform for efficiency.","category":"page"},{"location":"solvers/ode_solve/#Parallel-Diagonally-Implicit-Runge-Kutta-Methods","page":"ODE Solvers","title":"Parallel Diagonally Implicit Runge-Kutta Methods","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"PDIRK44 - A 2 processor 4th order diagonally non-adaptive implicit method.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"These methods also have option nlsolve same as SDIRK methods. These methods also need f to be thread safe. It parallelises the nlsolve calls inside the method.","category":"page"},{"location":"solvers/ode_solve/#Rosenbrock-Methods","page":"ODE Solvers","title":"Rosenbrock Methods","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"ROS3P - 3rd order A-stable and stiffly stable Rosenbrock method. Keeps high accuracy on discretizations of nonlinear parabolic PDEs.\nRodas3 - 3rd order A-stable and stiffly stable Rosenbrock method.\nRosShamp4- An A-stable 4th order Rosenbrock method.\nVeldd4 - A 4th order D-stable Rosenbrock method.\nVelds4 - A 4th order A-stable Rosenbrock method.\nGRK4T - An efficient 4th order Rosenbrock method.\nGRK4A - An A-stable 4th order Rosenbrock method. Essentially \"anti-L-stable\" but efficient.\nRos4LStab - A 4th order L-stable Rosenbrock method.\nRodas4 - A 4th order A-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant\nRodas42 - A 4th order A-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant\nRodas4P - A 4th order A-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant. 4th order on linear parabolic problems and 3rd order accurate on nonlinear parabolic problems (as opposed to lower if not corrected).\nRodas4P2 - A 4th order L-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant. 4th order on linear parabolic problems and 3rd order accurate on nonlinear parabolic problems. It is an improvement of Roadas4P and in case of inexact Jacobians a second order W method.\nRodas5 - A 5th order A-stable stiffly stable Rosenbrock method. Currently has a Hermite interpolant because its stiff-aware 3rd order interpolant is not yet implemented.","category":"page"},{"location":"solvers/ode_solve/#Rosenbrock-W-Methods","page":"ODE Solvers","title":"Rosenbrock-W Methods","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Rosenbrock23 - An Order 2/3 L-Stable Rosenbrock-W method which is good for very stiff equations with oscillations at low tolerances. 2nd order stiff-aware interpolation.\nRosenbrock32 - An Order 3/2 A-Stable Rosenbrock-W method which is good for mildy stiff equations without oscillations at low tolerances. Note that this method is prone to instability in the presence of oscillations, so use with caution. 2nd order stiff-aware interpolation.\nRosenbrockW6S4OS - A 4th order L-stable Rosenbrock-W method (fixed step only).\nROS34PW1a - A 4th order L-stable Rosenbrock-W method.\nROS34PW1b - A 4th order L-stable Rosenbrock-W method.\nROS34PW2 - A 4th order stiffy accurate Rosenbrock-W method for PDAEs.\nROS34PW3 - A 4th order strongly A-stable (Rinf~0.63) Rosenbrock-W method.","category":"page"},{"location":"solvers/ode_solve/#Stabilized-Explicit-Methods","page":"ODE Solvers","title":"Stabilized Explicit Methods","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"ROCK2 - Second order stabilized Runge-Kutta method. Exhibits high stability for real eigenvalues and is smoothened to allow for moderate sized complex eigenvalues.\nROCK4 - Fourth order stabilized Runge-Kutta method. Exhibits high stability for real eigenvalues and is smoothened to allow for moderate sized complex eigenvalues.\nRKC - Second order stabilized Runge-Kutta method. Exhibits high stability for real eigenvalues and is smoothened to allow for moderate sized complex eigenvalues.\nSERK2v2 - Second order stabilized extrapolated Runge-Kutta method. Exhibits high stability for real eigenvalues and is smoothened to allow for moderate sized complex eigenvalues.\nESERK5 - Fifth order stabilized extrapolated Runge-Kutta method. Exhibits high stability for real eigenvalues and is smoothened to allow for moderate sized complex eigenvalues.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"ROCK methods offer a min_stages and max_stages functionality. SERK methods derive higher orders by Aitken-Neville algorithm. SERK2v2 is defaulted to Predictive control but has option of PI control.","category":"page"},{"location":"solvers/ode_solve/#Parallelized-Implicit-Extrapolation-Methods","page":"ODE Solvers","title":"Parallelized Implicit Extrapolation Methods","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The following are adaptive order, adaptive step size extrapolation methods:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"ImplicitEulerExtrapolation - Extrapolation of implicit Euler method with Romberg sequence. Similar to Hairer's SEULEX.\nImplicitDeuflhardExtrapolation - Midpoint extrapolation using Barycentric coordinates\nImplicitHairerWannerExtrapolation - Midpoint extrapolation using Barycentric coordinates, following Hairer's SODEX in the adaptivity behavior.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"These methods have arguments for max_order, min_order, and init_order on the adaptive order algorithm. threading denotes whether to automatically multithread the f evaluations and J/W instantiations+factorizations, allowing for a high degree of within-method parallelism. We recommend to switch to multi-threading when the system consists of more than ~ 150 ODES. The defaults are:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"max_order=10\nmin_order=1 except for ImplicitHairerWannerExtrapolation it's 2.\ninit_order=5\nthreading=false","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Additionally, the ImplicitDeuflhardExtrapolation and ImplicitHairerWannerExtrapolation methods have the additional argument:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"sequence: the step-number sequences, also called the subdividing","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"sequence. Possible values are :harmonic, :romberg or :bulirsch. Default  is :harmonic.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"To override, utilize the keyword arguments. For example:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"alg = ImplicitDeuflhardExtrapolation(max_order=7,min_order=4,init_order=4,sequence=:bulirsch)\nsolve(prob,alg)","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that the order that is referred to is the extrapolation order. For ImplicitEulerExtrapolation this is the order of the method, for the others an extrapolation order of n gives an order 2(n+1) method.","category":"page"},{"location":"solvers/ode_solve/#Parallelized-DIRK-Methods","page":"ODE Solvers","title":"Parallelized DIRK Methods","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"These methods parallelize the J/W instantiation and factorization, making them efficient on small highly stiff ODEs. Has an option threading=true to turn on/off multithreading.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"PDIRK44: a 4th order 2-processor DIRK method.","category":"page"},{"location":"solvers/ode_solve/#Exponential-Runge-Kutta-Methods","page":"ODE Solvers","title":"Exponential Runge-Kutta Methods","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"These methods are all fixed timestepping only.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"LawsonEuler - First order exponential Euler scheme.\nNorsettEuler - First order exponential-RK scheme. Alias: ETD1.\nETD2 - Second order Exponential Time Differencing method (in development).\nETDRK2 - 2nd order exponential-RK scheme.\nETDRK3 - 3rd order exponential-RK scheme.\nETDRK4 - 4th order exponential-RK scheme.\nHochOst4 - 4th order exponential-RK scheme with stiff order 4.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The methods are intended for semilinear problems constructed by SplitODEProblem or SplitODEFunction. They can also be used for a general nonlinear problem, in which case the jacobian of the right hand side is used as the linear operator in each time step.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Except for ETD2, all methods come with these options, which can be set in the methods' constructor:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"krylov - boolean, default: false. Determines whether Krylov approximation or operator caching is used, the latter only available for semilinear problems.\nm - integer, default: 30. Controls the size of Krylov subsapce.\niop - integer, default: 0. If not zero, determines the length of the incomplete orthogonalization procedure (IOP) [1]. Note that if the linear operator/jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\nautodiff and chunksize: autodiff control if problem is not semilinear and explicit jacobian is not given. See Extra Options for more details.","category":"page"},{"location":"solvers/ode_solve/#Adaptive-Exponential-Rosenbrock-Methods","page":"ODE Solvers","title":"Adaptive Exponential Rosenbrock Methods","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Exprb32 - 3rd order adaptive Exponential-Rosenbrock scheme.\nExprb43 - 4th order adaptive Exponential-Rosenbrock scheme.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The exponential rosenbrock methods cannot be applied to semilinear problems. Options for the solvers are the same as Exponential Runge-Kutta Methods except that Krylov approximation is always used.","category":"page"},{"location":"solvers/ode_solve/#Exponential-Propagation-Iterative-Runge-Kutta-Methods-(EPIRK)","page":"ODE Solvers","title":"Exponential Propagation Iterative Runge-Kutta Methods (EPIRK)","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"These methods are all fixed timestepping only.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Exp4 - 4th order EPIRK scheme.\nEPIRK4s3A - 4th order EPIRK scheme with stiff order 4.\nEPIRK4s3B - 4th order EPIRK scheme with stiff order 4.\nEPIRK5P1 - 5th order EPIRK scheme.\nEPIRK5P2 - 5th order EPIRK scheme.\nEPIRK5s3 - 5th order \"horizontal\" EPIRK scheme with stiff order 5. Broken.\nEXPRB53s3- 5th order EPIRK scheme with stiff order 5.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Options:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"adaptive_krylov - boolean, default: true. Determines if the adaptive Krylov algorithm with timestepping of Neisen & Wright is used.\nm - integer, default: 30. Controls the size of Krylov subsapce, or the size for the first step if adaptive_krylov=true.\niop - integer, default: 0. If not zero, determines the length of the incomplete orthogonalization procedure (IOP) [1]. Note that if the linear operator/jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.\nautodiff and chunksize: autodiff control if problem is not semilinear and explicit jacobian is not given. See Extra Options for more details.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"It should be noted that many of the methods are still at an experimental stage of development, and thus should be used with caution.","category":"page"},{"location":"solvers/ode_solve/#Multistep-Methods","page":"ODE Solvers","title":"Multistep Methods","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Quasi-constant stepping is the time stepping strategy which matches the classic GEAR, LSODE,  and ode15s integrators. The variable-coefficient methods match the ideas of the classic EPISODE integrator and early VODE designs. The Fixed Leading Coefficient (FLC) methods match the behavior of the classic VODE and Sundials CVODE integrator.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"QNDF1 - An adaptive order 1 quasi-constant timestep L-stable numerical differentiation function (NDF) method. Optional parameter kappa defaults to Shampine's accuracy-optimal -0.1850.\nQBDF1 - An adaptive order 1 L-stable BDF method. This is equivalent to implicit Euler but using the BDF error estimator.\nABDF2 - An adaptive order 2 L-stable fixed leading coefficient multistep BDF method.\nQNDF2 - An adaptive order 2 quasi-constant timestep L-stable numerical differentiation function (NDF) method.\nQBDF2 - An adaptive order 2 L-stable BDF method using quasi-constant timesteps.\nQNDF - An adaptive order quasi-constant timestep NDF method. Utilizes Shampine's accuracy-optimal kappa values as defaults (has a keyword argument for a tuple of kappa coefficients).\nQBDF - An adaptive order quasi-constant timestep BDF method.\nJVODE_BDF - An adaptive time adaptive order fixed-leading coefficient BDF method in Nordsieck form. In development.\nMEBDF2 - The second order Modified Extended BDF method, which has improved stability properties over the standard BDF. Fixed timestep only.","category":"page"},{"location":"solvers/ode_solve/#Implicit-Strong-Stability-Preserving-Runge-Kutta-Methods-for-Hyperbolic-PDEs-(Conservation-Laws)","page":"ODE Solvers","title":"Implicit Strong-Stability Preserving Runge-Kutta Methods for Hyperbolic PDEs (Conservation Laws)","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"SSPSDIRK2 - A second order A-L stable symplectic SDIRK method with the strong stability preserving (SSP) property (SSP coefficient 2). Fixed timestep only.","category":"page"},{"location":"solvers/ode_solve/#Extra-Options","page":"ODE Solvers","title":"Extra Options","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"All of the Rosenbrock and SDIRK methods allow for specification of linsolve: the linear solver which is used. For more information on specifying the linear solver, see the manual page on solver specification.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that performance overload information (Jacobians etc.) are not used in this mode. This can control autodifferentiation of the Jacobian as well. For more information on specifying the nonlinear solver, see the manual page on solver specification.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Additionally, the Rosenbrock and SDIRK methods have differentiation controls. In each of these, autodiff can be set to turn on/off autodifferentiation, and chunk_size can be used to set the chunksize of the Dual  numbers (see the documentation for ForwardDiff.jl for details). In addition, the Rosenbrock and SDIRK methods can set diff_type, which is the type of numerical differentiation that is used (when autodifferentiation is disabled). The choices are Val{:central}, Val{:forward} or Val{:complex}.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Examples:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"sol = solve(prob,Rosenbrock23()) # Standard, uses autodiff\nsol = solve(prob,Rosenbrock23(chunk_size=10)) # Autodiff with chunksize of 10\nsol = solve(prob,Rosenbrock23(autodiff=false)) # Numerical differentiation with central differencing\nsol = solve(prob,Rosenbrock23(autodiff=false,diff_type=Val{:forward})) # Numerical differentiation with forward differencing","category":"page"},{"location":"solvers/ode_solve/#Tableau-Method","page":"ODE Solvers","title":"Tableau Method","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Additionally, there is the tableau method:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"ExplicitRK - A general Runge-Kutta solver which takes in a tableau. Can be adaptive. Tableaus are specified via the keyword argument tab=tableau. The default tableau is for Dormand-Prince 4/5. Other supplied tableaus can be found in the Supplied Tableaus section.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Example usage:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"alg = ExplicitRK(tableau=constructDormandPrince())\nsolve(prob,alg)","category":"page"},{"location":"solvers/ode_solve/#CompositeAlgorithm","page":"ODE Solvers","title":"CompositeAlgorithm","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"One unique feature of OrdinaryDiffEq.jl is the CompositeAlgorithm, which allows you to, with very minimal overhead, design a multimethod which switches between chosen algorithms as needed. The syntax is CompositeAlgorithm(algtup,choice_function) where algtup is a tuple of OrdinaryDiffEq.jl algorithms, and choice_function is a function which declares which method to use in the following step. For example, we can design a multimethod which uses Tsit5() but switches to Vern7() whenever dt is too small:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"choice_function(integrator) = (Int(integrator.dt<0.001) + 1)\nalg_switch = CompositeAlgorithm((Tsit5(),Vern7()),choice_function)","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The choice_function takes in an integrator and thus all of the features available in the Integrator Interface can be used in the choice function.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"A helper algorithm was created for building 2-method automatic switching for stiffness detection algorithms. This is the AutoSwitch algorithm with the following options:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"AutoSwitch(nonstiffalg::nAlg, stiffalg::sAlg;\n           maxstiffstep=10, maxnonstiffstep=3,\n           nonstifftol::T=9//10, stifftol::T=9//10,\n           dtfac=2.0, stiffalgfirst=false)","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The nonstiffalg must have an appropriate stiffness estimate built into the method. The stiffalg can receive its estimate from the Jacobian calculation. maxstiffstep is the number of stiffness detects before switching to the stiff algorithm and maxnonstiffstep is vice versa. nonstifftol and stifftol are the tolerances associated with the stiffness comparison against the stability region. Decreasing stifftol makes switching to the non-stiff algorithm less likely. Decreasing nonstifftol makes switching to the stiff algorithm more likely. dtfac is the factor that dt is changed when switching: multiplied when going from non-stiff to stiff and divided when going stiff to non-stiff. stiffalgfirst denotes whether the first step should use the stiff algorithm.","category":"page"},{"location":"solvers/ode_solve/#Pre-Built-Stiffness-Detecting-and-Auto-Switching-Algorithms","page":"ODE Solvers","title":"Pre-Built Stiffness Detecting and Auto-Switching Algorithms","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"These methods require a Autoalg(stiffalg) to be chosen as the method to switch to when the ODE is stiff. It can be any of the OrdinaryDiffEq.jl one-step stiff methods and has all of the arguments of the AutoSwitch algorithm.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"AutoTsit5 - Tsit5 with automated switching.\nAutoDP5 - DP5 with automated switching.\nAutoVern6 - Vern6 with automated switching.\nAutoVern7 - Vern7 with automated switching.\nAutoVern8 - Vern8 with automated switching.\nAutoVern9 - Vern9 with automated switching.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Example:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"tsidas_alg = AutoTsit5(Rodas5())\nsol = solve(prob,tsidas_alg)\n\ntsidas_alg = AutoTsit5(Rodas5(),nonstifftol = 11/10)","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Is the Tsit5 method with automatic switching to Rodas5.","category":"page"},{"location":"solvers/ode_solve/#ode_solve_sundials","page":"ODE Solvers","title":"Sundials.jl","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use Sundials.jl:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add Sundials\nusing Sundials","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The Sundials suite is built around multistep methods. These methods are more efficient than other methods when the cost of the function calculations is really high, but for less costly functions the cost of nurturing the timestep overweighs the benefits. However, the BDF method is a classic method for stiff equations and \"generally works\".","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"CVODE_BDF - CVode Backward Differentiation Formula (BDF) solver.\nCVODE_Adams - CVode Adams-Moulton solver.\nARKODE - Explicit and ESDIRK Runge-Kutta methods of orders 2-8 depending on choice of options.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The Sundials algorithms all come with a 3rd order Hermite polynomial interpolation. Note that the constructors for the Sundials algorithms take two main arguments:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"method - This is the method for solving the implicit equation. For BDF this defaults to :Newton while for Adams this defaults to :Functional. These choices match the recommended pairing in the Sundials.jl manual. However, note that using the :Newton method may take less iterations but requires more memory than the :Function iteration approach.\nlinear_solver - This is the linear solver which is used in the :Newton method.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The choices for the linear solver are:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":":Dense - A dense linear solver.\n:Band - A solver specialized for banded Jacobians. If used, you must set the position of the upper and lower non-zero diagonals via jac_upper and jac_lower.\n:LapackDense - A version of the dense linear solver that uses the Julia-provided OpenBLAS-linked LAPACK for multithreaded operations. This will be faster than :Dense on larger systems but has noticable overhead on smaller (<100 ODE) systems.\n:LapackBand - A version of the banded linear solver that uses the Julia-provided OpenBLAS-linked LAPACK for multithreaded operations. This will be faster than :Band on larger systems but has noticable overhead on smaller (<100 ODE) systems.\n:Diagonal - This method is specialized for diagonal Jacobians.\n:GMRES - A GMRES method. Recommended first choice Krylov method\n:BCG - A Biconjugate gradient method.\n:PCG - A preconditioned conjugate gradient method. Only for symmetric linear systems.\n:TFQMR - A TFQMR method.\n:KLU - A sparse factorization method. Requires that the user specifies a Jacobian. The Jacobian must be set as a sparse matrix in the ODEProblem type.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Example:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"CVODE_BDF() # BDF method using Newton + Dense solver\nCVODE_BDF(method=:Functional) # BDF method using Functional iterations\nCVODE_BDF(linear_solver=:Band,jac_upper=3,jac_lower=3) # Banded solver with nonzero diagonals 3 up and 3 down\nCVODE_BDF(linear_solver=:BCG) # Biconjugate gradient method","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The main options for ARKODE are the choice between explicit and implicit and the method order, given via:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"ARKODE(Sundials.Explicit()) # Solve with explicit tableau of default order 4\nARKODE(Sundials.Implicit(),order = 3) # Solve with explicit tableau of order 3","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The order choices for explicit are 2 through 8 and for implicit 3 through 5. Specific methods can also be set through the etable and itable options for explicit and implicit tableaus respectively. The available tableaus are:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"etable:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"HEUN_EULER_2_1_2: 2nd order Heun's method\nBOGACKI_SHAMPINE_4_2_3:\nARK324L2SA_ERK_4_2_3: explicit portion of Kennedy and Carpenter's 3rd order method\nZONNEVELD_5_3_4: 4th order explicit method\nARK436L2SA_ERK_6_3_4: explicit portion of Kennedy and Carpenter's 4th order method\nSAYFY_ABURUB_6_3_4: 4th order explicit method\nCASH_KARP_6_4_5: 5th order explicit method\nFEHLBERG_6_4_5: Fehlberg's classic 5th order method\nDORMAND_PRINCE_7_4_5: the classic 5th order Dormand-Prince method\nARK548L2SA_ERK_8_4_5: explicit portion of Kennedy and Carpenter's 5th order method\nVERNER_8_5_6: Verner's classic 5th order method\nFEHLBERG_13_7_8: Fehlberg's 8th order method","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"itable:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"SDIRK_2_1_2: An A-B-stable 2nd order SDIRK method\nBILLINGTON_3_3_2: A second order method with a 3rd order error predictor of less stability\nTRBDF2_3_3_2: The classic TR-BDF2 method\nKVAERNO_4_2_3: an L-stable 3rd order ESDIRK method\nARK324L2SA_DIRK_4_2_3: implicit portion of Kennedy and Carpenter's 3th order method\nCASH_5_2_4: Cash's 4th order L-stable SDIRK method\nCASH_5_3_4: Cash's 2nd 4th order L-stable SDIRK method\nSDIRK_5_3_4: Hairer's 4th order SDIRK method\nKVAERNO_5_3_4: Kvaerno's 4th order ESDIRK method\nARK436L2SA_DIRK_6_3_4: implicit portion of Kennedy and Carpenter's 4th order method\nKVAERNO_7_4_5: Kvaerno's 5th order ESDIRK method\nARK548L2SA_DIRK_8_4_5: implicit portion of Kennedy and Carpenter's 5th order method","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"These can be set for example via:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"ARKODE(Sundials.Explicit(),etable = Sundials.DORMAND_PRINCE_7_4_5)\nARKODE(Sundials.Implicit(),itable = Sundials.KVAERNO_4_2_3)","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"All of the additional options are available. The full constructor is:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"CVODE_BDF(;method=:Newton,linear_solver=:Dense,\n          jac_upper=0,jac_lower=0,\n          stored_upper = jac_upper + jac_lower,\n          non_zero=0,krylov_dim=0,\n          stability_limit_detect=false,\n          max_hnil_warns = 10,\n          max_order = 5,\n          max_error_test_failures = 7,\n          max_nonlinear_iters = 3,\n          max_convergence_failures = 10,\n          prec = nothing, prec_side = 0)\n\nCVODE_Adams(;method=:Functional,linear_solver=:None,\n            jac_upper=0,jac_lower=0,\n            stored_upper = jac_upper + jac_lower,\n            krylov_dim=0,\n            stability_limit_detect=false,\n            max_hnil_warns = 10,\n            max_order = 12,\n            max_error_test_failures = 7,\n            max_nonlinear_iters = 3,\n            max_convergence_failures = 10,\n            prec = nothing, psetup = nothing, prec_side = 0)\n\nARKODE(stiffness=Sundials.Implicit();\n      method=:Newton,linear_solver=:Dense,\n      jac_upper=0,jac_lower=0,stored_upper = jac_upper+jac_lower,\n      non_zero=0,krylov_dim=0,\n      max_hnil_warns = 10,\n      max_error_test_failures = 7,\n      max_nonlinear_iters = 3,\n      max_convergence_failures = 10,\n      predictor_method = 0,\n      nonlinear_convergence_coefficient = 0.1,\n      dense_order = 3,\n      order = 4,\n      set_optimal_params = false,\n      crdown = 0.3,\n      dgmax = 0.2,\n      rdiv = 2.3,\n      msbp = 20,\n      adaptivity_method = 0,\n      prec = nothing, psetup = nothing, prec_side = 0\n      )","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"See the CVODE manual and the ARKODE manual for details on the additional options.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that here prec is a preconditioner function prec(z,r,p,t,y,fy,gamma,delta,lr) where:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"z: the computed output vector\nr: the right-hand side vector of the linear system\np: the parameters\nt: the current independent variable\ndu: the current value of f(u,p,t)\ngamma: the gamma of W = M - gamma*J\ndelta: the iterative method tolerance\nlr: a flag for whether lr=1 (left) or lr=2 (right) preconditioning","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"and psetup is the preconditioner setup function for pre-computing Jacobian information. Where:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"p: the parameters\nt: the current independent variable\nu: the current state\ndu: the current f(u,p,t)\njok: a bool indicating whether the Jacobian needs to be updated\njcurPtr: a reference to an Int for whether the Jacobian was updated. jcurPtr[]=true should be set if the Jacobian was updated, and jcurPtr[]=false should be set if the Jacobian was not updated.\ngamma: the gamma of W = M - gamma*J","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"psetup is optional when prec is set.","category":"page"},{"location":"solvers/ode_solve/#ODEInterface.jl","page":"ODE Solvers","title":"ODEInterface.jl","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The ODEInterface algorithms are the classic Fortran algorithms. While the non-stiff algorithms are superseded by the more featured and higher performance Julia implementations from OrdinaryDiffEq.jl, the stiff solvers such as radau are some of the most efficient methods available (but are restricted for use on arrays of Float64).","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use ODEInterfaceDiffEq.jl:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add ODEInterfaceDiffEq\nusing ODEInterfaceDiffEq","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"dopri5 - Hairer's classic implementation of the Dormand-Prince 4/5 method.\ndop853 - Explicit Runge-Kutta 8(5,3) by Dormand-Prince.\nodex - GBS extrapolation-algorithm based on the midpoint rule.\nseulex - Extrapolation-algorithm based on the linear implicit Euler method.\nradau - Implicit Runge-Kutta (Radau IIA) of variable order between 5 and 13.\nradau5 - Implicit Runge-Kutta method (Radau IIA) of order 5.\nrodas - Rosenbrock 4(3) method.\nddeabm - Adams-Bashforth-Moulton Predictor-Corrector method (order between 1 and 12)\nddebdf - Backward Differentiation Formula (orders between 1 and 5)","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that while the output only has a linear interpolation, a higher order interpolation is used for intermediate dense output for saveat and for event handling.","category":"page"},{"location":"solvers/ode_solve/#LSODA.jl","page":"ODE Solvers","title":"LSODA.jl","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"This setup provides a wrapper to the algorithm LSODA, a well-known method which uses switching to solve both stiff and non-stiff equations.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"lsoda - The LSODA wrapper algorithm.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use LSODA.jl:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add LSODA\nusing LSODA","category":"page"},{"location":"solvers/ode_solve/#IRKGaussLegendre.jl","page":"ODE Solvers","title":"IRKGaussLegendre.jl","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"This setup provides a specific solver, IRKGL16, which is a 16th order Symplectic Gauss-Legendre scheme. This scheme is highly efficient for precise integration of ODEs, specifically ODEs derived from Hamiltonian systems.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use IRKGaussLegendre.jl:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add IRKGaussLegendre\nusing IRKGaussLegendre","category":"page"},{"location":"solvers/ode_solve/#SimpleDiffEq.jl","page":"ODE Solvers","title":"SimpleDiffEq.jl","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"This setup provides access to simplified versions of a few ODE solvers. They mostly exist for experimentation, but offer shorter compile times. They have limitations compared to OrdinaryDiffEq.jl and are not generally faster.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"SimpleTsit5 - A fixed timestep integrator form of Tsit5. Not compatible with events.\nSimpleATsit5 - An adaptive Tsit5 with an interpolation in its simplest form. Not compatible with events.\nGPUSimpleATsit5 - A version of SimpleATsit5 without the integrator interface. Only allows solve.\nSimpleRK4 - A fixed timestep barebones RK4 implementation with integrators.\nLoopRK4 - A fixed timestep barebones RK4. Not compatible with events or the integrator interface.\nGPURK4 - A fully static RK4 for specialized compilation to accelerators like GPUs and TPUs.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use SimpleDiffEq.jl:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add SimpleDiffEq\nusing SimpleDiffEq","category":"page"},{"location":"solvers/ode_solve/#ODE.jl","page":"ODE Solvers","title":"ODE.jl","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use ODE.jl:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add ODE\nusing ODE","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"ode23 - Bogacki-Shampine's order 2/3 Runge-Kutta  method\node45 - A Dormand-Prince order 4/5 Runge-Kutta method\node23s - A modified Rosenbrock order 2/3 method due to Shampine\node78 - A Fehlburg order 7/8 Runge-Kutta method\node4 - The classic Runge-Kutta order 4 method\node4ms - A fixed-step, fixed order Adams-Bashforth-Moulton method†\node4s - A 4th order Rosenbrock method due to Shampine","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"†: Does not step to the interval endpoint. This can cause issues with discontinuity detection, and discrete variables need to be updated appropriately.","category":"page"},{"location":"solvers/ode_solve/#MATLABDiffEq.jl","page":"ODE Solvers","title":"MATLABDiffEq.jl","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use MATLABDiffEq.jl:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add https://github.com/JuliaDiffEq/MATLABDiffEq.jl\nusing MATLABDiffEq","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"This requires a licensed MATLAB installation. The available methods are:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"MATLABDiffEq.ode23\nMATLABDiffEq.ode45\nMATLABDiffEq.ode113\nMATLABDiffEq.ode23s\nMATLABDiffEq.ode23t\nMATLABDiffEq.ode23tb\nMATLABDiffEq.ode15s\nMATLABDiffEq.ode15i","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"For more information on these algorithms, see the MATLAB documentation.","category":"page"},{"location":"solvers/ode_solve/#SciPyDiffEq.jl","page":"ODE Solvers","title":"SciPyDiffEq.jl","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"SciPyDiffEq.jl is a wrapper over SciPy for easing the transition of new users (same exact results!) and benchmarking. This wrapper uses Julia's JIT acceleration to accelerate about 3x over SciPy+Numba, but it is still around 1000x slower than the pure-Julia methods and thus should probably be used sparingly.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use SciPyDiffEq.jl:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add https://github.com/JuliaDiffEq/SciPyDiffEq.jl\nusing SciPyDiffEq","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The available methods are:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"SciPyDiffEq.RK45\nSciPyDiffEq.RK23\nSciPyDiffEq.Radau\nSciPyDiffEq.BDF\nSciPyDiffEq.LSODA","category":"page"},{"location":"solvers/ode_solve/#deSolveDiffEq.jl","page":"ODE Solvers","title":"deSolveDiffEq.jl","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"deSolveDiffEq.jl is a wrapper over R's deSolve for easing the transition of new users (same exact results!) and benchmarking. This wrapper is around 1000x slower than the pure-Julia methods (~2x-3x overhead from directly using R) and thus should probably be used sparingly.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use deSolveDiffEq.jl:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add https://github.com/JuliaDiffEq/deSolveDiffEq.jl\nusing deSolveDiffEq","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"The available methods are:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"deSolveDiffEq.lsoda\ndeSolveDiffEq.lsode\ndeSolveDiffEq.lsodes\ndeSolveDiffEq.lsodar\ndeSolveDiffEq.vode\ndeSolveDiffEq.daspk\ndeSolveDiffEq.euler\ndeSolveDiffEq.rk4\ndeSolveDiffEq.ode23\ndeSolveDiffEq.ode45\ndeSolveDiffEq.radau\ndeSolveDiffEq.bdf\ndeSolveDiffEq.bdf_d\ndeSolveDiffEq.adams\ndeSolveDiffEq.impAdams\ndeSolveDiffEq.impAdams_d","category":"page"},{"location":"solvers/ode_solve/#GeometricIntegrators.jl","page":"ODE Solvers","title":"GeometricIntegrators.jl","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"GeometricIntegrators.jl is a set of fixed timestep algorithms written in Julia. Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use GeometricIntegratorsDiffEq.jl:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add https://github.com/JuliaDiffEq/GeometricIntegratorsDiffEq.jl\nusing GeometricIntegratorsDiffEq","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"GIEuler - 1st order Euler method\nGIMidpoint - 2nd order explicit midpoint method\nGIHeun2 - 2nd order Heun's method\nGIRalston2 - 2nd order Ralston's method\nGIHeun3 - 3rd order Heun's method\nGIRalston3 - 3rd order Ralston's method\nGIRunge - 3rd order Kutta's method\nGIKutta - 3rd order Kutta's method\nGIRK4 - standard 4th order Runge-Kutta\nGIRK416\nGIRK438 - 4th order Runge-Kutta, 3/8's rule\nGIImplicitEuler - 1st order implicit Euler method\nGIImplicitMidpoint - 2nd order implicit midpoint method\nGIRadauIA(s) - s-stage Radau-IA\nGIRadauIIA(s) - s-stage Radau-IA\nGILobattoIIIA(s)\nGILobattoIIIB(s)\nGILobattoIIIC(s)\nGILobattoIIIC̄(s)\nGILobattoIIID(s)\nGILobattoIIIE(s)\nGILobattoIIIF(s)\nGISRK3 - 3-stage order 4 symmetric Runge-Kutta method\nGISSPRK3 - 3rd orer explicit SSP method\n`GICrankNicholson\nGIKraaijevangerSpijker\nGIQinZhang\nGICrouzeix\nGIGLRK(s) - Gauss-Legendre Runge-Kutta method of order 2s","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that all of these methods require the user supplies dt.","category":"page"},{"location":"solvers/ode_solve/#BridgeDiffEq.jl","page":"ODE Solvers","title":"BridgeDiffEq.jl","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Bridge.jl is a set of fixed timestep algorithms written in Julia. These methods are made and optimized for out-of-place functions on immutable (static vector) types. Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use BridgeDiffEq.jl:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add https://github.com/JuliaDiffEq/BridgeDiffEq.jl\nusing BridgeDiffEq","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"BridgeR3 - 3rd order Ralston method\nBridgeBS3 - 3rd order Bogacki-Shampine method","category":"page"},{"location":"solvers/ode_solve/#TaylorIntegration.jl","page":"ODE Solvers","title":"TaylorIntegration.jl","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"TaylorIntegration.jl is a pure-Julia implementation of an adaptive order Taylor series method for high accuracy integration of ODEs. These methods are optimized when the absolute tolerance is required to be very low. Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use TaylorIntegration.jl:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add TaylorIntegration\nusing TaylorIntegration","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"TaylorMethod(order) - Taylor integration method with maximal order (required)","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note: this method is much faster if you put @taylorize on your derivative function!","category":"page"},{"location":"solvers/ode_solve/#QuDiffEq.jl","page":"ODE Solvers","title":"QuDiffEq.jl","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"QuDiffEq.jl is a package for solving differential equations using quantum algorithm. It makes use of the Yao framework for simulating quantum circuits.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use QuDiffEq.jl:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add https://github.com/QuantumBFS/QuDiffEq.jl\nusing QuDiffEq","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"QuLDE(k) - Algorithm based on truncated Taylor series. The method linearizes a system of non-linear differential equations and solves the resultant by means of a quantum circuit. k selects the order in the Taylor series aprroximation (for the quantum circuit).\nQuNLDE(k,ϵ)- Algorithm uses forward Euler to solve quadratc differential equations. k selects the order in the Taylor series aprroximation (for the quantum circuit). ϵ sets the precision for Hamiltonian evolution.","category":"page"},{"location":"solvers/ode_solve/#NeuralPDE.jl","page":"ODE Solvers","title":"NeuralPDE.jl","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"This method trains a neural network using Flux.jl to approximate the solution of the ODE. Currently this method isn't competitive but it is a fun curiosity that will be improved with future integration with Zygote.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use NeuralPDE.jl:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add NeuralPDE\nusing NeuralPDE","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"nnode(chain,opt=ADAM(0.1)) - Defines a neural network solver which utilizes a Flux.jl chain under the hood which must be supplied by the user. Defaults to using the ADAM optimization method, but the user can pass any Flux.jl optimizer.","category":"page"},{"location":"solvers/ode_solve/#List-of-Supplied-Tableaus","page":"ODE Solvers","title":"List of Supplied Tableaus","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"A large variety of tableaus have been supplied by default, via DiffEqDevTools.jl. The list of tableaus can be found in the developer docs. To use them, note you must install the library:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add DiffEqDevTools\nusing DiffEqDevTools","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"For the most useful and common algorithms, a hand-optimized version is supplied in OrdinaryDiffEq.jl which is recommended for general uses (i.e. use DP5 instead of ExplicitRK with tableau=constructDormandPrince()). However, these serve as a good method for comparing between tableaus and understanding the pros/cons of the methods. Implemented are every published tableau (that I know exists). Note that user-defined tableaus also are accepted. To see how to define a tableau, checkout the premade tableau source code. Tableau docstrings should have appropriate citations (if not, file an issue).","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Plot recipes are provided which will plot the stability region for a given tableau.","category":"page"},{"location":"solvers/ode_solve/#ProbNumDiffEq.jl","page":"ODE Solvers","title":"ProbNumDiffEq.jl","text":"","category":"section"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"ProbNumDiffEq.jl provides probabilistic numerical solvers for ODEs. By casting the solution of ODEs as a problem of Bayesian inference, they return a posterior probability distribution over ODE solutions and thereby provide estimates of their own numerical approximation error. The solvers have adaptive timestepping, their order can be freely specified, and the returned posterior distribution naturally enables dense output and sampling. The full documentation is available at ProbNumDiffEq.jl.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use ProbNumDiffEq.jl:","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"]add ProbNumDiffEq\nusing ProbNumDiffEq","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"EK1(order=3) - A semi-implicit ODE solver based on extended Kalman filtering and smoothing with first order linearization. Recommended, but requires that the Jacobian of the vector field is specified.\nEK0(order=3) - An explicit ODE solver based on extended Kalman filtering and smoothing with zeroth order linearization.","category":"page"},{"location":"solvers/ode_solve/","page":"ODE Solvers","title":"ODE Solvers","text":"[1]: Koskela, A. (2015). Approximating the matrix exponential of an advection-diffusion operator using the incomplete orthogonalization method. In Numerical Mathematics and Advanced Applications-ENUMATH 2013 (pp. 345-353). Springer, Cham.","category":"page"},{"location":"solvers/steady_state_solve/#Steady-State-Solvers","page":"Steady State Solvers","title":"Steady State Solvers","text":"","category":"section"},{"location":"solvers/steady_state_solve/","page":"Steady State Solvers","title":"Steady State Solvers","text":"solve(prob::SteadyStateProblem,alg;kwargs)","category":"page"},{"location":"solvers/steady_state_solve/","page":"Steady State Solvers","title":"Steady State Solvers","text":"Solves for the steady states in the problem defined by prob using the algorithm alg. If no algorithm is given, a default algorithm will be chosen.","category":"page"},{"location":"solvers/steady_state_solve/#Recommended-Methods","page":"Steady State Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"solvers/steady_state_solve/","page":"Steady State Solvers","title":"Steady State Solvers","text":"DynamicSS is a good choice if you think you may have multiple steady states or a bad initial guess. SSRootfind can be faster if you have a good initial guess. For DynamicSS, in many cases an adaptive stiff solver, like a Rosenbrock or BDF method (Rodas5 or QNDF), is a good way to allow for very large time steps as the steady state approaches.","category":"page"},{"location":"solvers/steady_state_solve/","page":"Steady State Solvers","title":"Steady State Solvers","text":"Note that if you use CVODE_BDF you may need to give a starting dt via dt=.....","category":"page"},{"location":"solvers/steady_state_solve/#Full-List-of-Methods","page":"Steady State Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"solvers/steady_state_solve/#SteadyStateDiffEq.jl","page":"Steady State Solvers","title":"SteadyStateDiffEq.jl","text":"","category":"section"},{"location":"solvers/steady_state_solve/","page":"Steady State Solvers","title":"Steady State Solvers","text":"SSRootfind : Uses a rootfinding algorithm to find a steady state. Defaults to using NLsolve.jl. A different algorithm can be specified via the nlsolve keyword argument.\nDynamicSS : Uses an ODE solver to find the steady state. Automatically terminates when close to the steady state. DynamicSS(alg;abstol=1e-8,reltol=1e-6,tspan=Inf) requires that an ODE algorithm is given as the first argument.  The absolute and relative tolerances specify the termination conditions on the derivative's closeness to zero.  This internally uses the TerminateSteadyState callback from the Callback Library.  The simulated time for which given ODE is solved can be limited by tspan.  If tspan is a number, it is equivalent to passing (zero(tspan), tspan).","category":"page"},{"location":"solvers/steady_state_solve/","page":"Steady State Solvers","title":"Steady State Solvers","text":"Example usage:","category":"page"},{"location":"solvers/steady_state_solve/","page":"Steady State Solvers","title":"Steady State Solvers","text":"sol = solve(prob,SSRootfind())\nsol = solve(prob,DynamicSS(Tsit5()))\nusing Sundials\nsol = solve(prob,DynamicSS(CVODE_BDF()),dt=1.0)","category":"page"},{"location":"features/performance_overloads/#performance_overloads","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"","category":"section"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"The DiffEq ecosystem provides an extensive interface for declaring extra functions associated with the differential equation's data. In traditional libraries there is usually only one option: the Jacobian. However, we allow for a large array of pre-computed functions to speed up the calculations. This is offered via the DiffEqFunction types which can be passed to the problems.","category":"page"},{"location":"features/performance_overloads/#Function-Type-Definitions","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"Function Type Definitions","text":"","category":"section"},{"location":"features/performance_overloads/#Function-Choice-Definitions","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"Function Choice Definitions","text":"","category":"section"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"The full interface available to the solvers is as follows:","category":"page"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"jac: The Jacobian of the differential equation with respect to the state variable u at a time t with parameters p.\ntgrad: The gradient of the differential equation with respect to t at state u with parameters p.\nparamjac: The Jacobian of the differential equation with respect to p at state u at time t.\nanalytic: Defines an analytical solution using u0 at time t with p which will cause the solvers to return errors. Used for testing.\nWfact: The LU-factorization of M - gamma*J where J is the jac.\nWfact_t: The LU-factorization of M/gamma - J where J is the jac.\nggprime: See the definition in the SDEProblem page.\nsyms: Allows you to name your variables for automatic names in plots and other output.","category":"page"},{"location":"features/performance_overloads/#ODEFunction","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"ODEFunction","text":"","category":"section"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"function ODEFunction{iip,recompile}(f;\n                 mass_matrix=I,\n                 analytic=nothing, # (u0,p,t)\n                 tgrad=nothing, # (dT,u,p,t) or (u,p,t)\n                 jac=nothing, # (J,u,p,t) or (u,p,t)\n                 jac_prototype=nothing, # Type for the Jacobian\n                 Wfact=nothing, # (iW,u,p,gamma,t) or (u,p,gamma,t)\n                 Wfact_t=nothing, # (iW,u,p,gamma,t) or (u,p,gamma,t)\n                 paramjac = nothing, # (pJ,u,p,t) or (u,p,t)\n                 colorvec = nothing,\n                 syms = nothing) # collection of names for variables","category":"page"},{"location":"features/performance_overloads/#DynamicalODEFunction","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DynamicalODEFunction","text":"","category":"section"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"DynamicalODEFunction{iip,recompile}(f1, # (du,u,v,p,t) or (u,v,p,t)\n                                    f2; # (du,u,v,p,t) or (u,v,p,t)\n                                    mass_matrix=(I,I), # Mass matrix for each partition\n                                    analytic=nothing)","category":"page"},{"location":"features/performance_overloads/#SplitFunction","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"SplitFunction","text":"","category":"section"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"SplitFunction{iip,recompile}(f1, # ODEFunction\n                        f2; # ODEFunction\n                        mass_matrix=I,\n                        _func_cache=nothing, # This is a cache used in f = f1+f2\n                        analytic=nothing)","category":"page"},{"location":"features/performance_overloads/#SDEFunction","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"SDEFunction","text":"","category":"section"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"function SDEFunction{iip,recompile}(f,g;\n                 mass_matrix=I,\n                 analytic=nothing,\n                 tgrad=nothing,\n                 jac=nothing,\n                 jac_prototype=nothing,\n                 Wfact=nothing,\n                 Wfact_t=nothing,\n                 paramjac = nothing,\n                 ggprime = nothing,\n                 colorvec = nothing,\n                 syms = nothing)","category":"page"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"Notice here that the Jacobian refers to the Jacobian of the drift function f. The sparsity, colorvec, etc. all refer to this Jacobian. ggprime refers to g(u,p,t)*Dg(u,p,t).","category":"page"},{"location":"features/performance_overloads/#SplitSDEFunction","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"SplitSDEFunction","text":"","category":"section"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"SplitSDEFunction{iip,recompile}(f1, # ODEFunction\n                           f2, # ODEFunction\n                           g;\n                           mass_matrix=I,\n                           _func_cache=nothing,\n                           analytic=nothing)","category":"page"},{"location":"features/performance_overloads/#RODEFunction","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"RODEFunction","text":"","category":"section"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"function RODEFunction{iip,recompile}(f;\n                 mass_matrix=I,\n                 analytic=nothing,\n                 tgrad=nothing,\n                 jac=nothing,\n                 jac_prototype=nothing,\n                 Wfact=nothing,\n                 Wfact_t=nothing,\n                 paramjac = nothing,\n                 colorvec = nothing,\n                 syms = nothing)","category":"page"},{"location":"features/performance_overloads/#DAEFunction","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DAEFunction","text":"","category":"section"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"function DAEFunction{iip,recompile}(f;\n                 analytic=nothing,\n                 tgrad=nothing,\n                 jac=nothing, # (J,du,u,p,gamma,t) or (du,u,p,gamma,t)\n                 jac_prototype=nothing,\n                 Wfact=nothing,\n                 Wfact_t=nothing,\n                 paramjac = nothing,\n                 syms = nothing)","category":"page"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"Note that the Jacobian of a DAE is defined as gamma*dG/d(du) + dG/du where gamma is given by the solver.","category":"page"},{"location":"features/performance_overloads/#DDEFunction","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DDEFunction","text":"","category":"section"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"function DDEFunction{iip,recompile}(f;\n                 mass_matrix=I,\n                 analytic=nothing,\n                 tgrad=nothing,\n                 jac=nothing,\n                 jac_prototype=nothing,\n                 Wfact=nothing,\n                 Wfact_t=nothing,\n                 paramjac = nothing,\n                 colorvec = nothing,\n                 syms = nothing)","category":"page"},{"location":"features/performance_overloads/#Inplace-Specification-and-No-Recompile-Mode","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"Inplace Specification and No-Recompile Mode","text":"","category":"section"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"Each DiffEqFunction type can be called with an \"is inplace\" (iip) choice.","category":"page"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"ODEFunction(f)\nODEFunction{iip}(f)","category":"page"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"which is a boolean for whether the function is in the inplace form (mutating to change the first value). This is automatically determined using the methods table but note that for full type-inferrability of the DEProblem this iip-ness should be specified.","category":"page"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"Additionally, the functions are fully specialized to reduce the runtimes. If one would instead like to not specialize on the functions to reduce compile time, then one can set recompile to false.","category":"page"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"ODEFunction{iip,false}(f)","category":"page"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"This makes the ODE solver compilation independent of the function and so changing the function will not cause recompilation. One can change the default value by changing the const RECOMPILE_BY_DEFAULT = true to false in the SciMLBase.jl source code.","category":"page"},{"location":"features/performance_overloads/#Specifying-Jacobian-Types","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"Specifying Jacobian Types","text":"","category":"section"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"The jac field of an inplace style DiffEqFunction has the signature jac(J,u,p,t), which updates the jacobian J in-place. The intended type for J can sometimes be inferred (e.g. when it is just a dense Matrix), but not in general. To supply the type information, you can provide a jac_prototype in the function's constructor.","category":"page"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"The following example creates an inplace ODEFunction whose jacobian is a Diagonal:","category":"page"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"using LinearAlgebra\nf = (du,u,p,t) -> du .= t .* u\njac = (J,u,p,t) -> (J[1,1] = t; J[2,2] = t; J)\njp = Diagonal(zeros(2))\nfun = ODEFunction(f; jac=jac, jac_prototype=jp)","category":"page"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"Note that the integrators will always make a deep copy of fun.jac_prototype, so there's no worry of aliasing.","category":"page"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"In general the jacobian prototype can be anything that has mul! defined, in particular sparse matrices or custom lazy types that support mul!. A special case is when the jac_prototype is a AbstractDiffEqLinearOperator, in which case you do not need to supply jac as it is automatically set to update_coefficients!. Refer to the DiffEqOperators section for more information on setting up time/parameter dependent operators.","category":"page"},{"location":"features/performance_overloads/#Examples","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"Examples","text":"","category":"section"},{"location":"features/performance_overloads/#ode_explicit_jac","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"Declaring Explicit Jacobians for ODEs","text":"","category":"section"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"The most standard case, declaring a function for a Jacobian is done by overloading the function f(du,u,p,t) with an in-place updating function for the Jacobian: f_jac(J,u,p,t) where the value type is used for dispatch. For example, take the LotkaVolterra model:","category":"page"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"function f(du,u,p,t)\n  du[1] = 2.0 * u[1] - 1.2 * u[1]*u[2]\n  du[2] = -3 * u[2] + u[1]*u[2]\nend","category":"page"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"To declare the Jacobian we simply add the dispatch:","category":"page"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"function f_jac(J,u,p,t)\n  J[1,1] = 2.0 - 1.2 * u[2]\n  J[1,2] = -1.2 * u[1]\n  J[2,1] = 1 * u[2]\n  J[2,2] = -3 + u[1]\n  nothing\nend","category":"page"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"Then we can supply the Jacobian with our ODE as:","category":"page"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"ff = ODEFunction(f;jac=f_jac)","category":"page"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"and use this in an ODEProblem:","category":"page"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"prob = ODEProblem(ff,ones(2),(0.0,10.0))","category":"page"},{"location":"features/performance_overloads/#Declaring-Explicit-Jacobians-for-DAEs","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"Declaring Explicit Jacobians for DAEs","text":"","category":"section"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"For fully implicit ODEs (DAEProblems), a slightly different Jacobian function is necessary. For the DAE","category":"page"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"G(duupt) = res","category":"page"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"The Jacobian should be given in the form gamma*dG/d(du) + dG/du where gamma is given by the solver. This means that the signature is:","category":"page"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"f(J,du,u,p,gamma,t)","category":"page"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"For example, for the equation","category":"page"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"function testjac(res,du,u,p,t)\n  res[1] = du[1] - 2.0 * u[1] + 1.2 * u[1]*u[2]\n  res[2] = du[2] -3 * u[2] - u[1]*u[2]\nend","category":"page"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"we would define the Jacobian as:","category":"page"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"function testjac(J,du,u,p,gamma,t)\n  J[1,1] = gamma - 2.0 + 1.2 * u[2]\n  J[1,2] = 1.2 * u[1]\n  J[2,1] = - 1 * u[2]\n  J[2,2] = gamma - 3 - u[1]\n  nothing\nend","category":"page"},{"location":"features/performance_overloads/#Symbolically-Calculating-the-Functions","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"Symbolically Calculating the Functions","text":"","category":"section"},{"location":"features/performance_overloads/","page":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","title":"DiffEqFunctions (Jacobians, Gradients, etc.) and Jacobian Types","text":"See the modelingtoolkitize function from ModelingToolkit.jl for automatically symbolically calculating the Jacobian for numerically-defined functions.","category":"page"},{"location":"types/steady_state_types/#Steady-State-Problems","page":"Steady State Problems","title":"Steady State Problems","text":"","category":"section"},{"location":"types/steady_state_types/#Mathematical-Specification-of-a-Steady-State-Problem","page":"Steady State Problems","title":"Mathematical Specification of a Steady State Problem","text":"","category":"section"},{"location":"types/steady_state_types/","page":"Steady State Problems","title":"Steady State Problems","text":"To define an Steady State Problem, you simply need to give the function f which defines the ODE:","category":"page"},{"location":"types/steady_state_types/","page":"Steady State Problems","title":"Steady State Problems","text":"fracdudt = f(upt)","category":"page"},{"location":"types/steady_state_types/","page":"Steady State Problems","title":"Steady State Problems","text":"and an initial guess u₀ of where f(u,p,t)=0. f should be specified as f(u,p,t) (or in-place as f(du,u,p,t)), and u₀ should be an AbstractArray (or number) whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher dimension tensors as well.","category":"page"},{"location":"types/steady_state_types/","page":"Steady State Problems","title":"Steady State Problems","text":"Note that for the steady-state to be defined, we must have that f is autonomous, that is f is independent of t. But the form which matches the standard ODE solver should still be used. The steady state solvers interpret the f by fixing t=0.","category":"page"},{"location":"types/steady_state_types/#Problem-Type","page":"Steady State Problems","title":"Problem Type","text":"","category":"section"},{"location":"types/steady_state_types/#Constructors","page":"Steady State Problems","title":"Constructors","text":"","category":"section"},{"location":"types/steady_state_types/","page":"Steady State Problems","title":"Steady State Problems","text":"SteadyStateProblem(f::ODEFunction,u0,p=NullParameters();kwargs...)\nSteadyStateProblem{isinplace}(f,u0,p=NullParameters();kwargs...)","category":"page"},{"location":"types/steady_state_types/","page":"Steady State Problems","title":"Steady State Problems","text":"isinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred. Additionally, the constructor from ODEProblems is provided:","category":"page"},{"location":"types/steady_state_types/","page":"Steady State Problems","title":"Steady State Problems","text":"SteadyStateProblem(prob::ODEProblem)","category":"page"},{"location":"types/steady_state_types/","page":"Steady State Problems","title":"Steady State Problems","text":"Parameters are optional, and if not given then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.","category":"page"},{"location":"types/steady_state_types/","page":"Steady State Problems","title":"Steady State Problems","text":"For specifying Jacobians and mass matrices, see the DiffEqFunctions page.","category":"page"},{"location":"types/steady_state_types/#Fields","page":"Steady State Problems","title":"Fields","text":"","category":"section"},{"location":"types/steady_state_types/","page":"Steady State Problems","title":"Steady State Problems","text":"f: The function in the ODE.\nu0: The initial guess for the steady state.\np: The parameters for the problem. Defaults to NullParameters\nkwargs: The keyword arguments passed onto the solves.","category":"page"},{"location":"types/steady_state_types/#Special-Solution-Fields","page":"Steady State Problems","title":"Special Solution Fields","text":"","category":"section"},{"location":"types/steady_state_types/","page":"Steady State Problems","title":"Steady State Problems","text":"The SteadyStateSolution type is different from the other DiffEq solutions because it does not have temporal information.","category":"page"},{"location":"types/bvp_types/#BVP-Problems","page":"BVP Problems","title":"BVP Problems","text":"","category":"section"},{"location":"types/bvp_types/#Mathematical-Specification-of-a-BVP-Problem","page":"BVP Problems","title":"Mathematical Specification of a BVP Problem","text":"","category":"section"},{"location":"types/bvp_types/","page":"BVP Problems","title":"BVP Problems","text":"To define a BVP Problem, you simply need to give the function f and the initial condition u₀ which define an ODE:","category":"page"},{"location":"types/bvp_types/","page":"BVP Problems","title":"BVP Problems","text":"fracdudt = f(upt)","category":"page"},{"location":"types/bvp_types/","page":"BVP Problems","title":"BVP Problems","text":"along with an implicit function bc! which defines the residual equation, where","category":"page"},{"location":"types/bvp_types/","page":"BVP Problems","title":"BVP Problems","text":"bc(upt) = 0","category":"page"},{"location":"types/bvp_types/","page":"BVP Problems","title":"BVP Problems","text":"is the manifold on which the solution must live. A common form for this is the two-point BVProblem where the manifold defines the solution at two points:","category":"page"},{"location":"types/bvp_types/","page":"BVP Problems","title":"BVP Problems","text":"u(t_0) = a\nu(t_f) = b","category":"page"},{"location":"types/bvp_types/#Problem-Type","page":"BVP Problems","title":"Problem Type","text":"","category":"section"},{"location":"types/bvp_types/#Constructors","page":"BVP Problems","title":"Constructors","text":"","category":"section"},{"location":"types/bvp_types/","page":"BVP Problems","title":"BVP Problems","text":"TwoPointBVProblem{isinplace}(f,bc!,u0,tspan,p=NullParameters();kwargs...)\nBVProblem{isinplace}(f,bc!,u0,tspan,p=NullParameters();kwargs...)","category":"page"},{"location":"types/bvp_types/","page":"BVP Problems","title":"BVP Problems","text":"For any BVP problem type, bc! is the inplace function:","category":"page"},{"location":"types/bvp_types/","page":"BVP Problems","title":"BVP Problems","text":"bc!(residual, u, p, t)","category":"page"},{"location":"types/bvp_types/","page":"BVP Problems","title":"BVP Problems","text":"where residual computed from the current u. u is an array of solution values where u[i] is at time t[i], while p are the parameters. For a TwoPointBVProblem, t = tspan. For the more general BVProblem, u can be all of the internal time points, and for shooting type methods u=sol the ODE solution. Note that all features of the ODESolution are present in this form. In both cases, the size of the residual matches the size of the initial condition.","category":"page"},{"location":"types/bvp_types/","page":"BVP Problems","title":"BVP Problems","text":"Parameters are optional, and if not given then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.","category":"page"},{"location":"types/bvp_types/#Fields","page":"BVP Problems","title":"Fields","text":"","category":"section"},{"location":"types/bvp_types/","page":"BVP Problems","title":"BVP Problems","text":"f: The function for the ODE.\nbc: The boundary condition function.\nu0: The initial condition. Either the initial condition for the ODE as an initial value problem, or a Vector of values for u(t_i) for collocation methods\ntspan: The timespan for the problem.\np: The parameters for the problem. Defaults to NullParameters\nkwargs: The keyword arguments passed onto the solves.","category":"page"},{"location":"tutorials/jump_diffusion/#Jump-Diffusion-Equations","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"","category":"section"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"note: Note\nThis tutorial assumes you have read the Ordinary Differential Equations tutorial.","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"Jump Diffusion equations are stochastic differential equations with discontinuous jumps. These can be written as:","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"fracdudt = f(upt) + Σgᵢ(ut)dWⁱ + Σ h_i(upt)N_i(t)","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"where N_i is a Poisson-counter which denotes jumps of size h_i. In this tutorial we will show how to solve problems with even more general jumps.","category":"page"},{"location":"tutorials/jump_diffusion/#Defining-a-ConstantRateJump-Problem","page":"Jump Diffusion Equations","title":"Defining a ConstantRateJump Problem","text":"","category":"section"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"To start, let's solve an ODE with constant rate jumps. A jump is defined as being \"constant rate\" if the rate is only dependent on values from other constant rate jumps, meaning that its rate must not be coupled with time or the solution to the differential equation. However, these types of jumps are cheaper to compute.","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"(Note: if your rate is only \"slightly\" dependent on the solution of the differential equation, then it may be okay to use a ConstantRateJump. Accuracy loss will be related to the percentage that the rate changes over the jump intervals.)","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"Let's solve the following problem. We will have a linear ODE with a Poisson counter of rate 2 (which is the mean and variance), where at each jump the current solution will be halved. To solve this problem, we first define the ODEProblem:","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"function f(du,u,p,t)\n  du[1] = u[1]\nend\n\nprob = ODEProblem(f,[0.2],(0.0,10.0))","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"Notice that, even though our equation is on 1 number, we define it using the in-place array form. Variable rate jump equations will require this form. Note that for this tutorial we solve a one-dimensional problem, but the same syntax applies for solving a system of differential equations with multiple jumps.","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"Now we define our rate equation for our jump. Since it's just the constant value 2, we do:","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"rate(u,p,t) = 2","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"Now we define the affect! of the jump. This is the same as an affect! from a DiscreteCallback, and thus acts directly on the integrator. Therefore, to make it halve the current value of u, we do:","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"affect!(integrator) = (integrator.u[1] = integrator.u[1]/2)","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"Then we build our jump:","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"jump = ConstantRateJump(rate,affect!)","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"Next, we extend our ODEProblem to a JumpProblem by attaching the jump:","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"jump_prob = JumpProblem(prob,Direct(),jump)","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"We can now solve this extended problem using any ODE solver:","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"sol = solve(jump_prob,Tsit5())\nplot(sol)","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"(Image: constant_rate_jump)","category":"page"},{"location":"tutorials/jump_diffusion/#Variable-Rate-Jumps","page":"Jump Diffusion Equations","title":"Variable Rate Jumps","text":"","category":"section"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"Now let's define a jump which is coupled to the differential equation. Let's let the rate be the current value of the solution, that is:","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"rate(u,p,t) = u[1]","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"Using the same affect!","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"affect!(integrator) = (integrator.u[1] = integrator.u[1]/2)","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"we build a VariableRateJump:","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"jump = VariableRateJump(rate,affect!)","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"To make things interesting, let's copy this jump:","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"jump2 = deepcopy(jump)","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"so that way we have two independent jump processes. We now couple these jumps to the ODEProblem:","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"jump_prob = JumpProblem(prob,Direct(),jump,jump2)","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"which we once again solve using an ODE solver:","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"sol = solve(jump_prob,Tsit5())\nplot(sol)","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"(Image: variable_rate_jump)","category":"page"},{"location":"tutorials/jump_diffusion/#Jump-Diffusion","page":"Jump Diffusion Equations","title":"Jump Diffusion","text":"","category":"section"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"Now we will finally solve the jump diffusion problem. The steps are the same as before, except we now start with a SDEProblem instead of an ODEProblem. Using the same drift function f as before, we add multiplicative noise via:","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"function g(du,u,p,t)\n  du[1] = u[1]\nend\n\nprob = SDEProblem(f,g,[0.2],(0.0,10.0))","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"and couple it to the jumps:","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"jump_prob = JumpProblem(prob,Direct(),jump,jump2)","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"We then solve it using an SDE algorithm:","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"sol = solve(jump_prob,SRIW1())\nplot(sol)","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"(Image: jump_diffusion)","category":"page"},{"location":"tutorials/jump_diffusion/#Coupling-Jump-Problems","page":"Jump Diffusion Equations","title":"Coupling Jump Problems","text":"","category":"section"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"In many applications one is interested in coupling two stochastic processes. This has applications in Monte Carlo simulations and sensitivity analysis, for example. Currently, the coupling that is implemented for jump processes is known as the split coupling. The split coupling couples two jump processes by coupling the underlying Poisson processes driving the jump components.","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"Suppose prob and prob_control are two problems we wish to couple. Then the coupled problem is obtained by","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"prob_coupled =  SplitCoupledJumpProblem(jump_prob,jump_prob_control,Direct(),coupling_map)","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"Here, coupling_map specifies which jumps to couple. If (j,i) is in coupling_map, then the ith jump in prob will be coupled to the jth jump in prob_control. Note that currently SplitCoupledJumpProblem is only implemented for constant rate jump problems.","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"As an example, consider a doubly stochastic Poisson process, that is, a Poisson process whose rate is itself a stochastic process. In particular, we will take the rate to randomly switch between zero and 10 at unit rates:","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"rate(u,p,t) = u[2]*10\naffect!(integrator) = integrator.u[1] += 1.\njump1 = ConstantRateJump(rate,affect!)\nrate(u,p,t) = u[2]\naffect!(integrator) = (integrator.u[2] -= 1.;integrator.u[3] += 1.)\njump2 = ConstantRateJump(rate,affect!)\n\nrate(u,p,t) = u[3]\naffect!(integrator) = (integrator.u[2] += 1.;integrator.u[3] -= 1.)\njump3 = ConstantRateJump(rate,affect!)\nprob = DiscreteProblem(u0,tspan)\njump_prob = JumpProblem(prob,Direct(),jump1,jump2,jump3)","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"The doubly stochastic poisson process has two sources of randomness: one due to the Poisson process, and another due to random evolution of the rate. This is typical of many multiscale stochastic processes appearing in applications, and it is often useful to compare such a process to one obtained by removing one source of randomness. In present context, this means looking at an ODE with constant jump rates, where the deterministic evolution between jumps is given by the expected value of the Poisson process:","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"function f(du,u,p,t)\n  du[1] = u[2]*10\n  du[2] = 0.\n  du[3] = 0.\nend\nprob_control = ODEProblem(f,u0,tspan)\njump_prob_control = JumpProblem(prob_control,Direct(),jump2,jump3)","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"Let's couple the two problems by coupling the jumps corresponding the switching of the rate:","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"coupling_map = [(2,1),(3,2)]\nprob_coupled =  SplitCoupledJumpProblem(jump_prob,jump_prob_control,Direct(),coupling_map)","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"Now prob_coupled will be dealt with like any other JumpProblem:","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"sol = solve(prob_coupled,Tsit5())","category":"page"},{"location":"tutorials/jump_diffusion/","page":"Jump Diffusion Equations","title":"Jump Diffusion Equations","text":"(Image: jump_diffusion)","category":"page"},{"location":"extras/timestepping/#timestepping","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"","category":"section"},{"location":"extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"CurrentModule = OrdinaryDiffEq","category":"page"},{"location":"extras/timestepping/#Common-Setup","page":"Timestepping Method Descriptions","title":"Common Setup","text":"","category":"section"},{"location":"extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"All methods start by calculating a scaled error estimate on each scalar component of u:","category":"page"},{"location":"extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"err^scaled_i = norm(err_i(abstol_i + max(uprev_iu_i)reltol_i))","category":"page"},{"location":"extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"On this scaled error estimate, we calculate the norm. This norm is usually the Hairer norm:","category":"page"},{"location":"extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"norm(x) = sqrt(sum(x^2)length(x))","category":"page"},{"location":"extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"This norm works well because it does not change if we add new pieces to the differential equation: it scales our error by the number of equations so that independent equations will not step differently than a single solve.","category":"page"},{"location":"extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"In all cases, the step is rejected if err^scaled1 since that means the error is larger than the tolerances, and the step is accepted if err^scaled1.","category":"page"},{"location":"extras/timestepping/#Integral-Controller-(Standard-Controller)","page":"Timestepping Method Descriptions","title":"Integral Controller (Standard Controller)","text":"","category":"section"},{"location":"extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"The proportional control algorithm is the \"standard algorithm\" for adaptive timestepping. Note that it is not the default in DifferentialEquations.jl because it is usually awful for performance, but it is explained first because it is the most widely taught algorithm and others build off of its techniques.","category":"page"},{"location":"extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"The control simply changes dt proportional to the error. There is an exponentiation based on the order of the algorithm which goes back to a result by Cechino for the optimal stepsize to reduce the error. The algorithm is:","category":"page"},{"location":"extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"qtmp = integrator.EEst^(1/(alg_adaptive_order(integrator.alg)+1))/integrator.opts.gamma\n@fastmath q = max(inv(integrator.opts.qmax),min(inv(integrator.opts.qmin),qtmp))\nintegrator.dtnew = integrator.dt/q","category":"page"},{"location":"extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"Thus q is the scaling factor for dt, and it must be between qmin and qmax. gamma is the safety factor, 0.9, for how much dt is decreased below the theoretical \"optimal\" value.","category":"page"},{"location":"extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"Since proportional control is \"jagged\", i.e. can cause large changes between one step to the next, it can effect the stability of explicit methods. Thus it's only applied by default to low order implicit solvers.","category":"page"},{"location":"extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"IController","category":"page"},{"location":"extras/timestepping/#OrdinaryDiffEq.IController","page":"Timestepping Method Descriptions","title":"OrdinaryDiffEq.IController","text":"IController()\n\nThe standard (integral) controller is the most basic step size controller. This controller is usually the first one introduced in numerical analysis classes but should only be used rarely in practice because of efficiency problems for many problems/algorithms.\n\nConstruct an integral (I) step size controller adapting the time step based on the formula\n\nΔtₙ₊₁ = εₙ₊₁^(1/k) * Δtₙ\n\nwhere k = get_current_adaptive_order(alg, integrator.cache) + 1 and εᵢ is the inverse of the error estimate integrator.EEst scaled by the tolerance (Hairer, Nørsett, Wanner, 2008, Section II.4). The step size factor is multiplied by the safety factor gamma and clipped to the interval [qmin, qmax]. A step will be accepted whenever the estimated error integrator.EEst is less than or equal to unity. Otherwise, the step is rejected and re-tried with the predicted step size.\n\nReferences\n\nHairer, Nørsett, Wanner (2008) Solving Ordinary Differential Equations I Nonstiff Problems DOI: 10.1007/978-3-540-78862-1\n\n\n\n\n\n","category":"type"},{"location":"extras/timestepping/#Proportional-Integral-Controller-(PI-Controller)","page":"Timestepping Method Descriptions","title":"Proportional-Integral Controller (PI Controller)","text":"","category":"section"},{"location":"extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"The proportional-integral control algorithm is a standard control algorithm from control theory. It mixes proportional control with memory in order to make the timesteps more stable, which actually increases the adaptive stability region of the algorithm. This stability property means that it's well-suited for explicit solvers, and it's applied by default to the Rosenbrock methods as well. The form for the updates is:","category":"page"},{"location":"extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"EEst,beta1,q11,qold,beta2 = integrator.EEst, integrator.opts.beta1, integrator.q11,integrator.qold,integrator.opts.beta2\n@fastmath q11 = EEst^beta1\n@fastmath q = q11/(qold^beta2)\nintegrator.q11 = q11\n@fastmath q = max(inv(integrator.opts.qmax),min(inv(integrator.opts.qmin),q/integrator.opts.gamma))\nif q <= integrator.opts.qsteady_max && q >= integrator.opts.qsteady_min\n  q = one(q)\nend\nq","category":"page"},{"location":"extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"beta1 is the gain on the proportional part, and beta2 is the gain for the history portion. qoldinit is the initialized value for the gain history.","category":"page"},{"location":"extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"PIController","category":"page"},{"location":"extras/timestepping/#OrdinaryDiffEq.PIController","page":"Timestepping Method Descriptions","title":"OrdinaryDiffEq.PIController","text":"PIController(beta1, beta2)\n\nThe proportional-integral (PI) controller is a widespread step size controller with improved stability properties compared to the IController. This controller is the default for most algorithms in OrdinaryDiffEq.jl.\n\nConstruct a PI step size controller adapting the time step based on the formula\n\nΔtₙ₊₁ = εₙ₊₁^β₁ * εₙ^β₂ * Δtₙ\n\nwhere εᵢ are inverses of the error estimates scaled by the tolerance (Hairer, Nørsett, Wanner, 2010, Section IV.2). The step size factor is multiplied by the safety factor gamma and clipped to the interval [qmin, qmax]. A step will be accepted whenever the estimated error integrator.EEst is less than or equal to unity. Otherwise, the step is rejected and re-tried with the predicted step size.\n\nnote: Note\nThe coefficients beta1, beta2 are not scaled by the order of the method, in contrast to the PIDController. For the PIController, this scaling by the order must be done when the controller is constructed.\n\nReferences\n\nHairer, Nørsett, Wanner (2010) Solving Ordinary Differential Equations II Stiff and Differential-Algebraic Problems DOI: 10.1007/978-3-642-05221-7\nHairer, Nørsett, Wanner (2008) Solving Ordinary Differential Equations I Nonstiff Problems DOI: 10.1007/978-3-540-78862-1\n\n\n\n\n\n","category":"type"},{"location":"extras/timestepping/#Proportional-Integral-Derivative-Controller-(PID-Controller)","page":"Timestepping Method Descriptions","title":"Proportional-Integral-Derivative Controller (PID Controller)","text":"","category":"section"},{"location":"extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"PIDController","category":"page"},{"location":"extras/timestepping/#OrdinaryDiffEq.PIDController","page":"Timestepping Method Descriptions","title":"OrdinaryDiffEq.PIDController","text":"PIDController(beta1, beta2, beta3=zero(beta1);\n              limiter=default_dt_factor_limiter,\n              accept_safety=0.81)\n\nThe proportional-integral-derivative (PID) controller is a generalization of the PIController and can have improved stability and efficiency properties.\n\nConstruct a PID step size controller adapting the time step based on the formula\n\nΔtₙ₊₁ = εₙ₊₁^(β₁/k) * εₙ^(β₂/k) * εₙ₋₁^(β₃/ k) * Δtₙ\n\nwhere k = min(alg_order, alg_adaptive_order) + 1 and εᵢ are inverses of the error estimates scaled by the tolerance (Söderlind, 2003). The step size factor is limited by the limiter with default value\n\nlimiter(x) = one(x) + atan(x - one(x))\n\nas proposed by Söderlind and Wang (2006). A step will be accepted whenever the predicted step size change is bigger than accept_safety. Otherwise, the step is rejected and re-tried with the predicted step size.\n\nSome standard controller parameters suggested in the literature are\n\nController beta1 beta2 beta3\nbasic 1.00 0.00 0\nPI42 0.60 -0.20 0\nPI33 2//3 -1//3 0\nPI34 0.70 -0.40 0\nH211PI 1//6 1//6 0\nH312PID 1//18 1//9 1//18\n\nnote: Note\nIn contrast to the PIController, the coefficients beta1, beta2, beta3 are scaled by the order of the method. Thus, standard controllers such as PI42 can use the same coefficients beta1, beta2, beta3 for different algorithms.\n\nnote: Note\nIn contrast to other controllers, the PIDController does not use the keyword arguments qmin, qmax to limit the step size change or the safety factor gamma. These common keyword arguments are replaced by the limiter and accept_safety to guarantee a smooth behavior (Söderlind and Wang, 2006). Because of this, a PIDController behaves different from a PIController, even if beta1, beta2 are adapted accordingly and iszero(beta3).\n\nReferences\n\nSöderlind (2003) Digital Filters in Adaptive Time-Stepping DOI: 10.1145/641876.641877\nSöderlind, Wang (2006) Adaptive time-stepping and computational stability DOI: 10.1016/j.cam.2005.03.008\nRanocha, Dalcin, Parsani, Ketcheson (2021) Optimized Runge-Kutta Methods with Automatic Step Size Control for Compressible Computational Fluid Dynamics arXiv:2104.06836\n\n\n\n\n\n","category":"type"},{"location":"extras/timestepping/#Gustafsson-Acceleration","page":"Timestepping Method Descriptions","title":"Gustafsson Acceleration","text":"","category":"section"},{"location":"extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"The Gustafsson acceleration algorithm accelerates changes so that way algorithms can more swiftly change to handle quick transients. This algorithm is thus well-suited for stiff solvers where this can be expected, and is the default for algorithms like the (E)SDIRK methods.","category":"page"},{"location":"extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"gamma = integrator.opts.gamma\nniters = integrator.cache.newton_iters\nfac = min(gamma,(1+2*integrator.alg.max_newton_iter)*gamma/(niters+2*integrator.alg.max_newton_iter))\nexpo = 1/(alg_order(integrator.alg)+1)\nqtmp = (integrator.EEst^expo)/fac\n@fastmath q = max(inv(integrator.opts.qmax),min(inv(integrator.opts.qmin),qtmp))\nif q <= integrator.opts.qsteady_max && q >= integrator.opts.qsteady_min\n  q = one(q)\nend\nintegrator.qold = q\nq","category":"page"},{"location":"extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"In this case, niters is the number of Newton iterations which was required in the most recent step of the algorithm. Note that these values are used differently depending on acceptance and rejection. When the step is accepted, the following logic is applied:","category":"page"},{"location":"extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"if integrator.success_iter > 0\n  expo = 1/(alg_adaptive_order(integrator.alg)+1)\n  qgus=(integrator.dtacc/integrator.dt)*(((integrator.EEst^2)/integrator.erracc)^expo)\n  qgus = max(inv(integrator.opts.qmax),min(inv(integrator.opts.qmin),qgus/integrator.opts.gamma))\n  qacc=max(q,qgus)\nelse\n  qacc = q\nend\nintegrator.dtacc = integrator.dt\nintegrator.erracc = max(1e-2,integrator.EEst)\nintegrator.dt/qacc","category":"page"},{"location":"extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"When it rejects, its the same as the proportional control:","category":"page"},{"location":"extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"if integrator.success_iter == 0\n  integrator.dt *= 0.1\nelse\n  integrator.dt = integrator.dt/integrator.qold\nend","category":"page"},{"location":"extras/timestepping/","page":"Timestepping Method Descriptions","title":"Timestepping Method Descriptions","text":"PredictiveController","category":"page"},{"location":"extras/timestepping/#OrdinaryDiffEq.PredictiveController","page":"Timestepping Method Descriptions","title":"OrdinaryDiffEq.PredictiveController","text":"PredictiveController()\n\nThe Gustafsson acceleration algorithm accelerates changes so that way algorithms can more swiftly change to handle quick transients. This algorithm is thus well-suited for stiff solvers where this can be expected, and is the default for algorithms like the (E)SDIRK methods.\n\ngamma = integrator.opts.gamma\nniters = integrator.cache.newton_iters\nfac = min(gamma,(1+2*integrator.alg.max_newton_iter)*gamma/(niters+2*integrator.alg.max_newton_iter))\nexpo = 1/(alg_order(integrator.alg)+1)\nqtmp = (integrator.EEst^expo)/fac\n@fastmath q = max(inv(integrator.opts.qmax),min(inv(integrator.opts.qmin),qtmp))\nif q <= integrator.opts.qsteady_max && q >= integrator.opts.qsteady_min\n  q = one(q)\nend\nintegrator.qold = q\nq\n\nIn this case, niters is the number of Newton iterations which was required in the most recent step of the algorithm. Note that these values are used differently depending on acceptance and rejectance. When the step is accepted, the following logic is applied:\n\nif integrator.success_iter > 0\n  expo = 1/(alg_adaptive_order(integrator.alg)+1)\n  qgus=(integrator.dtacc/integrator.dt)*(((integrator.EEst^2)/integrator.erracc)^expo)\n  qgus = max(inv(integrator.opts.qmax),min(inv(integrator.opts.qmin),qgus/integrator.opts.gamma))\n  qacc=max(q,qgus)\nelse\n  qacc = q\nend\nintegrator.dtacc = integrator.dt\nintegrator.erracc = max(1e-2,integrator.EEst)\nintegrator.dt/qacc\n\nWhen it rejects, it's the same as the IController:\n\nif integrator.success_iter == 0\n  integrator.dt *= 0.1\nelse\n  integrator.dt = integrator.dt/integrator.qold\nend\n\n\n\n\n\n","category":"type"},{"location":"features/ensemble/#ensemble","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"","category":"section"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Performing Monte Carlo simulations, solving with a predetermined set of initial conditions, and GPU-parallelizing a parameter search all fall under the ensemble simulation interface. This interface allows one to declare a template DEProblem to parallelize, tweak the template in trajectories for many trajectories, solve each in parallel batches, reduce the solutions down to specific answers, and compute summary statistics on the results.","category":"page"},{"location":"features/ensemble/#Performing-an-Ensemble-Simulation","page":"Parallel Ensemble Simulations","title":"Performing an Ensemble Simulation","text":"","category":"section"},{"location":"features/ensemble/#Building-a-Problem","page":"Parallel Ensemble Simulations","title":"Building a Problem","text":"","category":"section"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"To perform a simulation on an ensemble of trajectories, define a EnsembleProblem. The constructor is:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"EnsembleProblem(prob::DEProblem;\n                output_func = (sol,i) -> (sol,false),\n                prob_func= (prob,i,repeat)->(prob),\n                reduction = (u,data,I)->(append!(u,data),false),\n                u_init = [], safetycopy = prob_func !== DEFAULT_PROB_FUNC)","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"prob_func: The function by which the problem is to be modified. prob is the problem, i is the unique id 1:trajectories for the problem, and repeat is for if the iteration of the repeat. At first it's 0, but if rerun was true this will be 1, 2, etc. counting the number of times problem i has been repeated.\noutput_func: The function determines what is saved from the solution to the output array. Defaults to saving the solution itself. The output is (out,rerun) where out is the output and rerun is a boolean which designates whether to rerun\nreduction: This function determines how to reduce the data in each batch. Defaults to appending the data from the batches. The second part of the output determines whether the simulation has converged. If true, the simulation will exit early. By default, this is always false.\nsafetycopy: Determines whether a safety deepcopy is called on the prob before the prob_func. By default this is true for any user-given prob_func, as without this, modifying the arguments of something in the prob_func, such as parameters or caches stored within the user function, are not necessarily thread-safe. If you know that your function is thread-safe, then setting this to false can improve performance when used with threads.","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"One can specify a function prob_func which changes the problem. For example:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"function prob_func(prob,i,repeat)\n  @. prob.u0 = randn()*prob.u0\n  prob\nend","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"modifies the initial condition for all of the problems by a standard normal random number (a different random number per simulation). Notice that since problem types are immutable, it uses .=. Otherwise, one can just create a new problem type:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"function prob_func(prob,i,repeat)\n  @. prob.u0 = u0_arr[i]\n  prob\nend","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"If your function is a ParameterizedFunction, you can do similar modifications to prob.f to perform a parameter search. The output_func is a reduction function. It's arguments are the generated solution and the unique index for the run. For example, if we wish to only save the 2nd coordinate at the end of each solution, we can do:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"output_func(sol,i) = (sol[end,2],false)","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Thus the ensemble simulation would return as its data an array which is the end value of the 2nd dependent variable for each of the runs.","category":"page"},{"location":"features/ensemble/#Solving-the-Problem","page":"Parallel Ensemble Simulations","title":"Solving the Problem","text":"","category":"section"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"sim = solve(prob,alg,ensemblealg,kwargs...)","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"The keyword arguments take in the arguments for the common solver interface and will pass them to the differential equation solver. The ensemblealg is optional, and will default to EnsembleThreads(). The special keyword arguments to note are:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"trajectories: The number of simulations to run. This argument is required.\nbatch_size : The size of the batches on which the reductions are applies. Defaults to trajectories.\npmap_batch_size: The size of the pmap batches. Default is  batch_size÷100 > 0 ? batch_size÷100 : 1","category":"page"},{"location":"features/ensemble/#EnsembleAlgorithms","page":"Parallel Ensemble Simulations","title":"EnsembleAlgorithms","text":"","category":"section"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"The choice of ensemble algorithm allows for control over how the multiple trajectories are handled. Currently, the ensemble algorithm types are:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"EnsembleSerial() - No parallelism\nEnsembleThreads() - The default. This uses multithreading. It's local (single computer, shared memory) parallelism only. Fastest when the trajectories are quick.\nEnsembleDistributed() - Uses pmap internally. It will use as many processors as you have Julia processes. To add more processes, use addprocs(n). See Julia's documentation for more details. Recommended for the case when each trajectory calculation isn't \"too quick\" (at least about a millisecond each?).\nEnsembleSplitThreads() - This uses threading on each process, splitting the problem into nprocs() even parts. This is for solving many quick trajectories on a multi-node machine. It's recommended you have one process on each node.\nEnsembleGPUArray() - Requires installing and using DiffEqGPU. This uses a GPU for computing the ensemble with hyperparallelism. It will automatically recompile your Julia functions to the GPU. A standard GPU sees a 5x performance increase over a 16 core Xeon CPU. However, there are limitations on what functions can auto-compile in this fashion, please see the DiffEqGPU README for more details","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"For example, EnsembleThreads() is invoked by:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"solve(ensembleprob,alg,EnsembleThreads();trajectories=1000)","category":"page"},{"location":"features/ensemble/#Solution-Type","page":"Parallel Ensemble Simulations","title":"Solution Type","text":"","category":"section"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"The resulting type is a EnsembleSimulation, which includes the array of solutions.","category":"page"},{"location":"features/ensemble/#Plot-Recipe","page":"Parallel Ensemble Simulations","title":"Plot Recipe","text":"","category":"section"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"There is a plot recipe for a AbstractEnsembleSimulation which composes all of the plot recipes for the component solutions. The keyword arguments are passed along. A useful argument to use is linealpha which will change the transparency of the plots. An additional argument is idxs which allows you to choose which components of the solution to plot. For example, if the differential equation is a vector of 9 values, idxs=1:2:9 will plot only the solutions of the odd components. An other additional argument is zcolors (an alias of marker_z) which allows you to pass a zcolor for each series. For details about zcolor see the Series documentation for Plots.jl.","category":"page"},{"location":"features/ensemble/#Analyzing-an-Ensemble-Experiment","page":"Parallel Ensemble Simulations","title":"Analyzing an Ensemble Experiment","text":"","category":"section"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Analysis tools are included for generating summary statistics and summary plots for a EnsembleSimulation.","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"To use this functionality, import the analysis module via:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"using DifferentialEquations.EnsembleAnalysis","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"(or more directly SciMLBase.EnsembleAnalysis).","category":"page"},{"location":"features/ensemble/#Time-steps-vs-time-points","page":"Parallel Ensemble Simulations","title":"Time steps vs time points","text":"","category":"section"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"For the summary statistics, there are two types. You can either summarize by time steps or by time points. Summarizing by time steps assumes that the time steps are all the same time point, i.e. the integrator used a fixed dt or the values were saved using saveat. Summarizing by time points requires interpolating the solution.","category":"page"},{"location":"features/ensemble/#Analysis-at-a-time-step-or-time-point","page":"Parallel Ensemble Simulations","title":"Analysis at a time step or time point","text":"","category":"section"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"get_timestep(sim,i) # Returns an iterator of each simulation at time step i\nget_timepoint(sim,t) # Returns an iterator of each simulation at time point t\ncomponentwise_vectors_timestep(sim,i) # Returns a vector of each simulation at time step i\ncomponentwise_vectors_timepoint(sim,t) # Returns a vector of each simulation at time point t","category":"page"},{"location":"features/ensemble/#Summary-Statistics","page":"Parallel Ensemble Simulations","title":"Summary Statistics","text":"","category":"section"},{"location":"features/ensemble/#Single-Time-Statistics","page":"Parallel Ensemble Simulations","title":"Single Time Statistics","text":"","category":"section"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"The available functions for time steps are:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"timestep_mean(sim,i) # Computes the mean of each component at time step i\ntimestep_median(sim,i) # Computes the median of each component at time step i\ntimestep_quantile(sim,q,i) # Computes the quantile q of each component at time step i\ntimestep_meanvar(sim,i)  # Computes the mean and variance of each component at time step i\ntimestep_meancov(sim,i,j) # Computes the mean at i and j, and the covariance, for each component\ntimestep_meancor(sim,i,j) # Computes the mean at i and j, and the correlation, for each component\ntimestep_weighted_meancov(sim,W,i,j) # Computes the mean at i and j, and the weighted covariance W, for each component","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"The available functions for time points are:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"timepoint_mean(sim,t) # Computes the mean of each component at time t\ntimepoint_median(sim,t) # Computes the median of each component at time t\ntimepoint_quantile(sim,q,t) # Computes the quantile q of each component at time t\ntimepoint_meanvar(sim,t) # Computes the mean and variance of each component at time t\ntimepoint_meancov(sim,t1,t2) # Computes the mean at t1 and t2, the covariance, for each component\ntimepoint_meancor(sim,t1,t2) # Computes the mean at t1 and t2, the correlation, for each component\ntimepoint_weighted_meancov(sim,W,t1,t2) # Computes the mean at t1 and t2, the weighted covariance W, for each component","category":"page"},{"location":"features/ensemble/#Full-Timeseries-Statistics","page":"Parallel Ensemble Simulations","title":"Full Timeseries Statistics","text":"","category":"section"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Additionally, the following functions are provided for analyzing the full timeseries. The mean and meanvar versions return a DiffEqArray which can be directly plotted. The meancov and meancor return a matrix of tuples, where the tuples are the (mean_t1,mean_t2,cov or cor).","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"The available functions for the time steps are:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"timeseries_steps_mean(sim) # Computes the mean at each time step\ntimeseries_steps_median(sim) # Computes the median at each time step\ntimeseries_steps_quantile(sim,q) # Computes the quantile q at each time step\ntimeseries_steps_meanvar(sim) # Computes the mean and variance at each time step\ntimeseries_steps_meancov(sim) # Computes the covariance matrix and means at each time step\ntimeseries_steps_meancor(sim) # Computes the correlation matrix and means at each time step\ntimeseries_steps_weighted_meancov(sim) # Computes the weighted covariance matrix and means at each time step","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"The available functions for the time points are:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"timeseries_point_mean(sim,ts) # Computes the mean at each time point in ts\ntimeseries_point_median(sim,ts) # Computes the median at each time point in ts\ntimeseries_point_quantile(sim,q,ts) # Computes the quantile q at each time point in ts\ntimeseries_point_meanvar(sim,ts) # Computes the mean and variance at each time point in ts\ntimeseries_point_meancov(sim,ts) # Computes the covariance matrix and means at each time point in ts\ntimeseries_point_meancor(sim,ts) # Computes the correlation matrix and means at each time point in ts\ntimeseries_point_weighted_meancov(sim,ts) # Computes the weighted covariance matrix and means at each time point in ts","category":"page"},{"location":"features/ensemble/#EnsembleSummary","page":"Parallel Ensemble Simulations","title":"EnsembleSummary","text":"","category":"section"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"The EnsembleSummary type is included to help with analyzing the general summary statistics. Two constructors are provided:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"EnsembleSummary(sim;quantiles=[0.05,0.95])\nEnsembleSummary(sim,ts;quantiles=[0.05,0.95])","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"The first produces a (mean,var) summary at each time step. As with the summary statistics, this assumes that the time steps are all the same. The second produces a (mean,var) summary at each time point t in ts. This requires the ability to interpolate the solution. Quantile is used to determine the qlow and qhigh quantiles at each timepoint. It defaults to the 5% and 95% quantiles.","category":"page"},{"location":"features/ensemble/#Plot-Recipe-2","page":"Parallel Ensemble Simulations","title":"Plot Recipe","text":"","category":"section"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"The EnsembleSummary comes with a plot recipe for visualizing the summary statistics. The extra keyword arguments are:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"idxs: the solution components to plot. Defaults to plotting all components.\nerror_style: The style for plotting the error. Defaults to ribbon. Other choices are :bars for error bars and :none for no error bars.\nci_type : Defaults to :quantile which has (qlow,qhigh) quantiles whose limits were determined when constructing the EnsembleSummary. Gaussian CI 1.96*(standard error of the mean) can be set using ci_type=:SEM.","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"One useful argument is fillalpha which controls the transparency of the ribbon around the mean.","category":"page"},{"location":"features/ensemble/#Example-1:-Solving-an-ODE-With-Different-Initial-Conditions","page":"Parallel Ensemble Simulations","title":"Example 1: Solving an ODE With Different Initial Conditions","text":"","category":"section"},{"location":"features/ensemble/#Random-Initial-Conditions","page":"Parallel Ensemble Simulations","title":"Random Initial Conditions","text":"","category":"section"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Let's test the sensitivity of the linear ODE to its initial condition. To do this, we would like to solve the linear ODE 100 times and plot what the trajectories look like. Let's start by opening up some extra processes so that way the computation will be parallelized. Here we will choose to use distributed parallelism which means that the required functions must be made available to all processes. This can be achieved with @everywhere macro:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"using Distributed\nusing DifferentialEquations\nusing Plots\n\naddprocs()\n@everywhere using DifferentialEquations","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Now let's define the linear ODE which is our base problem:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"# Linear ODE which starts at 0.5 and solves from t=0.0 to t=1.0\nprob = ODEProblem((u,p,t)->1.01u,0.5,(0.0,1.0))","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"For our ensemble simulation, we would like to change the initial condition around. This is done through the prob_func. This function takes in the base problem and modifies it to create the new problem that the trajectory actually solves. Here we will take the base problem, multiply the initial condition by a rand(), and use that for calculating the trajectory:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"@everywhere function prob_func(prob,i,repeat)\n  remake(prob,u0=rand()*prob.u0)\nend","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Now we build and solve the EnsembleProblem with this base problem and prob_func:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"ensemble_prob = EnsembleProblem(prob,prob_func=prob_func)\nsim = solve(ensemble_prob,Tsit5(),EnsembleDistributed(),trajectories=100)","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"We can use the plot recipe to plot what the 100 ODEs look like:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"plotly()\nplot(sim,linealpha=0.4)","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"(Image: monte_carlo_plot)","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"We note that if we wanted to find out what the initial condition was for a given trajectory, we can retrieve it from the solution. sim[i] returns the ith solution object. sim[i].prob is the problem that specific trajectory solved, and sim[i].prob.u0 would then be the initial condition used in the ith trajectory.","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Note: If the problem has callbacks, the functions for the condition and affect! must be named functions (not anonymous functions).","category":"page"},{"location":"features/ensemble/#Using-multithreading","page":"Parallel Ensemble Simulations","title":"Using multithreading","text":"","category":"section"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"The previous ensemble simulation can also be parallelized using a multithreading approach, which will make use of the different cores within a single computer. Because the memory is shared across the different threads, it is not necessary to use the @everywhere macro. Instead, the same problem can be implemented simply as:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"using DifferentialEquations\nprob = ODEProblem((u,p,t)->1.01u,0.5,(0.0,1.0))\nfunction prob_func(prob,i,repeat)\n  remake(prob,u0=rand()*prob.u0)\nend\nensemble_prob = EnsembleProblem(prob,prob_func=prob_func)\nsim = solve(ensemble_prob,Tsit5(),EnsembleThreads(),trajectories=100)","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"The number of threads to be used has to be defined outside of Julia, in the environmental variable JULIA_NUM_THREADS (see Julia's documentation for details).","category":"page"},{"location":"features/ensemble/#Pre-Determined-Initial-Conditions","page":"Parallel Ensemble Simulations","title":"Pre-Determined Initial Conditions","text":"","category":"section"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"In many cases, you may already know what initial conditions you want to use. This can be specified by the i argument of the prob_func. This i is the unique index of each trajectory. So, if we have trajectories=100, then we have i as some index in 1:100, and it's different for each trajectory.","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"So, if we wanted to use a grid of evenly spaced initial conditions from 0 to 1, we could simply index the linspace type:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"initial_conditions = range(0, stop=1, length=100)\nfunction prob_func(prob,i,repeat)\n  remake(prob,u0=initial_conditions[i])\nend","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"It's worth noting that if you run this code successfully, there will be no visible output.","category":"page"},{"location":"features/ensemble/#Example-2:-Solving-an-SDE-with-Different-Parameters","page":"Parallel Ensemble Simulations","title":"Example 2: Solving an SDE with Different Parameters","text":"","category":"section"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Let's solve the same SDE but with varying parameters. Let's create a Lotka-Volterra system with multiplicative noise. Our Lotka-Volterra system will have as its drift component:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"function f(du,u,p,t)\n  du[1] = p[1] * u[1] - p[2] * u[1]*u[2]\n  du[2] = -3 * u[2] + u[1]*u[2]\nend","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"For our noise function we will use multiplicative noise:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"function g(du,u,p,t)\n  du[1] = p[3]*u[1]\n  du[2] = p[4]*u[2]\nend","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Now we build the SDE with these functions:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"p = [1.5,1.0,0.1,0.1]\nprob = SDEProblem(f,g,[1.0,1.0],(0.0,10.0),p)","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"This is the base problem for our study. What would like to do with this experiment is keep the same parameters in the deterministic component each time, but very the parameters for the amount of noise using 0.3rand(2) as our parameters. Once again, we do this with a prob_func, and here we modify the parameters in prob.p:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"function prob_func(prob,i,repeat)\n  x = 0.3rand(2)\n  remake(prob,p=[p[1:2];x])\nend","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Now we solve the problem 10 times and plot all of the trajectories in phase space:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"ensemble_prob = EnsembleProblem(prob,prob_func=prob_func)\nsim = solve(ensemble_prob,SRIW1(),trajectories=10)\nusing Plots; plotly()\nusing Plots; plot(sim,linealpha=0.6,color=:blue,vars=(0,1),title=\"Phase Space Plot\")\nplot!(sim,linealpha=0.6,color=:red,vars=(0,2),title=\"Phase Space Plot\")","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"(Image: monte_lotka_blue)","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"We can then summarize this information with the mean/variance bounds using a EnsembleSummary plot. We will take the mean/quantile at every 0.1 time units and directly plot the summary:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"summ = EnsembleSummary(sim,0:0.1:10)\npyplot() # Note that plotly does not support ribbon plots\nplot(summ,fillalpha=0.5)","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"(Image: monte_carlo_quantile)","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Note that here we used the quantile bounds, which default to [0.05,0.95] in the EnsembleSummary constructor. We can change to standard error of the mean bounds using ci_type=:SEM in the plot recipe.","category":"page"},{"location":"features/ensemble/#Example-3:-Using-the-Reduction-to-Halt-When-Estimator-is-Within-Tolerance","page":"Parallel Ensemble Simulations","title":"Example 3: Using the Reduction to Halt When Estimator is Within Tolerance","text":"","category":"section"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"In this problem we will solve the equation just as many times as needed to get the standard error of the mean for the final time point below our tolerance 0.5. Since we only care about the endpoint, we can tell the output_func to discard the rest of the data.","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"function output_func(sol,i)\n  last(sol)\nend","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Our prob_func will simply randomize the initial condition:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"# Linear ODE which starts at 0.5 and solves from t=0.0 to t=1.0\nprob = ODEProblem((u,p,t)->1.01u,0.5,(0.0,1.0))\n\nfunction prob_func(prob,i,repeat)\n  remake(prob,u0=rand()*prob.u0)\nend","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Our reduction function will append the data from the current batch to the previous batch, and declare convergence if the standard error of the mean is calculated as sufficiently small:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"function reduction(u,batch,I)\n  u = append!(u,batch)\n  finished = (var(u) / sqrt(last(I))) / mean(u) < 0.5\n  u, finished\nend","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Then we can define and solve the problem:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"prob2 = EnsembleProblem(prob,prob_func=prob_func,output_func=output_func,reduction=reduction,u_init=Vector{Float64}())\nsim = solve(prob2,Tsit5(),trajectories=10000,batch_size=20)","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Since batch_size=20, this means that every 20 simulations, it will take this batch, append the results to the previous batch, calculate (var(u)/sqrt(last(I)))/mean(u), and if that's small enough, exit the simulation. In this case, the simulation exits only after 20 simulations (i.e. after calculating the first batch). This can save a lot of time!","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"In addition to saving time by checking convergence, we can save memory by reducing between batches. For example, say we only care about the mean at the end once again. Instead of saving the solution at the end for each trajectory, we can instead save the running summation of the endpoints:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"function reduction(u,batch,I)\n  u+sum(batch),false\nend\nprob2 = EnsembleProblem(prob,prob_func=prob_func,output_func=output_func,reduction=reduction,u_init=0.0)\nsim2 = solve(prob2,Tsit5(),trajectories=100,batch_size=20)","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"this will sum up the endpoints after every 20 solutions, and save the running sum. The final result will have sim2.u as simply a number, and thus sim2.u/100 would be the mean.","category":"page"},{"location":"features/ensemble/#Example-4:-Using-the-Analysis-Tools","page":"Parallel Ensemble Simulations","title":"Example 4: Using the Analysis Tools","text":"","category":"section"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"In this example we will show how to analyze a EnsembleSolution. First, let's generate a 10 solution Monte Carlo experiment. For our problem we will use a 4x2 system of linear stochastic differential equations:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"function f(du,u,p,t)\n  for i = 1:length(u)\n    du[i] = 1.01*u[i]\n  end\nend\nfunction σ(du,u,p,t)\n  for i in 1:length(u)\n    du[i] = .87*u[i]\n  end\nend\nprob = SDEProblem(f,σ,ones(4,2)/2,(0.0,1.0)) #prob_sde_2Dlinear","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"To solve this 10 times, we use the EnsembleProblem constructor and solve with trajectories=10. Since we wish to compare values at the timesteps, we need to make sure the steps all hit the same times. Thus we set adaptive=false and explicitly give a dt.","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"prob2 = EnsembleProblem(prob)\nsim = solve(prob2,SRIW1(),dt=1//2^(3),trajectories=10,adaptive=false)","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Note that if you don't do the timeseries_steps calculations, this code is compatible with adaptive timestepping. Using adaptivity is usually more efficient!","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"We can compute the mean and the variance at the 3rd timestep using:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"m,v = timestep_meanvar(sim,3)","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"or we can compute the mean and the variance at the t=0.5 using:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"m,v = timepoint_meanvar(sim,0.5)","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"We can get a series for the mean and the variance at each time step using:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"m_series,v_series = timeseries_steps_meanvar(sim)","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"or at chosen values of t:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"ts = 0:0.1:1\nm_series = timeseries_point_mean(sim,ts)","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Note that these mean and variance series can be directly plotted. We can compute covariance matrices similarly:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"timeseries_steps_meancov(sim) # Use the time steps, assume fixed dt\ntimeseries_point_meancov(sim,0:1//2^(3):1,0:1//2^(3):1) # Use time points, interpolate","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"For general analysis, we can build a EnsembleSummary type.","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"summ = EnsembleSummary(sim)","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"will summarize at each time step, while","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"summ = EnsembleSummary(sim,0.0:0.1:1.0)","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"will summarize at the 0.1 time points using the interpolations. To visualize the results we can plot it. Since there are 8 components to the differential equation, this can get messy, so let's only plot the 3rd component:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"plot(summ;idxs=3)","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"(Image: monte_ribbon)","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"We can change to errorbars instead of ribbons and plot two different indices:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"plot(summ;idxs=(3,5),error_style=:bars)","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"(Image: monte_bars)","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"Or we can simply plot the mean of every component over time:","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"plot(summ;error_style=:none)","category":"page"},{"location":"features/ensemble/","page":"Parallel Ensemble Simulations","title":"Parallel Ensemble Simulations","text":"(Image: monte_means)","category":"page"},{"location":"types/jump_types/#Jump-Problems","page":"Jump Problems","title":"Jump Problems","text":"","category":"section"},{"location":"types/jump_types/#Mathematical-Specification-of-an-problem-with-jumps","page":"Jump Problems","title":"Mathematical Specification of an problem with jumps","text":"","category":"section"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"Jumps are defined as a Poisson process which changes states at some rate. When there are multiple possible jumps, the process is a compound Poisson process. On their own, a jump equation is continuous-time Markov Chain where the time to the next jump is exponentially distributed as calculated by the rate. This type of process, known in biology as \"Gillespie discrete stochastic simulations\" and modeled by the Chemical Master Equation (CME), is the same thing as adding jumps to a DiscreteProblem. However, any differential equation can be extended by jumps as well. For example, we have an ODE with jumps, denoted by","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"fracdudt = f(upt) + Σ c_i(upt)dp_i","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"where dp_i is a Poisson counter of rate lambda_i(upt). Extending a stochastic differential equation to have jumps is commonly known as a Jump Diffusion, and is denoted by","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"fracdudt = f(upt) + Σgᵢ(ut)dWⁱ + Σ c_i(upt)dp_i","category":"page"},{"location":"types/jump_types/#Types-of-Jumps:-Regular,-Variable,-Constant-Rate-and-Mass-Action","page":"Jump Problems","title":"Types of Jumps: Regular, Variable, Constant Rate and Mass Action","text":"","category":"section"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"A RegularJump is a set of jumps that do not make structural changes to the underlying equation. These kinds of jumps only change values of the dependent variable (u) and thus can be treated in an inexact manner. Other jumps, such as those which change the size of u, require exact handling which is also known as time-adaptive jumping. These can only be specified as a ConstantRateJump, MassActionJump, or a VariableRateJump.","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"We denote a jump as variable rate if its rate function is dependent on values which may change between constant rate jumps. For example, if there are multiple jumps whose rates only change when one of them occur, than that set of jumps is a constant rate jump. If a jump's rate depends on the differential equation, time, or by some value which changes outside of any constant rate jump, then it is denoted as variable.","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"A MassActionJump is a specialized representation for a collection of constant rate jumps that can each be interpreted as a standard mass action reaction. For systems comprised of many mass action reactions, using the MassActionJump type will offer improved performance. Note, only one MassActionJump should be defined per JumpProblem; it is then responsible for handling all mass action reaction type jumps. For systems with both mass action jumps and non-mass action jumps, one can create one MassActionJump to handle the mass action jumps, and create a number of ConstantRateJumps to handle the non-mass action jumps.","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"RegularJumps are optimized for regular jumping algorithms like tau-leaping and hybrid algorithms. ConstantRateJumps and MassActionJumps are optimized for SSA algorithms. ConstantRateJumps, MassActionJumps and VariableRateJumps can be added to standard DiffEq algorithms since they are simply callbacks, while RegularJumps require special algorithms. ","category":"page"},{"location":"types/jump_types/#Defining-a-Regular-Jump","page":"Jump Problems","title":"Defining a Regular Jump","text":"","category":"section"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"The constructor for a RegularJump is:","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"RegularJump(rate,c,numjumps;mark_dist = nothing)","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"rate(out,u,p,t) is the function which computes the rate for every regular jump process\nc(du,u,p,t,counts,mark) is calculates the update given counts number of jumps for each jump process in the interval.\nnumjumps is the number of jump processes, i.e. the number of rate equations and the number of counts\nmark_dist is the distribution for the mark.","category":"page"},{"location":"types/jump_types/#Defining-a-Constant-Rate-Jump","page":"Jump Problems","title":"Defining a Constant Rate Jump","text":"","category":"section"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"The constructor for a ConstantRateJump is:","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"ConstantRateJump(rate,affect!)","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"rate(u,p,t) is a function which calculates the rate given the time and the state.\naffect!(integrator) is the effect on the equation, using the integrator interface.","category":"page"},{"location":"types/jump_types/#Defining-a-Mass-Action-Jump","page":"Jump Problems","title":"Defining a Mass Action Jump","text":"","category":"section"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"The constructor for a MassActionJump is:","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"MassActionJump(reactant_stoich, net_stoich; scale_rates = true, param_idxs=nothing)","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"reactant_stoich is a vector whose kth entry is the reactant stoichiometry of the kth reaction. The reactant stoichiometry for an individual reaction is assumed to be represented as a vector of Pairs, mapping species id to stoichiometric coefficient.\nnet_stoich is assumed to have the same type as reactant_stoich; a vector whose kth entry is the net stoichiometry of the kth reaction. The net stoichiometry for an individual reaction is again represented as a vector of Pairs, mapping species id to the net change in the species when the reaction occurs.\nscale_rates is an optional parameter that specifies whether the rate constants correspond to stochastic rate constants in the sense used by Gillespie, and hence need to be rescaled. The default, scale_rates=true, corresponds to rescaling the passed in rate constants. See below.\nparam_idxs is a vector of the indices within the parameter vector, p, that correspond to the rate constant for each jump.","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"Notes for Mass Action Jumps","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"When using MassActionJump the default behavior is to assume rate constants correspond to stochastic rate constants in the sense used by Gillespie (J. Comp. Phys., 1976, 22 (4)). This means that for a reaction such as 2A oversetkrightarrow B, the jump rate function constructed by MassActionJump would be k*A*(A-1)/2!. For a trimolecular reaction like 3A oversetkrightarrow B the rate function would be k*A*(A-1)*(A-2)/3!. To avoid having the reaction rates rescaled (by 1/2 and 1/6 for these two examples), one can pass the MassActionJump constructor the optional named parameter scale_rates=false, i.e. use\nMassActionJump(reactant_stoich, net_stoich; scale_rates = false, param_idxs)\nZero order reactions can be passed as reactant_stoichs in one of two ways. Consider the varnothing oversetkrightarrow A reaction with rate k=1:\np = [1.]\nreactant_stoich = [[0 => 1]]\nnet_stoich = [[1 => 1]]\njump = MassActionJump(reactant_stoich, net_stoich; param_idxs=[1])\nAlternatively one can create an empty vector of pairs to represent the reaction:\np = [1.]\nreactant_stoich = [Vector{Pair{Int,Int}}()]\nnet_stoich = [[1 => 1]]\njump = MassActionJump(reactant_stoich, net_stoich; param_idxs=[1])\nFor performance reasons, it is recommended to order species indices in stoichiometry vectors from smallest to largest. That is \nreactant_stoich = [[1 => 2, 3 => 1, 4 => 2], [2 => 2, 3 => 2]]\nis preferred over\nreactant_stoich = [[3 => 1, 1 => 2, 4 = > 2], [3 => 2, 2 => 2]]","category":"page"},{"location":"types/jump_types/#Defining-a-Variable-Rate-Jump","page":"Jump Problems","title":"Defining a Variable Rate Jump","text":"","category":"section"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"The constructor for a VariableRateJump is:","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"VariableRateJump(rate,affect!;\n                   idxs = nothing,\n                   rootfind=true,\n                   save_positions=(true,true),\n                   interp_points=10,\n                   abstol=1e-12,reltol=0)","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"Note that this is the same as defining a ContinuousCallback, except that instead of the condition function, you provide a rate(u,p,t) function for the rate at a given time and state.","category":"page"},{"location":"types/jump_types/#Defining-a-Jump-Problem","page":"Jump Problems","title":"Defining a Jump Problem","text":"","category":"section"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"To define a JumpProblem, you must first define the basic problem. This can be a DiscreteProblem if there is no differential equation, or an ODE/SDE/DDE/DAE if you would like to augment a differential equation with jumps. Denote this previously defined problem as prob. Then the constructor for the jump problem is:","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"JumpProblem(prob,aggregator::Direct,jumps::JumpSet;\n            save_positions = typeof(prob) <: AbstractDiscreteProblem ? (false,true) : (true,true))","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"The aggregator is the method for aggregating the constant jumps. These are defined below. jumps is a JumpSet which is just a gathering of jumps. Instead of passing a JumpSet, one may just pass a list of jumps themselves. For example:","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"JumpProblem(prob,aggregator,jump1,jump2)","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"and the internals will automatically build the JumpSet. save_positions is the save_positions argument built by the aggregation of the constant rate jumps.","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"Note that a JumpProblem/JumpSet can only have 1 RegularJump (since a RegularJump itself describes multiple processes together). Similarly, it can only have one MassActionJump (since it also describes multiple processes together).","category":"page"},{"location":"types/jump_types/#Constant-Rate-Jump-Aggregators","page":"Jump Problems","title":"Constant Rate Jump Aggregators","text":"","category":"section"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"Constant rate jump aggregators are the methods by which constant rate jumps, including MassActionJumps, are lumped together. This is required in all algorithms for both speed and accuracy. The current methods are:","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"Direct: the Gillespie Direct method SSA.\nRDirect: A variant of Gillespie's Direct method that uses rejection to sample the next reaction.\nDirectCR: The Composition-Rejection Direct method of Slepoy et al. For large networks and linear chain-type networks it will often give better performance than Direct. (Requires dependency graph, see below.)\nDirectFW: the Gillespie Direct method SSA with FunctionWrappers. This aggregator uses a different internal storage format for collections of ConstantRateJumps. \nFRM: the Gillespie first reaction method SSA. Direct should generally offer better performance and be preferred to FRM.\nFRMFW: the Gillespie first reaction method SSA with FunctionWrappers.\nNRM: The Gibson-Bruck Next Reaction Method. For some reaction network  structures this may offer better performance than Direct (for example,  large, linear chains of reactions). (Requires dependency graph, see below.) \nRSSA: The Rejection SSA (RSSA) method of Thanh et al. With RSSACR, for very large reaction networks it often offers the best performance of all methods. (Requires dependency graph, see below.)\nRSSACR: The Rejection SSA (RSSA) with Composition-Rejection method of Thanh et al. With RSSA, for very large reaction networks it often offers the best performance of all methods. (Requires dependency graph, see below.)\nSortingDirect: The Sorting Direct Method of McCollum et al. It will usually offer performance as good as Direct, and for some systems can offer substantially better performance. (Requires dependency graph, see below.)","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"To pass the aggregator, pass the instantiation of the type. For example:","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"JumpProblem(prob,Direct(),jump1,jump2)","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"will build a problem where the constant rate jumps are solved using Gillespie's Direct SSA method.","category":"page"},{"location":"types/jump_types/#Constant-Rate-Jump-Aggregators-Requiring-Dependency-Graphs","page":"Jump Problems","title":"Constant Rate Jump Aggregators Requiring Dependency Graphs","text":"","category":"section"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"Italicized constant rate jump aggregators require the user to pass a dependency graph to JumpProblem. DirectCR, NRM and SortingDirect require a jump-jump dependency graph, passed through the named parameter dep_graph. i.e.","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"JumpProblem(prob,DirectCR(),jump1,jump2; dep_graph=your_dependency_graph)","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"For systems with only MassActionJumps, or those generated from a Catalyst reaction_network, this graph will be auto-generated. Otherwise you must construct the dependency graph manually. Dependency graphs are represented as a Vector{Vector{Int}}, with the ith vector containing the indices of the jumps for which rates must be recalculated when the ith jump occurs. Internally, all MassActionJumps are ordered before ConstantRateJumps (with the latter internally ordered in the same order they were passed in).","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"RSSA and RSSACR require two different types of dependency graphs, passed through the following JumpProblem kwargs:","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"vartojumps_map - A Vector{Vector{Int}} mapping each variable index, i, to a set of jump indices. The jump indices correspond to jumps with rate functions that depend on the value of u[i].\njumptovars_map - A Vector{Vector{Int}}  mapping each jump index to a set  of variable indices. The corresponding variables are those that have their  value, u[i], altered when the jump occurs.","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"For systems generated from a Catalyst reaction_network these will be auto-generated. Otherwise you must explicitly construct and pass in these mappings.","category":"page"},{"location":"types/jump_types/#Recommendations-for-Constant-Rate-Jumps","page":"Jump Problems","title":"Recommendations for Constant Rate Jumps","text":"","category":"section"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"For representing and aggregating constant rate jumps ","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"Use a MassActionJump to handle all jumps that can be represented as mass action reactions. This will generally offer the fastest performance. \nUse ConstantRateJumps for any remaining jumps.\nFor a small number of jumps, < ~10, Direct will often perform as well as the other aggregators.\nFor > ~10 jumps SortingDirect will often offer better performance than Direct.\nFor large numbers of jumps with sparse chain like structures and similar jump rates, for example continuous time random walks, RSSACR, DirectCR and then NRM often have the best performance.\nFor very large networks, with many updates per jump, RSSA and RSSACR will often substantially outperform the other methods. ","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"In general, for systems with sparse dependency graphs if Direct is slow, one of SortingDirect, RSSA or RSSACR will usually offer substantially better performance. See DiffEqBenchmarks.jl for benchmarks on several example networks.","category":"page"},{"location":"types/jump_types/#Remaking-JumpProblems","page":"Jump Problems","title":"Remaking JumpProblems","text":"","category":"section"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"When running many simulations, it can often be convenient to update the initial condition or simulation parameters without having to create and initialize a new JumpProblem. In such situations remake can be used to change the initial condition, time span, and the parameter vector. Note, the new JumpProblem will alias internal data structures from the old problem, including core components of the SSA aggregators. As such, only the new problem generated by remake should be used for subsequent simulations.","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"As an example, consider the following SIR model:","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"rate1(u,p,t) = (0.1/1000.0)*u[1]*u[2]\nfunction affect1!(integrator)\n  integrator.u[1] -= 1\n  integrator.u[2] += 1\nend\njump = ConstantRateJump(rate1,affect1!)\n\nrate2(u,p,t) = 0.01u[2]\nfunction affect2!(integrator)\n  integrator.u[2] -= 1\n  integrator.u[3] += 1\nend\njump2 = ConstantRateJump(rate2,affect2!)\nu0    = [999,1,0]\np     = (0.1/1000,0.01)\ntspan = (0.0,250.0)\ndprob = DiscreteProblem(u0, tspan, p)\njprob = JumpProblem(dprob, Direct(), jump, jump2)\nsol   = solve(jprob, SSAStepper())","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"We can change any of u0, p and tspan by either making a new DiscreteProblem","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"u02    = [10,1,0]\np2     = (.1/1000, 0.0)\ntspan2 = (0.0,2500.0)\ndprob2 = DiscreteProblem(u02, tspan2, p2)\njprob2 = remake(jprob, prob=dprob2)\nsol2   = solve(jprob2, SSAStepper())","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"or by directly remaking with the new parameters","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"jprob2 = remake(jprob, u0=u02, p=p2, tspan=tspan2)\nsol2   = solve(jprob2, SSAStepper())","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"To avoid ambiguities, the following will give an error","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"jprob2 = remake(jprob, prob=dprob2, u0=u02)","category":"page"},{"location":"types/jump_types/","page":"Jump Problems","title":"Jump Problems","text":"as will trying to update either p or tspan while passing a new DiscreteProblem using the prob kwarg.","category":"page"},{"location":"solvers/benchmarks/#Solver-Benchmarks","page":"Solver Benchmarks","title":"Solver Benchmarks","text":"","category":"section"},{"location":"solvers/benchmarks/","page":"Solver Benchmarks","title":"Solver Benchmarks","text":"Benchmarks for the solvers can be found at SciMLBenchmarks.jl. Many different problems are tested. However, if you would like additional problems to be benchmarked, please open an issue or PR at the SciMLBenchmarks.jl repository with the code that defines the DEProblem.","category":"page"},{"location":"tutorials/dae_example/#Differential-Algebraic-Equations","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"","category":"section"},{"location":"tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"This tutorial will introduce you to the functionality for solving differential algebraic equations (DAEs). Other introductions can be found by checking out DiffEqTutorials.jl. ","category":"page"},{"location":"tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"note: Note\nThis tutorial assumes you have read the Ordinary Differential Equations tutorial.","category":"page"},{"location":"tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"In this example we will solve the implicit ODE equation","category":"page"},{"location":"tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"f(duupt) = 0","category":"page"},{"location":"tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"where f is a variant of the Roberts equation. This equation is a DAE of the form:","category":"page"},{"location":"tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"beginaligned\nfracdudt = f(upt) \n 0 = g(upt) \n endaligned","category":"page"},{"location":"tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"which is also known as a constrained differential equation, where g is the constraint equation. The Robertson model can be written in the form:","category":"page"},{"location":"tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"beginaligned\nfracdy_1dt = -004y₁ + 10^4 y_2 y_3 \nfracdy_2dt = 004 y_1 - 10^4 y_2 y_3 - 3*10^7 y_2^2 \n1 =  y_1 + y_2 + y_3 \nendaligned","category":"page"},{"location":"tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"with initial conditions y_1(0) = 1, y_2(0) = 0, y_3(0) = 0, dy_1 = - 004, dy_2 = 004, and dy_3 = 00.","category":"page"},{"location":"tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"The workflow for DAEs is the same as for the other types of equations, where all you need to know is how to define the problem. A DAEProblem is specified by defining an in-place update f(out,du,u,p,t) which uses the values to mutate out as the output. To makes this into a DAE, we move all of the variables to one side. Thus, we can define the function:","category":"page"},{"location":"tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"function f(out,du,u,p,t)\n  out[1] = - 0.04u[1]              + 1e4*u[2]*u[3] - du[1]\n  out[2] = + 0.04u[1] - 3e7*u[2]^2 - 1e4*u[2]*u[3] - du[2]\n  out[3] = u[1] + u[2] + u[3] - 1.0\nend","category":"page"},{"location":"tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"with initial conditions","category":"page"},{"location":"tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"u₀ = [1.0, 0, 0]\ndu₀ = [-0.04, 0.04, 0.0]\ntspan = (0.0,100000.0)","category":"page"},{"location":"tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"and make the DAEProblem:","category":"page"},{"location":"tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"using DifferentialEquations\ndifferential_vars = [true,true,false]\nprob = DAEProblem(f,du₀,u₀,tspan,differential_vars=differential_vars)","category":"page"},{"location":"tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"differential_vars is an option which states which of the variables are differential, i.e. not purely algebraic (which means that their derivative shows up in the residual equations). This is required for the algorithm to be able to find consistent initial conditions. Notice that the first two variables are determined by their changes, but the last is simply determined by the conservation equation. Thus, we use differential_vars = [true,true,false].","category":"page"},{"location":"tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"As with the other DifferentialEquations problems, the commands are then to solve and plot. Here we will use the IDA solver from Sundials:","category":"page"},{"location":"tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"using Sundials\nsol = solve(prob,IDA())","category":"page"},{"location":"tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"In order to clearly see all the features of this solution, it should be plotted on a logarithmic scale. We'll also plot each on a different subplot, to allow scaling the y-axis appropriately.","category":"page"},{"location":"tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"using Plots\nplot(sol, xscale=:log10, tspan=(1e-6, 1e5), layout=(3,1))","category":"page"},{"location":"tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"This gives the following plot:","category":"page"},{"location":"tutorials/dae_example/","page":"Differential Algebraic Equations","title":"Differential Algebraic Equations","text":"(Image: IntroDAEPlot)","category":"page"},{"location":"analysis/dev_and_test/#Algorithm-Development-and-Testing","page":"Algorithm Development and Testing","title":"Algorithm Development and Testing","text":"","category":"section"},{"location":"analysis/dev_and_test/","page":"Algorithm Development and Testing","title":"Algorithm Development and Testing","text":"Algorithm developing and testing tools are provided by DiffEqDevTools.jl and are documented in the developer documentation.","category":"page"},{"location":"analysis/dev_and_test/#Installation","page":"Algorithm Development and Testing","title":"Installation","text":"","category":"section"},{"location":"analysis/dev_and_test/","page":"Algorithm Development and Testing","title":"Algorithm Development and Testing","text":"This functionality does not come standard with DifferentialEquations.jl. To use this functionality, you must install DiffEqDevTools.jl:","category":"page"},{"location":"analysis/dev_and_test/","page":"Algorithm Development and Testing","title":"Algorithm Development and Testing","text":"]add DiffEqDevTools\nusing DiffEqDevTools","category":"page"},{"location":"solvers/dynamical_solve/#Dynamical,-Hamiltonian,-and-2nd-Order-ODE-Solvers","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"","category":"section"},{"location":"solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"Dynamical ODEs, such as those arising from Hamiltonians or second order ordinary differential equations, give rise to a special structure that can be specialized on in the solver for more efficiency. These algorithms require an ODE defined in the following ways:","category":"page"},{"location":"solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"DynamicalODEProblem{isinplace}(f1,f2,v0,u0,tspan,p=NullParameters();kwargs...)\nSecondOrderODEProblem{isinplace}(f,du0,u0,tspan,p=NullParameters();kwargs...)\nHamiltonianProblem{T}(H,p0,q0,tspan,p=NullParameters();kwargs...)","category":"page"},{"location":"solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"These correspond to partitioned equations of motion:","category":"page"},{"location":"solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"fracdvdt = f_1(tu) \nfracdudt = f_2(v) ","category":"page"},{"location":"solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"The functions should be specified as f1(dv,v,u,p,t) and f2(du,v,u,p,t) (in the inplace form), where f1 is independent of v (unless specified by the solver), and f2 is independent of t and u. This includes discretizations arising from SecondOrderODEProblems where the velocity is not used in the acceleration function, and Hamiltonians where the potential is (or can be) time-dependent but the kinetic energy is only dependent on v.","category":"page"},{"location":"solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"Note that some methods assume that the integral of f2 is a quadratic form. That means that f2=v'*M*v, i.e. int f_2 = frac12 m v^2, giving du = v. This is equivalent to saying that the kinetic energy is related to v^2. The methods which require this assumption will lose accuracy if this assumption is violated. Methods listed below make note of this requirement with \"Requires quadratic kinetic energy\".","category":"page"},{"location":"solvers/dynamical_solve/#Recommendations","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Recommendations","text":"","category":"section"},{"location":"solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"When energy conservation is required, use a symplectic method. Otherwise the Runge-Kutta-Nyström methods will be more efficient. Energy is mostly conserved by Runge-Kutta-Nyström methods, but is not conserved for long time integrations. Thus it is suggested that for shorter integrations you use Runge-Kutta-Nyström methods as well.","category":"page"},{"location":"solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"As a go-to method for efficiency, DPRKN6 is a good choice. DPRKN12 is a good choice when high accuracy, like tol<1e-10 is necessary. However, DPRKN6 is the only Runge-Kutta-Nyström method with a higher order interpolant (all default to order 3 Hermite, whereas DPRKN6 is order 6th interpolant) and thus in cases where interpolation matters (ex: event handling) one should use DPRKN6. For very smooth problems with expensive acceleration function evaluations, IRKN4 can be a good choice as it minimizes the number of evaluations.","category":"page"},{"location":"solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"For symplectic methods, higher order algorithms are the most efficient when higher accuracy is needed, and when less accuracy is needed lower order methods do better. Optimized efficiency methods take more steps and thus have more force calculations for the same order, but have smaller error. Thus the \"optimized efficiency\" algorithms are recommended if your force calculation is not too sufficiency large, while the other methods are recommend when force calculations are really large (for example, like in MD simulations VelocityVerlet is very popular since it only requires one force calculation per timestep). A good go-to method would be McAte5, and a good high order choice is KahanLi8.","category":"page"},{"location":"solvers/dynamical_solve/#Standard-ODE-Integrators","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Standard ODE Integrators","text":"","category":"section"},{"location":"solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"The standard ODE integrators will work on Dynamical ODE problems via an automatic transformation to a first-order ODE. See the ODE solvers page for more details.","category":"page"},{"location":"solvers/dynamical_solve/#Specialized-OrdinaryDiffEq.jl-Integrators","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Specialized OrdinaryDiffEq.jl Integrators","text":"","category":"section"},{"location":"solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"Unless otherwise specified, the OrdinaryDiffEq algorithms all come with a 3rd order Hermite polynomial interpolation. The algorithms denoted as having a \"free\" interpolation means that no extra steps are required for the interpolation. For the non-free higher order interpolating functions, the extra steps are computed lazily (i.e. not during the solve).","category":"page"},{"location":"solvers/dynamical_solve/#Runge-Kutta-Nyström-Integrators","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Runge-Kutta-Nyström Integrators","text":"","category":"section"},{"location":"solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"Nystrom4: 4th order explicit Runge-Kutta-Nyström method. Allows acceleration to depend on velocity. Fixed timestep only.\nIRKN3: 4th order explicit two-step Runge-Kutta-Nyström method. Fixed timestep only.\nIRKN4: 4th order explicit two-step Runge-Kutta-Nyström method. Can be more efficient for smooth problems. Fixed timestep only.\nERKN4: 4th order Runge-Kutta-Nyström method which is integrates the periodic properties of the harmonic oscillator exactly. Gets extra efficiency on periodic problems.\nERKN5: 5th order Runge-Kutta-Nyström method which is integrates the periodic properties of the harmonic oscillator exactly. Gets extra efficiency on periodic problems.\nNystrom4VelocityIndependent: 4th order explicit Runge-Kutta-Nyström method. Fixed timestep only.\nNystrom5VelocityIndependent: 5th order explicit Runge-Kutta-Nyström method. Fixed timestep only.\nDPRKN6: 6th order explicit adaptive Runge-Kutta-Nyström method. Free 6th order interpolant.\nDPRKN8: 8th order explicit adaptive Runge-Kutta-Nyström method.\nDPRKN12: 12th order explicit adaptive Runge-Kutta-Nyström method.","category":"page"},{"location":"solvers/dynamical_solve/#Symplectic-Integrators","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Symplectic Integrators","text":"","category":"section"},{"location":"solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"Note that all symplectic integrators are fixed timestep only.","category":"page"},{"location":"solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"SymplecticEuler: First order explicit symplectic integrator\nVelocityVerlet: 2nd order explicit symplectic integrator. Requires f_2(t,u) = v, i.e. a second order ODE.\nVerletLeapfrog: 2nd order explicit symplectic integrator.\nPseudoVerletLeapfrog: 2nd order explicit symplectic integrator.\nMcAte2: Optimized efficiency 2nd order explicit symplectic integrator.\nRuth3: 3rd order explicit symplectic integrator.\nMcAte3: Optimized efficiency 3rd order explicit symplectic integrator.\nCandyRoz4: 4th order explicit symplectic integrator.\nMcAte4: 4th order explicit symplectic integrator. Requires quadratic kinetic energy.\nCalvoSanz4: Optimized efficiency 4th order explicit symplectic integrator.\nMcAte42: 4th order explicit symplectic integrator. (Broken)\nMcAte5: Optimized efficiency 5th order explicit symplectic integrator. Requires quadratic kinetic energy\nYoshida6: 6th order explicit symplectic integrator.\nKahanLi6: Optimized efficiency 6th order explicit symplectic integrator.\nMcAte8: 8th order explicit symplectic integrator.\nKahanLi8: Optimized efficiency 8th order explicit symplectic integrator.\nSofSpa10: 10th order explicit symplectic integrator.","category":"page"},{"location":"solvers/dynamical_solve/#GeometricIntegrators.jl","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"GeometricIntegrators.jl","text":"","category":"section"},{"location":"solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"GeometricIntegrators.jl is a set of fixed timestep algorithms written in Julia. Note that this setup is not automatically included with DifferentialEquaitons.jl. To use the following algorithms, you must install and use GeometricIntegratorsDiffEq.jl:","category":"page"},{"location":"solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"Pkg.clone(\"https://github.com/JuliaDiffEq/GeometricIntegratorsDiffEq.jl\")\nusing GeometricIntegratorsDiffEq","category":"page"},{"location":"solvers/dynamical_solve/","page":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","title":"Dynamical, Hamiltonian, and 2nd Order ODE Solvers","text":"GISymplecticEulerA - First order explicit symplectic Euler A\nGISymplecticEulerB - First order explicit symplectic Euler B\nGILobattoIIIAIIIB2 - Second order Gauss-Labatto-IIIA-IIIB\nGILobattoIIIBIIIA2 - Second order Gauss-Labatto-IIIB-IIIA","category":"page"},{"location":"basics/common_solver_opts/#solver_options","page":"Common Solver Options","title":"Common Solver Options","text":"","category":"section"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"The DifferentialEquations.jl universe has a large set of common arguments available for the solve function. These arguments apply to solve on any problem type and are only limited by limitations of the specific implementations.","category":"page"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"Many of the defaults depend on the algorithm or the package the algorithm derives from. Not all of the interface is provided by every algorithm. For more detailed information on the defaults and the available options for specific algorithms / packages, see the manual pages for the solvers of specific problems. To see whether a specific package is compaible with the use of a given option, see the Solver Compatibility Chart","category":"page"},{"location":"basics/common_solver_opts/#Default-Algorithm-Hinting","page":"Common Solver Options","title":"Default Algorithm Hinting","text":"","category":"section"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"To help choose the default algorithm, the keyword argument alg_hints is provided to solve. alg_hints is a Vector{Symbol} which describe the problem at a high level to the solver. The options are:","category":"page"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":":auto vs :nonstiff vs :stiff - Denotes the equation as nonstiff/stiff. :auto allow the default handling algorithm to choose stiffness detection algorithms. The default handling defaults to using :auto.","category":"page"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"Currently unused options include:","category":"page"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":":interpolant - Denotes that a high-precision interpolation is important.\n:memorybound - Denotes that the solver will be memory bound.","category":"page"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"This functionality is derived via the benchmarks in DiffEqBenchmarks.jl","category":"page"},{"location":"basics/common_solver_opts/#SDE-Specific-Alghints","page":"Common Solver Options","title":"SDE Specific Alghints","text":"","category":"section"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":":additive - Denotes that the underlying SDE has additive noise.\n:stratonovich - Denotes that the solution should adhere to the Stratonovich interpretation.","category":"page"},{"location":"basics/common_solver_opts/#Output-Control","page":"Common Solver Options","title":"Output Control","text":"","category":"section"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"These arguments control the output behavior of the solvers. It defaults to maximum output to give the best interactive user experience, but can be reduced all the way to only saving the solution at the final timepoint.","category":"page"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"The following options are all related to output control. See the \"Examples\" section at the end of this page for some example usage.","category":"page"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"dense: Denotes whether to save the extra pieces required for dense (continuous) output. Default is save_everystep && !isempty(saveat) for algorithms which have the ability to produce dense output, i.e. by default it's true unless the user has turned off saving on steps or has chosen a saveat value. If dense=false, the solution still acts like a function, and sol(t) is a linear interpolation between the saved time points.\nsaveat: Denotes specific times to save the solution at, during the solving phase. The solver will save at each of the timepoints in this array in the most efficient manner available to the solver. If only saveat is given, then the arguments save_everystep and dense are false by default. If saveat is given a number, then it will automatically expand to tspan[1]:saveat:tspan[2]. For methods where interpolation is not possible, saveat may be equivalent to tstops. The default value is [].\nsave_idxs: Denotes the indices for the components of the equation to save. Defaults to saving all indices. For example, if you are solving a 3-dimensional ODE, and given save_idxs = [1, 3], only the first and third components of the solution will be outputted. Notice that of course in this case the outputed solution will be two-dimensional.\ntstops: Denotes extra times that the timestepping algorithm must step to. This should be used to help the solver deal with discontinuities and singularities, since stepping exactly at the time of the discontinuity will improve accuracy. If a method cannot change timesteps (fixed timestep multistep methods), then tstops will use an interpolation, matching the behavior of saveat. If a method cannot change timesteps and also cannot interpolate, then tstops must be a multiple of dt or else an error will be thrown. Default is [].\nd_discontinuities: Denotes locations of discontinuities in low order derivatives. This will force FSAL algorithms which assume derivative continuity to re-evaluate the derivatives at the point of discontinuity. The default is [].\nsave_everystep: Saves the result at every step. Default is true if isempty(saveat).\nsave_on: Denotes whether intermediate solutions are saved. This overrides the settings of dense, saveat and save_everystep and is used by some applicatioins to manually turn off saving temporarily. Everyday use of the solvers should leave this unchanged. Defaults to true.\nsave_start: Denotes whether the initial condition should be included in the solution type as the first timepoint. Defaults to true.\nsave_end: Denotes whether the final timepoint is forced to be saved, regardless of the other saving settings. Defaults to true.\ninitialize_save: Denotes whether to save after the callback initialization phase (when u_modified=true). Defaults to true.","category":"page"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"Note that dense requires save_everystep=true and saveat=false. If you need additional saving while keeping dense output, see the SavingCallback in the Callback Library.","category":"page"},{"location":"basics/common_solver_opts/#Stepsize-Control","page":"Common Solver Options","title":"Stepsize Control","text":"","category":"section"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"These arguments control the timestepping routines.","category":"page"},{"location":"basics/common_solver_opts/#Basic-Stepsize-Control","page":"Common Solver Options","title":"Basic Stepsize Control","text":"","category":"section"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"These are the standard options for controlling stepping behavior. Error estimates do the comparison","category":"page"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"err_scaled = err(abstol + max(uprevu)*reltol)","category":"page"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"The scaled error is guaranteed to be <1 for a given local error estimate (note: error estimates are local unless the method specifies otherwise). abstol controls the non-scaling error and thus can be thought of as the error around zero. reltol scales with the size of the dependent variables and so one can interpret reltol=1e-3 as roughly being (locally) correct to 3 digits. Note tolerances can be specified element-wise by passing a vector whose size matches u0.","category":"page"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"adaptive: Turns on adaptive timestepping for appropriate methods. Default is true.\nabstol: Absolute tolerance in adaptive timestepping. This is the tolerance on local error estimates, not necessarily the global error (though these quantities are related). Defaults to 1e-6 on deterministic equations (ODEs/DDEs/DAEs) and 1e-2 on stochastic equations (SDEs/RODEs).\nreltol: Relative tolerance in adaptive timestepping.  This is the tolerance on local error estimates, not necessarily the global error (though these quantities are related). Defaults to 1e-3 on deterministic equations (ODEs/DDEs/DAEs) and 1e-2 on stochastic equations (SDEs/RODEs).\ndt: Sets the initial stepsize. This is also the stepsize for fixed timestep methods. Defaults to an automatic choice if the method is adaptive.\ndtmax: Maximum dt for adaptive timestepping. Defaults are package-dependent.\ndtmin: Minimum dt for adaptive timestepping. Defaults are package-dependent.\nforce_dtmin: Declares whether to continue, forcing the minimum dt usage. Default is false, which has the solver throw a warning and exit early when encountering the minimum dt. Setting this true allows the solver to continue, never letting dt go below dtmin (and ignoring error tolerances in those cases). Note that true is not compatible with most interop packages.","category":"page"},{"location":"basics/common_solver_opts/#Fixed-Stepsize-Usage","page":"Common Solver Options","title":"Fixed Stepsize Usage","text":"","category":"section"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"Note that if a method does not have adaptivity, the following rules apply:","category":"page"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"If dt is set, then the algorithm will step with size dt each iteration.\nIf tstops and dt are both set, then the algorithm will step with either a size dt, or use a smaller step to hit the tstops point.\nIf tstops is set without dt, then the algorithm will step directly to each value in tstops\nIf neither dt nor tstops are set, the solver will throw an error.","category":"page"},{"location":"basics/common_solver_opts/#advanced_adaptive_stepsize_control","page":"Common Solver Options","title":"Advanced Adaptive Stepsize Control","text":"","category":"section"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"CurrentModule = OrdinaryDiffEq","category":"page"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"These arguments control more advanced parts of the internals of adaptive timestepping and are mostly used to make it more efficient on specific problems. For detained explanations of the timestepping algorithms, see the timestepping descriptions","category":"page"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"internalnorm: The norm function internalnorm(u,t) which error estimates are calculated. Required are two dispatches: one dispatch for the state variable and the other on the elements of the state variable (scalar norm). Defaults are package-dependent.\ncontroller: Possible examples are IController, PIController, PIDController, PredictiveController. Default is algorithm-dependent.\ngamma: The risk-factor γ in the q equation for adaptive timestepping of the controllers using it. Default is algorithm-dependent.\nbeta1: The Lund stabilization α parameter. Default is algorithm-dependent.\nbeta2: The Lund stabilization β parameter. Default is algorithm-dependent.\nqmax: Defines the maximum value possible for the adaptive q. Default is algorithm-dependent.\nqmin: Defines the minimum value possible for the adaptive q. Default is algorithm-dependent.\nqsteady_min: Defines the minimum for the range around 1 where the timestep is held constant. Default is algorithm-dependent.\nqsteady_max: Defines the maximum for the range around 1 where the timestep is held constant. Default is algorithm-dependent.\nqoldinit: The initial qold in stabilization stepping. Default is algorithm-dependent.\nfailfactor: The amount to decrease the timestep by if the Newton iterations of an implicit method fail. Default is 2.","category":"page"},{"location":"basics/common_solver_opts/#Memory-Optimizations","page":"Common Solver Options","title":"Memory Optimizations","text":"","category":"section"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"calck: Turns on and off the internal ability for intermediate interpolations (also known as intermediate density). Not the same as dense, which is post-solution interpolation. This defaults to dense || !isempty(saveat) ||  \"no custom callback is given\". This can be used to turn off interpolations (to save memory) if one isn't using interpolations when a custom callback is used. Another case where this may be used is to turn on interpolations for usage in the integrator interface even when interpolations are used nowhere else. Note that this is only required if the algorithm doesn't have a free or lazy interpolation (DP8()). If calck = false, saveat cannot be used. The rare keyword calck can be useful in event handling.\nalias_u0: allows the solver to alias the initial condition array that is contained in the problem struct. Defaults to false.","category":"page"},{"location":"basics/common_solver_opts/#Miscellaneous","page":"Common Solver Options","title":"Miscellaneous","text":"","category":"section"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"maxiters: Maximum number of iterations before stopping. Defaults to 1e5.\ncallback: Specifies a callback. Defaults to a callback function which performs the saving routine. For more information, see the Event Handling and Callback Functions manual page.\nisoutofdomain: Specifies a function isoutofdomain(u,p,t) where, when it returns true, it will reject the timestep. Disabled by default.\nunstable_check: Specifies a function unstable_check(dt,u,p,t) where, when it returns true, it will cause the solver to exit and throw a warning. Defaults to any(isnan,u), i.e. checking if any value is a NaN.\nverbose: Toggles whether warnings are thrown when the solver exits early. Defaults to true.\nmerge_callbacks: Toggles whether to merge prob.callback with the solve keyword argument callback. Defaults to true.","category":"page"},{"location":"basics/common_solver_opts/#Progress-Monitoring","page":"Common Solver Options","title":"Progress Monitoring","text":"","category":"section"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"These arguments control the usage of the progressbar in the Juno IDE.","category":"page"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"progress: Turns on/off the Juno progressbar. Default is false.\nprogress_steps: Numbers of steps between updates of the progress bar. Default is 1000.\nprogress_name: Controls the name of the progressbar. Default is the name of the problem type.\nprogress_message: Controls the message with the progressbar. Defaults to showing dt, t, the maximum of u.","category":"page"},{"location":"basics/common_solver_opts/#Error-Calculations","page":"Common Solver Options","title":"Error Calculations","text":"","category":"section"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"If you are using the test problems (ex: ODETestProblem), then the following options control the errors which are calculated:","category":"page"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"timeseries_errors: Turns on and off the calculation of errors at the steps which were taken, such as the l2 error. Default is true.\ndense_errors: Turns on and off the calculation of errors at the steps which require dense output and calculate the error at 100 evenly-spaced points throughout tspan. An example is the L2 error. Default is false.","category":"page"},{"location":"basics/common_solver_opts/#Examples","page":"Common Solver Options","title":"Examples","text":"","category":"section"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"The following lines are examples of how one could use the configuration of solve(). For these examples a 3-dimensional ODE problem is assumed, however the extention to other types is straightforward.","category":"page"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"solve(prob, AlgorithmName()) : The \"default\" setting, with a user-specified","category":"page"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"algorithm (given by AlgorithmName()). All parameters get their default values.   This means that the solution is saved at the steps the Algorithm stops internally   and dense output is enabled if the chosen algorithm allows for it.","category":"page"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"All other integration parameters (e.g. stepsize) are chosen automatically.","category":"page"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"solve(prob, saveat = 0.01, abstol = 1e-9, reltol = 1e-9) : Standard setting","category":"page"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"for accurate output at specified (and equidistant) time intervals, used for   e.g. Fourier Transform. The solution is given every 0.01 time units,   starting from tspan[1]. The solver used is Tsit5() since no keyword   alg_hits is given.","category":"page"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"solve(prob, maxiters = 1e7, progress = true, save_idxs = [1]) : Using longer","category":"page"},{"location":"basics/common_solver_opts/","page":"Common Solver Options","title":"Common Solver Options","text":"maximum number of solver iterations can be useful when a given tspan is very   long. This example only saves the first of the variables of the system, either   to save size or because the user does not care about the others. Finally, with   progress = true you are enabling the progress bar, provided you are using   the Atom+Juno IDE set-up for your Julia.","category":"page"},{"location":"basics/plot/#plot","page":"Plot Functions","title":"Plot Functions","text":"","category":"section"},{"location":"basics/plot/#Standard-Plots-Using-the-Plot-Recipe","page":"Plot Functions","title":"Standard Plots Using the Plot Recipe","text":"","category":"section"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"Plotting functionality is provided by recipes to Plots.jl. To plot solutions, simply call the plot(type) after importing Plots.jl and the plotter will generate appropriate plots.","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"#]add Plots # You need to install Plots.jl before your first time using it!\nusing Plots\nplot(sol) # Plots the solution","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"Many of the types defined in the DiffEq universe, such as ODESolution, ConvergenceSimulation WorkPrecision, etc. have plot recipes to handle the default plotting behavior. Plots can be customized using all of the keyword arguments provided by Plots.jl. For example, we can change the plotting backend to the GR package and put a title on the plot by doing:","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"gr()\nplot(sol,title=\"I Love DiffEqs!\")","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"Then to save the plot, use savefig, for example:","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"savefig(\"myplot.png\")","category":"page"},{"location":"basics/plot/#Density","page":"Plot Functions","title":"Density","text":"","category":"section"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"If the problem was solved with dense=true, then denseplot controls whether to use the dense function for generating the plot, and plotdensity is the number of evenly-spaced points (in time) to plot. For example:","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"plot(sol,denseplot=false)","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"means \"only plot the points which the solver stepped to\", while:","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"plot(sol,plotdensity=1000)","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"means to plot 1000 points using the dense function (since denseplot=true by default).","category":"page"},{"location":"basics/plot/#plot_vars","page":"Plot Functions","title":"Choosing Variables","text":"","category":"section"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"In the plot command, one can choose the variables to be plotted in each plot. The master form is:","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"vars = [(f1,0,1), (f2,1,3), (f3,4,5)]","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"which could be used to plot f1(var₀, var₁), f2(var₁, var₃), and f3(var₄, var₅), all on the same graph. (0 is considered to be time, or the independent variable). Functions f1, f2 and f3 should take in scalars and return a tuple. If no function is given, for example,","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"vars = [(0,1), (1,3), (4,5)]","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"this would mean \"plot var₁(t) vs t (time), var₃(var₁) vs var₁, and var₅(var₄) vs var₄ all on the same graph, putting the independent variables (t, var₁ and var₄) on the x-axis.\" While this can be used for everything, the following conveniences are provided:","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"Everywhere in a tuple position where we only find an integer, this variable is plotted as a function of time.  For example, the list above is equivalent to:","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"vars = [1, (1,3), (4,5)]","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"and","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"vars = [1, 3, 4]","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"is the most concise way to plot the variables 1, 3, and 4 as a function of time.","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"It is possible to omit the list if only one plot is wanted: (2,3) and 4 are respectively equivalent to [(2,3)] and [(0,4)].\nA tuple containing one or several lists will be expanded by associating corresponding elements of the lists with each other:","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"vars = ([1,2,3], [4,5,6])","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"is equivalent to","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"vars = [(1,4), (2,5), (3,6)]","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"and","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"vars = (1, [2,3,4])","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"is equivalent to","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"vars = [(1,2), (1,3), (1,4)]","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"Instead of using integers, one can use the symbols from a ParameterizedFunction. For example, vars=(:x,:y) will replace the symbols with the integer values for components :x and :y.\nn-dimensional groupings are allowed. For example, (1,2,3,4,5) would be a 5-dimensional plot between the associated variables.","category":"page"},{"location":"basics/plot/#Complex-Numbers-and-High-Dimensional-Plots","page":"Plot Functions","title":"Complex Numbers and High Dimensional Plots","text":"","category":"section"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"The recipe library DimensionalPlotRecipes.jl is provided for extra functionality on high dimensional numbers (complex numbers) and other high dimensional plots. See the README for more details on the extra controls that exist.","category":"page"},{"location":"basics/plot/#Timespan","page":"Plot Functions","title":"Timespan","text":"","category":"section"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"A plotting timespan can be chosen by the tspan argument in plot. For example:","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"plot(sol,tspan=(0.0,40.0))","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"only plots between t=0.0 and t=40.0. If denseplot=true these bounds will be respected exactly. Otherwise the first point inside and last point inside the interval will be plotted, i.e. no points outside the interval will be plotted.","category":"page"},{"location":"basics/plot/#Example","page":"Plot Functions","title":"Example","text":"","category":"section"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"using DifferentialEquations, Plots\nfunction lorenz(du,u,p,t)\n du[1] = p[1]*(u[2]-u[1])\n du[2] = u[1]*(p[2]-u[3]) - u[2]\n du[3] = u[1]*u[2] - p[3]*u[3]\nend\n\nu0 = [1., 5., 10.]\ntspan = (0., 100.)\np = (10.0,28.0,8/3)\nprob = ODEProblem(lorenz, u0, tspan,p)\nsol = solve(prob)\nxyzt = plot(sol, plotdensity=10000,lw=1.5)\nxy = plot(sol, plotdensity=10000, vars=(1,2))\nxz = plot(sol, plotdensity=10000, vars=(1,3))\nyz = plot(sol, plotdensity=10000, vars=(2,3))\nxyz = plot(sol, plotdensity=10000, vars=(1,2,3))\nplot(plot(xyzt,xyz),plot(xy, xz, yz, layout=(1,3),w=1), layout=(2,1))","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"(Image: lorenz_plot)","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"An example using the functions:","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"f(x,y,z) = (sqrt(x^2+y^2+z^2),x)\nplot(sol,vars=(f,1,2,3))","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"(Image: norm_plot)","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"or the norm over time:","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"f(t,x,y,z) = (t,sqrt(x^2+y^2+z^2))\nplot(sol,vars=(f,0,1,2,3))","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"(Image: normtime plot)","category":"page"},{"location":"basics/plot/#Animations","page":"Plot Functions","title":"Animations","text":"","category":"section"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"Using the iterator interface over the solutions, animations can also be generated via the animate(sol) command. One can choose the filename to save to via animate(sol,filename), while the frames per second fps and the density of steps to show every can be specified via keyword arguments. The rest of the arguments will be directly passed to the plot recipe to be handled as normal. For example, we can animate our solution with a larger line-width which saves every 4th frame via:","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"#]add ImageMagick # You may need to install ImageMagick.jl before your first time using it!\n#using ImageMagick # Some installations require using ImageMagick for good animations\nanimate(sol,lw=3,every=4)","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"Please see Plots.jl's documentation for more information on the available attributes.","category":"page"},{"location":"basics/plot/#Plotting-Without-the-Plot-Recipe","page":"Plot Functions","title":"Plotting Without the Plot Recipe","text":"","category":"section"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"What if you don't want to use Plots.jl? Odd choice, but that's okay! If the differential equation was described by a vector of values, then the solution object acts as an AbstractMatrix sol[i,j] for the ith variable at timepoint j. You can use this to plot solutions. For example, in PyPlot, Gadfly, GR, etc., you can do the following to plot the timeseries:","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"plot(sol.t,sol')","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"since these plot along the columns, and sol' has the timeseries along the column. Phase plots can be done similarly, for example:","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"plot(sol[i,:],sol[j,:],sol[k,:])","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"is a 3d phase plot between variables i, j, and k.","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"Notice that this does not use the interpolation. When not using the plot recipe, the interpolation must be done manually. For example:","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"n = 101 #number of timepoints\nts = range(0, stop=1, length=n)\nplot(sol(ts,idxs=i),sol(ts,idxs=j),sol(ts,idxs=k))","category":"page"},{"location":"basics/plot/","page":"Plot Functions","title":"Plot Functions","text":"is the phase space using values 0.01 apart in time.","category":"page"},{"location":"#DifferentialEquations.jl:-Scientific-Machine-Learning-(SciML)-Enabled-Simulation-and-Estimation","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"","category":"section"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"This is a suite for numerically solving differential equations written in Julia and available for use in Julia, Python, and R. The purpose of this package is to supply efficient Julia implementations of solvers for various differential equations. Equations within the realm of this package include:","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Discrete equations (function maps, discrete stochastic (Gillespie/Markov) simulations)\nOrdinary differential equations (ODEs)\nSplit and Partitioned ODEs (Symplectic integrators, IMEX Methods)\nStochastic ordinary differential equations (SODEs or SDEs)\nStochastic differential-algebraic equations (SDAEs)\nRandom differential equations (RODEs or RDEs)\nDifferential algebraic equations (DAEs)\nDelay differential equations (DDEs)\nNeutral, retarded, and algebraic delay differential equations (NDDEs, RDDEs, and DDAEs)\nStochastic delay differential equations (SDDEs)\nExperimental support for stochastic neutral, retarded, and algebraic delay differential equations (SNDDEs, SRDDEs, and SDDAEs)\nMixed discrete and continuous equations (Hybrid Equations, Jump Diffusions)\n(Stochastic) partial differential equations ((S)PDEs) (with both finite difference and finite element methods)","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"The well-optimized DifferentialEquations solvers benchmark as the some of the fastest implementations, using classic algorithms and ones from recent research which routinely outperform the \"standard\" C/Fortran methods, and include algorithms optimized for high-precision and HPC applications. At the same time, it wraps the classic C/Fortran methods, making it easy to switch over to them whenever necessary. Solving differential equations with different methods from different languages and packages can be done by changing one line of code, allowing for easy benchmarking to ensure you are using the fastest method possible.","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"DifferentialEquations.jl integrates with the Julia package sphere with:","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"GPU accleration through CUDA.jl and DiffEqGPU.jl\nAutomated sparsity detection with SparsityDetection.jl\nAutomatic Jacobian coloring with SparseDiffTools.jl, allowing for fast solutions to problems with sparse or structured (Tridiagonal, Banded, BlockBanded, etc.) Jacobians\nAllowing the specification of linear solvers for maximal efficiency\nProgress meter integration with the Juno IDE for estimated time to solution\nAutomatic plotting of time series and phase plots\nBuilt-in interpolations\nWraps for common C/Fortran methods like Sundials and Hairer's radau\nArbitrary precision with BigFloats and Arbfloats\nArbitrary array types, allowing the definition of differential equations on matrices and distributed arrays\nUnit checked arithmetic with Unitful","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Additionally, DifferentialEquations.jl comes with built-in analysis features, including:","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Forward and Adjoint Sensitivity Analysis (Automatic Differentiation) for fast gradient computations\nParameter Estimation and Bayesian Analysis\nNeural differential equations with DiffEqFlux.jl for efficient scientific machine learning (scientific ML) and scientific AI.\nAutomatic distributed, multithreaded, and GPU Parallel Ensemble Simulations\nGlobal Sensitivity Analysis\nUncertainty Quantification","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"If you have any questions, or just want to chat about solvers/using the package, please feel free to use the Gitter channel. For bug reports, feature requests, etc., please submit an issue. If you're interested in contributing, please see the Developer Documentation.","category":"page"},{"location":"#Supporting-and-Citing","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Supporting and Citing","text":"","category":"section"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"The software in this ecosystem was developed as part of academic research. If you would like to help support it, please star the repository as such metrics may help us secure funding in the future. If you use SciML software as part of your research, teaching, or other activities, we would be grateful if you could cite our work.","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"@article{rackauckas2017differentialequations,\n  title={Differentialequations.jl--a performant and feature-rich ecosystem for solving differential equations in julia},\n  author={Rackauckas, Christopher and Nie, Qing},\n  journal={Journal of Open Research Software},\n  volume={5},\n  number={1},\n  year={2017},\n  publisher={Ubiquity Press}\n}","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"is necessary for any use of DifferentialEquations.jl or the packages that are maintained as part of its suite (OrdinaryDiffEq.jl, Sundials.jl, DiffEqDevTools.jl, etc.). Additionally, many of the solvers utilize novel algorithms, and if these algorithms  are used we asked that you cite the methods. Please see our citation page for guidelines.","category":"page"},{"location":"#Getting-Started:-Installation-And-First-Steps","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Getting Started: Installation And First Steps","text":"","category":"section"},{"location":"#Installing-from-Julia","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Installing from Julia","text":"","category":"section"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"To install the package, use the following command inside the Julia REPL:","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"using Pkg\nPkg.add(\"DifferentialEquations\")","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"To load the package, use the command:","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"using DifferentialEquations","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"This will add solvers and dependencies for all kinds of Differential Equations (e.g. ODEs or SDEs etc., see the Supported Equations section below). If you are interested in only one type of equation solvers of DifferentialEquations.jl or simply want a more lightweight version, see the Low Dependency Usage page.","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"To understand the package in more detail, check out the following tutorials in this manual. It is highly recommended that new users start with the ODE tutorial. Example IJulia notebooks can also be found in DiffEqTutorials.jl. If you find any example where there seems to be an error, please open an issue.","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"For the most up to date information on using the package, please join the Gitter channel.","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Using the bleeding edge for the latest features and development is only recommended for power users. Information on how to get to the bleeding edge is found in the developer documentation.","category":"page"},{"location":"#Installing-from-Python","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Installing from Python","text":"","category":"section"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Use of DifferentialEquations.jl from the Python programming language is available through the diffeqpy module. To install diffeqpy, use pip:","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"pip install diffeqpy","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Using diffeqpy requires that Julia is installed and in the path, along with DifferentialEquations.jl and PyCall.jl. To install Julia, download a generic binary from the JuliaLang site and add it to your path. To install Julia packages required for diffeqpy, open up Python interpreter then run:","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":">>> import diffeqpy\n>>> diffeqpy.install()","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"and you're good! In addition, to improve the performance of your code it is recommended that you use Numba to JIT compile your derivative functions. To install Numba, use:","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"pip install numba","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"diffeqpy supports the majority of DifferentialEquations.jl with very similar syntax, see the diffeqpy README for more details. One important point to note is that Numba is generally an order of magnitude slower than Julia in terms of  the generated differential equation solver code, and thus it is recommended to use julia.Main.eval for Julia-side derivative function implementations for maximal efficiency. See this blog post for more information.","category":"page"},{"location":"#Installing-from-R","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Installing from R","text":"","category":"section"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Use of DifferentialEquations.jl from the R programming language is available through the diffeqr module. diffeqr is registered into CRAN. Thus to add the package, use:","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"install.packages(\"diffeqr\")","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"To install the master branch of the package (for developers), use:","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"devtools::install_github('SciML/diffeqr', build_vignettes=T)","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"You will need a working installation of Julia in your path. To install Julia, download a generic binary from the JuliaLang site and add it to your path. The download and installation of DifferentialEquations.jl will happen on the first invocation of diffeqr::diffeq_setup().","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Currently, use from R supported a subset of DifferentialEquations.jl which is documented through CRAN.","category":"page"},{"location":"#IJulia-Notebook-Tutorials","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"IJulia Notebook Tutorials","text":"","category":"section"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"You can access extra tutorials supplied in the DiffEqTutorials.jl repository via the commands:","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"using Pkg\npkg\"add https://github.com/SciML/SciMLTutorials.jl\"\nusing SciMLTutorials\nSciMLTutorials.open_notebooks()","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Or you can view the webpages for the rendered tutorials at the links found in the repository.","category":"page"},{"location":"#Video-Tutorial","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Video Tutorial","text":"","category":"section"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"(Image: Video Tutorial)","category":"page"},{"location":"#Tutorials","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Tutorials","text":"","category":"section"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"The following tutorials will introduce you to the functionality of DifferentialEquations.jl. More examples can be found by checking out the IJulia notebooks in the examples folder.","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Pages = [\n    \"tutorials/ode_example.md\",\n    \"tutorials/sde_example.md\",\n    \"tutorials/dde_example.md\",\n    \"tutorials/dae_example.md\",\n    \"tutorials/discrete_stochastic_example.md\",\n    \"tutorials/jump_diffusion.md\",\n    \"tutorials/bvp_example.md\",\n    \"tutorials/additional.md\"\n    ]\nDepth = 2","category":"page"},{"location":"#Removing-and-Reducing-Compile-Times","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Removing and Reducing Compile Times","text":"","category":"section"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"In some situations one may wish to decrease the compile time associated with DifferenitalEquations.jl usage. If that's the case, there's two strategies to employ. One strategy is to use the low dependency usage. DifferentialEquations.jl is a metapackage composed of many smaller packages, and thus one could directly use a single component, such as OrdinaryDiffEq.jl for the pure Julia ODE solvers, and decrease the compile times by ignoring the rest (note: the interface is exactly the same, except using a solver other than those in OrdinaryDiffEq.jl will error). We recommend that downstream packages only rely on exactly the packages they need.","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"The other strategy is to use PackageCompiler.jl to create  a system image that precompiles the whole package. To do this, one simply does:","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"using PackageCompiler\nPackageCompiler.create_sysimage([:DifferentialEquations,:Plots];replace_default=true)","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Note that there are some drawbacks to adding a package in your system image, for example the package will never update until you manually rebuild the system image again. For more information on the consequences,  see this portion of the PackageCompiler manual","category":"page"},{"location":"#Basics","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Basics","text":"","category":"section"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"These pages introduce you to the core of DifferentialEquations.jl and the common interface. It explains the general workflow, options which are generally available, and the general tools for analysis.","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Pages = [\n    \"basics/overview.md\",\n    \"basics/common_solver_opts.md\",\n    \"basics/solution.md\",\n    \"basics/plot.md\",\n    \"basics/integrator.md\",\n    \"basics/problem.md\",\n    \"basics/faq.md\",\n    \"basics/compatibility_chart.md\"\n    ]\nDepth = 2","category":"page"},{"location":"#Problem-Types","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Problem Types","text":"","category":"section"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"These pages describe building the problem types to define differential equations for the solvers, and the special features of the different solution types.","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Pages = [\n  \"types/discrete_types.md\",\n  \"types/ode_types.md\",\n  \"types/dynamical_types.md\",\n  \"types/split_ode_types.md\",\n  \"types/steady_state_types.md\",\n  \"types/bvp_types.md\",\n  \"types/sde_types.md\",\n  \"types/rode_types.md\",\n  \"types/dde_types.md\",\n  \"types/dae_types.md\",\n  \"types/jump_types.md\",\n]\nDepth = 2","category":"page"},{"location":"#Solver-Algorithms","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Solver Algorithms","text":"","category":"section"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"These pages describe the solvers and available algorithms in detail.","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Pages = [\n  \"solvers/discrete_solve.md\",\n  \"solvers/ode_solve.md\",\n  \"solvers/dynamical_solve.md\",\n  \"solvers/split_ode_solve.md\",\n  \"solvers/steady_state_solve.md\",\n  \"solvers/bvp_solve.md\",\n  \"solvers/jump_solve.md\",\n  \"solvers/sde_solve.md\",\n  \"solvers/rode_solve.md\",\n  \"solvers/dde_solve.md\",\n  \"solvers/dae_solve.md\",\n  \"solvers/benchmarks.md\"\n]\nDepth = 2","category":"page"},{"location":"#Additional-Features","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Additional Features","text":"","category":"section"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"These sections discuss extra performance enhancements, event handling, and other in-depth features.","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Pages = [\n    \"features/performance_overloads.md\",\n    \"features/diffeq_arrays.md\",\n    \"features/diffeq_operator.md\",\n    \"features/noise_process.md\",\n    \"features/linear_nonlinear.md\",\n    \"features/callback_functions.md\",\n    \"features/callback_library.md\",\n    \"features/ensemble.md\",\n    \"features/io.md\",\n    \"features/low_dep.md\",\n    \"features/progress_bar.md\"\n]\nDepth = 2","category":"page"},{"location":"#Analysis-Tools","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Analysis Tools","text":"","category":"section"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Because DifferentialEquations.jl has a common interface on the solutions, it is easy to add functionality to the entire DiffEq ecosystem by developing it to the solution interface. These pages describe the add-on analysis tools which are available.","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Pages = [\n    \"analysis/parameterized_functions.md\",\n    \"analysis/parameter_estimation.md\",\n    \"analysis/bifurcation.md\",\n    \"analysis/sensitivity.md\",\n    \"analysis/global_sensitivity.md\",\n    \"analysis/uncertainty_quantification.md\",\n    \"analysis/neural_networks.md\",\n    \"analysis/dev_and_test.md\"\n]\nDepth = 2","category":"page"},{"location":"#Modeling-Tools","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Modeling Tools","text":"","category":"section"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"While DifferentialEquations.jl can be used to directly build any differential or difference equation (/ discrete stochastic) model, in many cases it can be helpful to have a tailored-built API for making certain types of common models easier. This is provided by the modeling functionality.","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Pages = [\n    \"models/multiscale.md\",\n    \"models/physical.md\",\n    \"models/financial.md\",\n    \"models/chemical_reactions.md\",\n    \"models/external_modeling.md\"\n]\nDepth = 2","category":"page"},{"location":"#Extra-Details","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Extra Details","text":"","category":"section"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"These are just assorted extra explanations for the curious.","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Pages = [\n    \"extras/timestepping.md\"\n    \"extras/sensitivity_math.md\"\n]\nDepth = 2","category":"page"},{"location":"#Acknowledgements","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Acknowledgements","text":"","category":"section"},{"location":"#Core-Contributors","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Core Contributors","text":"","category":"section"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"JuliaDiffEq and DifferentialEquations.jl has been a collaborative effort by many individuals. Significant contributions have been made by the following individuals:","category":"page"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Chris Rackauckas (@ChrisRackauckas) (lead developer)\nYingbo Ma (@YingboMa)\nDavid Widmann (@devmotion)\nHendrik Ranocha (@ranocha)\nEthan Levien (@elevien)\nTom Short (@tshort)\n@dextorious\nSamuel Isaacson (@isaacsas)","category":"page"},{"location":"#Google-Summer-of-Code-Alumni","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"Google Summer of Code Alumni","text":"","category":"section"},{"location":"","page":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","title":"DifferentialEquations.jl: Scientific Machine Learning (SciML) Enabled Simulation and Estimation","text":"Yingbo Ma (@YingboMa)\nShivin Srivastava (@shivin9)\nAyush Pandey (@Ayush-iitkgp)\nXingjian Guo (@MSeeker1340)\nShubham Maddhashiya (@sipah00)\nVaibhav Kumar Dixit (@Vaibhavdixit02)","category":"page"},{"location":"solvers/jump_solve/#jump_solve","page":"Jump Problem and Jump Diffusion Solvers","title":"Jump Problem and Jump Diffusion Solvers","text":"","category":"section"},{"location":"solvers/jump_solve/","page":"Jump Problem and Jump Diffusion Solvers","title":"Jump Problem and Jump Diffusion Solvers","text":"solve(prob::JumpProblem,alg;kwargs)","category":"page"},{"location":"solvers/jump_solve/#Recommended-Methods","page":"Jump Problem and Jump Diffusion Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"solvers/jump_solve/","page":"Jump Problem and Jump Diffusion Solvers","title":"Jump Problem and Jump Diffusion Solvers","text":"A JumpProblem(prob,aggregator,jumps...) come in two forms. The first major form is if it does not have a RegularJump. In this case, it can be solved with any integrator on  prob. However, in the case of a pure JumpProblem (a JumpProblem over a  DiscreteProblem), there are special algorithms available.  The SSAStepper() is an efficient streamlined algorithm for running the  aggregator version of the SSA for pure ConstantRateJump and/or MassActionJump problems. However, it is not compatible with event handling. If events are necessary, then FunctionMap does well.","category":"page"},{"location":"solvers/jump_solve/","page":"Jump Problem and Jump Diffusion Solvers","title":"Jump Problem and Jump Diffusion Solvers","text":"If there is a RegularJump, then specific methods must be used. The current recommended method is TauLeaping if you need adaptivity, events, etc. If you just need the most barebones fixed time step leaping method, then SimpleTauLeaping can have performance benefits.","category":"page"},{"location":"solvers/jump_solve/#Special-Methods-for-Pure-Jump-Problems","page":"Jump Problem and Jump Diffusion Solvers","title":"Special Methods for Pure Jump Problems","text":"","category":"section"},{"location":"solvers/jump_solve/","page":"Jump Problem and Jump Diffusion Solvers","title":"Jump Problem and Jump Diffusion Solvers","text":"If you are using jumps with a differential equations, use the same methods as in the case of the differential equation solving. However, the following algorithms are optimized for pure jump problems.","category":"page"},{"location":"solvers/jump_solve/#DiffEqJump.jl","page":"Jump Problem and Jump Diffusion Solvers","title":"DiffEqJump.jl","text":"","category":"section"},{"location":"solvers/jump_solve/","page":"Jump Problem and Jump Diffusion Solvers","title":"Jump Problem and Jump Diffusion Solvers","text":"SSAStepper: a stepping algorithm for pure ConstantRateJump and/or MassActionJump JumpProblems. Supports handling of DiscreteCallback and saving controls like saveat.","category":"page"},{"location":"solvers/jump_solve/#RegularJump-Compatible-Methods","page":"Jump Problem and Jump Diffusion Solvers","title":"RegularJump Compatible Methods","text":"","category":"section"},{"location":"solvers/jump_solve/#StochasticDiffEq.jl","page":"Jump Problem and Jump Diffusion Solvers","title":"StochasticDiffEq.jl","text":"","category":"section"},{"location":"solvers/jump_solve/","page":"Jump Problem and Jump Diffusion Solvers","title":"Jump Problem and Jump Diffusion Solvers","text":"These methods support mixing with event handling, other jump types, and all of the features of the normal differential equation solvers.","category":"page"},{"location":"solvers/jump_solve/","page":"Jump Problem and Jump Diffusion Solvers","title":"Jump Problem and Jump Diffusion Solvers","text":"TauLeaping: an adaptive tau-leaping algorithm with post-leap estimates.","category":"page"},{"location":"solvers/jump_solve/#DiffEqJump.jl-2","page":"Jump Problem and Jump Diffusion Solvers","title":"DiffEqJump.jl","text":"","category":"section"},{"location":"solvers/jump_solve/","page":"Jump Problem and Jump Diffusion Solvers","title":"Jump Problem and Jump Diffusion Solvers","text":"SimpleTauLeaping: a tau-leaping algorithm for pure RegularJump JumpProblems. Requires a choice of dt.\nRegularSSA: a version of SSA for pure RegularJump JumpProblems.","category":"page"},{"location":"solvers/jump_solve/#Regular-Jump-Diffusion-Compatible-Methods","page":"Jump Problem and Jump Diffusion Solvers","title":"Regular Jump Diffusion Compatible Methods","text":"","category":"section"},{"location":"solvers/jump_solve/","page":"Jump Problem and Jump Diffusion Solvers","title":"Jump Problem and Jump Diffusion Solvers","text":"Regular jump diffusions are JumpProblems where the internal problem is an SDEProblem and the jump process has designed a regular jump.","category":"page"},{"location":"solvers/jump_solve/#StochasticDiffEq.jl-2","page":"Jump Problem and Jump Diffusion Solvers","title":"StochasticDiffEq.jl","text":"","category":"section"},{"location":"solvers/jump_solve/","page":"Jump Problem and Jump Diffusion Solvers","title":"Jump Problem and Jump Diffusion Solvers","text":"EM: Explicit Euler-Maruyama.\nImplicitEM: Implicit Euler-Maruyama. See the SDE solvers page for more details.","category":"page"},{"location":"models/financial/#financial_models","page":"Financial Models","title":"Financial Models","text":"","category":"section"},{"location":"models/financial/","page":"Financial Models","title":"Financial Models","text":"The financial models functionality is provided by DiffEqFinancial.jl and helps the user build and solve the differential equation based financial models.","category":"page"},{"location":"models/financial/#SDE-Model-Library","page":"Financial Models","title":"SDE Model Library","text":"","category":"section"},{"location":"models/financial/","page":"Financial Models","title":"Financial Models","text":"The following constructors create SDEProblem types which can be solved using the stochastic differential equation solvers.","category":"page"},{"location":"models/financial/#HestonProblem","page":"Financial Models","title":"HestonProblem","text":"","category":"section"},{"location":"models/financial/","page":"Financial Models","title":"Financial Models","text":"dS = μSdt + sqrtvSdW_1 \ndv = κ(Θ-v)dt + σsqrtvdW_2 \ndW_1 dW_2 = ρ dt","category":"page"},{"location":"models/financial/","page":"Financial Models","title":"Financial Models","text":"Constructor:","category":"page"},{"location":"models/financial/","page":"Financial Models","title":"Financial Models","text":"HestonProblem(μ,κ,Θ,σ,ρ,u0,tspan)","category":"page"},{"location":"models/financial/#GeneralizedBlackScholesProblem","page":"Financial Models","title":"GeneralizedBlackScholesProblem","text":"","category":"section"},{"location":"models/financial/","page":"Financial Models","title":"Financial Models","text":"d ln S(t) = (r(t) - q(t) - fracΘ(tS)^22)dt + σ dW_t","category":"page"},{"location":"models/financial/","page":"Financial Models","title":"Financial Models","text":"Solves for log S(t). Constructor:","category":"page"},{"location":"models/financial/","page":"Financial Models","title":"Financial Models","text":"GeneralizedBlackScholesProblem(r,q,Θ,σ,u0,tspan)","category":"page"},{"location":"models/financial/#BlackScholesProblem","page":"Financial Models","title":"BlackScholesProblem","text":"","category":"section"},{"location":"models/financial/","page":"Financial Models","title":"Financial Models","text":"d ln S(t) = (r(t) - fracΘ(tS)^22)dt + σ dW_t","category":"page"},{"location":"models/financial/","page":"Financial Models","title":"Financial Models","text":"Solves for log S(t). Constructor:","category":"page"},{"location":"models/financial/","page":"Financial Models","title":"Financial Models","text":"BlackScholesProblem(r,Θ,σ,u0,tspan)","category":"page"},{"location":"models/financial/#ExtendedOrnsteinUhlenbeckProblem","page":"Financial Models","title":"ExtendedOrnsteinUhlenbeckProblem","text":"","category":"section"},{"location":"models/financial/","page":"Financial Models","title":"Financial Models","text":"dx = a(b(t)-x)dt + σ dW_t","category":"page"},{"location":"models/financial/","page":"Financial Models","title":"Financial Models","text":"Constructor:","category":"page"},{"location":"models/financial/","page":"Financial Models","title":"Financial Models","text":"ExtendedOrnsteinUhlenbeckProblem(a,b,σ,u0,tspan)","category":"page"},{"location":"models/financial/#OrnsteinUhlenbeckProblem","page":"Financial Models","title":"OrnsteinUhlenbeckProblem","text":"","category":"section"},{"location":"models/financial/","page":"Financial Models","title":"Financial Models","text":"dx = a(r-x)dt + σ dW_t","category":"page"},{"location":"models/financial/","page":"Financial Models","title":"Financial Models","text":"Constructor:","category":"page"},{"location":"models/financial/","page":"Financial Models","title":"Financial Models","text":"OrnsteinUhlenbeckProblem(a,r,σ,u0,tspan)","category":"page"},{"location":"types/dde_types/#dde_prob","page":"DDE Problems","title":"DDE Problems","text":"","category":"section"},{"location":"types/dde_types/#Mathematical-Specification-of-a-DDE-Problem","page":"DDE Problems","title":"Mathematical Specification of a DDE Problem","text":"","category":"section"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"To define a DDE Problem, you simply need to give the function f, the initial condition u_0 at time point t_0, and the history function h which together define a DDE:","category":"page"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"fracdudt = f(uhpt) qquad (t geq t_0)","category":"page"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"u(t_0) = u_0","category":"page"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"u(t) = h(t) qquad (t  t_0)","category":"page"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"f should be specified as f(u, h, p, t) (or in-place as f(du, u, h, p, t)), u_0 should be an AbstractArray (or number) whose geometry matches the desired geometry of u, and h should be specified as described below. The history function h is accessed for all delayed values. Note that we are not limited to numbers or vectors for u_0; one is allowed to provide u_0 as arbitrary matrices / higher dimension tensors as well.","category":"page"},{"location":"types/dde_types/#Functional-Forms-of-the-History-Function","page":"DDE Problems","title":"Functional Forms of the History Function","text":"","category":"section"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"The history function h can be called in the following ways:","category":"page"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"h(p, t): out-of-place calculation\nh(out, p, t): in-place calculation\nh(p, t, deriv::Type{Val{i}}): out-of-place calculation of the ith derivative\nh(out, p, t, deriv::Type{Val{i}}): in-place calculation of the ith derivative\nh(args...; idxs): calculation of h(args...) for indices idxs","category":"page"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"Note that a dispatch for the supplied history function of matching form is required for whichever function forms are used in the user derivative function f.","category":"page"},{"location":"types/dde_types/#Declaring-Lags","page":"DDE Problems","title":"Declaring Lags","text":"","category":"section"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"Lags are declared separately from their use. One can use any lag by simply using the interpolant of h at that point. However, one should use caution in order to achieve the best accuracy. When lags are declared, the solvers can more efficiently be more accurate and thus this is recommended.","category":"page"},{"location":"types/dde_types/#Neutral-and-Retarded-Delay-Differential-Equations","page":"DDE Problems","title":"Neutral and Retarded Delay Differential Equations","text":"","category":"section"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"Note that the history function specification can be used to specify general retarded arguments, i.e. h(p,α(u,t)). Neutral delay differential equations can be specified by using the deriv value in the history interpolation. For example, h(p,t-τ, Val{1}) returns the first derivative of the history values at time t-τ.","category":"page"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"Note that algebraic equations can be specified by using a singular mass matrix.","category":"page"},{"location":"types/dde_types/#Problem-Type","page":"DDE Problems","title":"Problem Type","text":"","category":"section"},{"location":"types/dde_types/#Constructors","page":"DDE Problems","title":"Constructors","text":"","category":"section"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"DDEProblem(f[, u0], h, tspan[, p]; <keyword arguments>)\nDDEProblem{isinplace}(f[, u0], h, tspan[, p]; <keyword arguments>)","category":"page"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"Parameter isinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred.","category":"page"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"Parameters are optional, and if not given then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.","category":"page"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"For specifying Jacobians and mass matrices, see the DiffEqFunctions page.","category":"page"},{"location":"types/dde_types/#Arguments","page":"DDE Problems","title":"Arguments","text":"","category":"section"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"f: The function in the DDE.\nu0: The initial condition. Defaults to the value h(p, first(tspan)) of the history function evaluated at the initial time point.\nh: The history function for the DDE before t0.\ntspan: The timespan for the problem.\np: The parameters with which function f is called. Defaults to NullParameters.\nconstant_lags: A collection of constant lags used by the history function h. Defaults to ().\ndependent_lags A tuple of functions (u, p, t) -> lag for the state-dependent lags used by the history function h. Defaults to ().\nneutral: If the DDE is neutral, i.e., if delays appear in derivative terms.\norder_discontinuity_t0: The order of the discontinuity at the initial time point. Defaults to 0 if an initial condition u0 is provided. Otherwise it is forced to be greater or equal than 1.\nkwargs: The keyword arguments passed onto the solves.","category":"page"},{"location":"types/dde_types/#Dynamical-Delay-Differential-Equations","page":"DDE Problems","title":"Dynamical Delay Differential Equations","text":"","category":"section"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"Much like Dynamical ODEs, a Dynamical DDE is a Partitioned DDE of the form:","category":"page"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"fracdvdt = f_1(uth) \nfracdudt = f_2(vh) ","category":"page"},{"location":"types/dde_types/#Constructors-2","page":"DDE Problems","title":"Constructors","text":"","category":"section"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"DynamicalDDEProblem(f1, f2[, v0, u0], h, tspan[, p]; <keyword arguments>)\nDynamicalDDEProblem{isinplace}(f1, f2[, v0, u0], h, tspan[, p]; <keyword arguments>)","category":"page"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"Parameter isinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred.","category":"page"},{"location":"types/dde_types/#Arguments-2","page":"DDE Problems","title":"Arguments","text":"","category":"section"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"f: The function in the DDE.\nv0 and u0: The initial condition. Defaults to the values h(p, first(tspan))... of the history function evaluated at the initial time point.\nh: The history function for the DDE before t0. Must return an object with the indices 1 and 2, with the values of v and u respectively.\ntspan: The timespan for the problem.\np: The parameters with which function f is called. Defaults to NullParameters.\nconstant_lags: A collection of constant lags used by the history function h. Defaults to ().\ndependent_lags A tuple of functions (v, u, p, t) -> lag for the state-dependent lags used by the history function h. Defaults to ().\nneutral: If the DDE is neutral, i.e., if delays appear in derivative terms.\norder_discontinuity_t0: The order of the discontinuity at the initial time point. Defaults to 0 if an initial condition u0 is provided. Otherwise it is forced to be greater or equal than 1.\nkwargs: The keyword arguments passed onto the solves.","category":"page"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"The for dynamical and second order DDEs, the history function will return an object with the indicies 1 and 2 defined, where h(p, t_prev)[1] is the value of f_2(v u h p t_mathrmprev) and h(p, t_prev)[2] is the value of f_1(v u h p t_mathrmprev) (this is for consistency with the ordering of the intitial conditions in the constructor). The supplied history function must also return such a 2-index object, which can be accomplished with a tuple (v,u) or vector [v,u].","category":"page"},{"location":"types/dde_types/#nd-Order-Delay-Differential-Equations","page":"DDE Problems","title":"2nd Order Delay Differential Equations","text":"","category":"section"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"To define a 2nd Order DDE Problem, you simply need to give the function f and the initial condition u_0 which define an DDE:","category":"page"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"u = f(uuhpt)","category":"page"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"f should be specified as f(du,u,p,t) (or in-place as f(ddu,du,u,p,t)), and u₀ should be an AbstractArray (or number) whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher dimension tensors as well.","category":"page"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"From this form, a dynamical ODE:","category":"page"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"v = f(vuhpt) \nu = v ","category":"page"},{"location":"types/dde_types/#Constructors-3","page":"DDE Problems","title":"Constructors","text":"","category":"section"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"SecondOrderDDEProblem(f, [, du0, u0], h, tspan[, p]; <keyword arguments>)\nSecondOrderDDEProblem{isinplace}(f, [, du0, u0], h, tspan[, p]; <keyword arguments>)","category":"page"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"Parameter isinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred.","category":"page"},{"location":"types/dde_types/#Arguments-3","page":"DDE Problems","title":"Arguments","text":"","category":"section"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"f: The function in the DDE.\ndu0 and u0: The initial condition. Defaults to the values h(p, first(tspan))... of the history function evaluated at the initial time point.\nh: The history function for the DDE before t0. Must return an object with the indices 1 and 2, with the values of v and u respectively.\ntspan: The timespan for the problem.\np: The parameters with which function f is called. Defaults to NullParameters.\nconstant_lags: A collection of constant lags used by the history function h. Defaults to ().\ndependent_lags A tuple of functions (v, u, p, t) -> lag for the state-dependent lags used by the history function h. Defaults to ().\nneutral: If the DDE is neutral, i.e., if delays appear in derivative terms.\norder_discontinuity_t0: The order of the discontinuity at the initial time point. Defaults to 0 if an initial condition u0 is provided. Otherwise it is forced to be greater or equal than 1.\nkwargs: The keyword arguments passed onto the solves.","category":"page"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"As above, the history function will return an object with indices 1 and 2, with the values of du and u respectively. The supplied history function must also match this return type, e.g. by returning a 2-element tuple or vector.","category":"page"},{"location":"types/dde_types/#Example-Problems","page":"DDE Problems","title":"Example Problems","text":"","category":"section"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"Example problems can be found in DiffEqProblemLibrary.jl.","category":"page"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"To use a sample problem, such as prob_ode_linear, you can do something like:","category":"page"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"#] add DiffEqProblemLibrary\nusing DiffEqProblemLibrary.ODEProblemLibrary\n# load problems\nODEProblemLibrary.importodeproblems()\nprob = ODEProblemLibrary.prob_ode_linear\nsol = solve(prob)","category":"page"},{"location":"types/dde_types/#DDEs-with-1-constant-delay","page":"DDE Problems","title":"DDEs with 1 constant delay","text":"","category":"section"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"CurrentModule = DDEProblemLibrary","category":"page"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"prob_dde_constant_1delay_ip\nprob_dde_constant_1delay_oop\nprob_dde_constant_1delay_scalar\nprob_dde_constant_1delay_long_ip\nprob_dde_constant_1delay_long_oop\nprob_dde_constant_1delay_long_scalar","category":"page"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_1delay_ip","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_1delay_ip","text":"prob_dde_constant_1delay_ip\n\nDelay differential equation\n\nu(t) = -u(t - 1)\n\nfor t in 0 1 with history function phi(t) = 0 if t  0 and phi(0) = 1.\n\nSolution\n\nThe analytical solution for t in 0 10 can be obtained by the method of steps and is provided in this implementation.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_1delay_oop","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_1delay_oop","text":"prob_dde_constant_1delay_oop\n\nSame delay differential equation as prob_dde_constant_1delay_ip, but purposefully implemented with an out-of-place function.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_1delay_scalar","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_1delay_scalar","text":"prob_dde_constant_1delay_scalar\n\nSame delay differential equation as prob_dde_constant_1delay_ip, but purposefully implemented with a scalar function.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_1delay_long_ip","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_1delay_long_ip","text":"prob_dde_constant_1delay_long_ip\n\nDelay differential equation\n\nu(t) = u(t) - u(t - 15)\n\nfor t in 0 100 with history function phi(t) = 0 if t  0 and phi(0) = 1.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_1delay_long_oop","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_1delay_long_oop","text":"prob_dde_constant_1delay_long_oop\n\nSame delay differential equation as prob_dde_constant_1delay_long_ip, but purposefully implemented with an out-of-place function.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_1delay_long_scalar","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_1delay_long_scalar","text":"prob_dde_constant_1delay_long_scalar\n\nSame delay differential equation as prob_dde_constant_1delay_long_ip, but purposefully implemented with a scalar function.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDEs-with-2-constant-delays","page":"DDE Problems","title":"DDEs with 2 constant delays","text":"","category":"section"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"prob_dde_constant_2delays_ip\nprob_dde_constant_2delays_oop\nprob_dde_constant_2delays_scalar\nprob_dde_constant_2delays_long_ip\nprob_dde_constant_2delays_long_oop\nprob_dde_constant_2delays_long_scalar","category":"page"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_2delays_ip","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_2delays_ip","text":"prob_dde_constant_2delays_ip\n\nDelay differential equation\n\nu(t) = -u(t - 13) - u(t - 15)\n\nfor t in 0 1 with history function phi(t) = 0 if t  0 and phi(0) = 1.\n\nSolution\n\nThe analytical solution for t in 0 10 can be obtained by the method of steps and is provided in this implementation.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_2delays_oop","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_2delays_oop","text":"prob_dde_constant_2delays_oop\n\nSame delay differential equation as prob_dde_constant_2delays_ip, but purposefully implemented with an out-of-place function.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_2delays_scalar","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_2delays_scalar","text":"prob_dde_constant_2delays_scalar\n\nSame delay differential equation as prob_dde_constant_2delays_ip, but purposefully implemented with a scalar function.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_2delays_long_ip","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_2delays_long_ip","text":"prob_dde_constant_2delays_long_ip\n\nDelay differential equation\n\nu(t) = - u(t - 13) - u(t - 15)\n\nfor t in 0 100 with history function phi(t) = 0 if t  0 and phi(0) = 1.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_2delays_long_oop","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_2delays_long_oop","text":"prob_dde_constant_2delays_long_oop\n\nSame delay differential equation as prob_dde_constant_2delays_long_ip, but purposefully implemented with an out-of-place function.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_2delays_long_scalar","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_constant_2delays_long_scalar","text":"prob_dde_constant_2delays_long_scalar\n\nSame delay differential equation as prob_dde_constant_2delays_long_ip, but purposefully implemented with a scalar function.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DDETest-Problems","page":"DDE Problems","title":"DDETest Problems","text":"","category":"section"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"Some details:","category":"page"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"# DDEs with time dependent delays\nprob_dde_DDETST_A1, prob_dde_DDETST_A2,\n# DDEs with vanishing time dependent delays\nprob_dde_DDETST_B1, prob_dde_DDETST_B2,\n# DDEs with state dependent delays\nprob_dde_DDETST_C1, prob_dde_DDETST_C2, prob_dde_DDETST_C3, prob_dde_DDETST_C4,\n# DDEs with vanishing state dependent delays\nprob_dde_DDETST_D1, prob_dde_DDETST_D2,\n# neutral DDEs with time dependent delays\nprob_dde_DDETST_E1, prob_dde_DDETST_E2,\n# neutral DDEs with vanishing time dependent delays\nprob_dde_DDETST_F1, prob_dde_DDETST_F2, prob_dde_DDETST_F3, prob_dde_DDETST_F4, prob_dde_DDETST_F5,\n# neutral DDEs with state dependent delays\nprob_dde_DDETST_G1, prob_dde_DDETST_G2,\n# neutral DDEs with vanishing state dependent delays\nprob_dde_DDETST_H1, prob_dde_DDETST_H2, prob_dde_DDETST_H3, prob_dde_DDETST_H4","category":"page"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"prob_dde_DDETST_A1\nprob_dde_DDETST_A2\nprob_dde_DDETST_B1\nprob_dde_DDETST_B2\nprob_dde_DDETST_C1\nprob_dde_DDETST_C2\nprob_dde_DDETST_C3\nprob_dde_DDETST_C4\nprob_dde_DDETST_D1\nprob_dde_DDETST_D2\nprob_dde_DDETST_E1\nprob_dde_DDETST_E2\nprob_dde_DDETST_F1\nprob_dde_DDETST_F2\nprob_dde_DDETST_F3\nprob_dde_DDETST_F4\nprob_dde_DDETST_F5\nprob_dde_DDETST_G1\nprob_dde_DDETST_G2\nprob_dde_DDETST_H1\nprob_dde_DDETST_H2\nprob_dde_DDETST_H3\nprob_dde_DDETST_H4","category":"page"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_A1","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_A1","text":"prob_dde_DDETST_A1\n\nDelay differential equation model of blood production, given by\n\nu(t) = frac02 u(t - 14)1 + u(t - 14)^10 - 01 u(t)\n\nfor t in 0 500 and history function phi(t) = 05 for t leq 0.\n\nReferences\n\nMackey, M. C. and Glass, L. (1977). Oscillation and chaos in physiological control systems, Science (197), pp. 287-289.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_A2","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_A2","text":"prob_dde_DDETST_A2\n\nDelay differential equation model of chronic granulocytic leukemia, given by\n\nu_1(t) = frac111 + sqrt10 u_1(t - 20)^54 - frac10 u_1(t)1 + 40 u_2(t)\n\nu_2(t) = frac100 u_1(t)1 + 40 u_2(t) - 243 u_2(t)\n\nfor t in 0 100 and history function\n\nphi_1(t) = 1057670273\n\nphi_2(t) = 10307134913\n\nfor t leq 0.\n\nReferences\n\nWheldon, T., Kirk, J. and Finlay, H. (1974). Cyclical granulopoiesis in chronic granulocytic leukemia: A simulation study., Blood (43), pp. 379-387.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_B1","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_B1","text":"prob_dde_DDETST_B1\n\nDelay differential equation\n\nu(t) = 1 - u(exp(1 - 1t))\n\nfor t in 01 10 with history function phi(t) = log t for t in (0 01.\n\nSolution\n\nThe analytical solution for t in 01 10 is\n\nu(t) = log t\n\nReferences\n\nNeves, K. W. (1975). Automatic integration of functional differential equations: An approach, ACM Trans. Math. Soft. (1), pp. 357-368.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_B2","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_B2","text":"prob_dde_DDETST_B2\n\nDelay differential equation\n\nu(t) = - 1 - u(t) + 2 u(t  2)  0\n\nfor t in 0 2 log 66 with history function phi(0) = 1.\n\nSolution\n\nThe analytical solution for t in 0 2 log 66 is\n\nu(t) = begincases\n  2 exp(-t) - 1  textif  t in 0 2 log 2 \n  1 - 6 exp(-t)  textif   t in (2 log 2 2 log 6 \n  66 exp(-t) - 1  textif  t in (2 log 6 2 log 66\nendcases\n\nReferences\n\nNeves, K. W. and Thompson, S. (1992). Solution of systems of functional differential equations with state dependent delays, Technical Report TR-92-009, Computer Science, Radford University.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_C1","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_C1","text":"prob_dde_DDETST_C1\n\nDelay differential equation\n\nu(t) = - 2 u(t - 1 - u(t)) (1 - u(t)^2)\n\nfor t in 0 30 with history function phi(t) = 05 for t leq 0.\n\nReferences\n\nPaul, C. A. H. (1994). A test set of functional differential equations, Technical Report 249, The Department of Mathematics, The University of Manchester, Manchester, England.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_C2","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_C2","text":"prob_dde_DDETST_C2\n\nDelay differential equation\n\nu_1(t) = - 2 u_1(t - u_2(t))\n\nu_₂(t) = fracu_1(t - u_2(t)) - u_1(t)1 + u_1(t - u_2(t))\n\nfor t in 0 40 with history function\n\nphi_1(t) = 1\n\nphi_2(t) = 05\n\nfor t leq 0.\n\nReferences\n\nPaul, C. A. H. (1994). A test set of functional differential equations, Technical Report 249, The Department of Mathematics, The University of Manchester, Manchester, England.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_C3","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_C3","text":"prob_dde_DDETST_C3\n\nDelay differential equation model of hematopoiesis, given by\n\nu_1(t) = hats_0 u_2(t - T_1) - gamma u_1(t) - Q\n\nu_2(t) = f(u_1(t)) - k u_2(t)\n\nu_3(t) = 1 - fracQ exp(gamma u_3(t))hats_0 u_2(t - T_1 - u_3(t))\n\nfor t in 0 300 with history function phi_1(0) = 3325, phi_3(0) = 120, and\n\nphi_2(t) = begincases\n  10  textif  t in - T_1 0\n  95  textif  t  - T_1\nendcases\n\nwhere f(y) = a  (1 + K y^r), hats_0 = 00031, T_1 = 6, gamma = 0001, Q = 00275, k = 28, a = 6570, K = 00382, and r = 696.\n\nReferences\n\nMahaffy, J. M., Belair, J. and Mackey, M. C. (1996). Hematopoietic model with moving boundary condition and state dependent delay, Private communication.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_C4","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_C4","text":"prob_dde_DDETST_C4\n\nDelay differential equation model of hematopoiesis, given by the same delay differential equation as prob_dde_DDETST_C3\n\nu_1(t) = hats_0 u_2(t - T_1) - gamma u_1(t) - Q\n\nu_2(t) = f(u_1(t)) - k u_2(t)\n\nu_3(t) = 1 - fracQ exp(gamma u_3(t))hats_0 u_2(t - T_1 - u_3(t))\n\nfor t in 0 100 with history function phi_1(0) = 35, phi_3(0) = 50, and phi_2(t) = 10 for t leq 0, where f(y) = a  (1 + K y^r), hats_0 = 000372, T_1 = 3, gamma = 01, Q = 000178, k = 665, a = 15600, K = 00382, and r = 696.\n\nReferences\n\nMahaffy, J. M., Belair, J. and Mackey, M. C. (1996). Hematopoietic model with moving boundary condition and state dependent delay, Private communication.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_D1","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_D1","text":"prob_dde_DDETST_D1\n\nDelay differential equation\n\nu_1(t) = u_2(t) \n\nu_2(t) = - u_2(exp(1 - u_2(t))) u_2(t)^2 exp(1 - u_2(t))\n\nfor t in 01 5 with history function\n\nphi_1(t) = log t \n\nphi_2(t) = 1  t\n\nfor t in (0 01.\n\nSolution\n\nThe analytical solution for t in 01 5 is\n\nu_1(t) = log t \n\nu_2(t) = 1  t\n\nReferences\n\nNeves, K. W. (1975). Automatic integration of functional differential equations: An approach, ACM Trans. Math. Soft. (1), pp. 357-368.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_D2","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_D2","text":"prob_dde_DDETST_D2\n\nDelay differential equation model of antigen antibody dynamics with fading memory, given by\n\nu_1(t) = - r_1 u_1(t) u_2(t) + r_2 u_3(t) \n\nu_2(t) = - r_1 u_1(t) u_2(t) + alpha r_1 u_1(t - u_4(t)) u_2(t - u_4(t))\n\nu_3(t) = r_1 u_1(t) u_2(t) - r_2 u_3(t) \n\nu_4(t) = 1 + frac3 delta - u_1(t) u_2(t) - u_3(t)u_1(t - u_4(t)) u_2(t - u_4(t)) + u_3(t - u_4(t)) exp(delta u_4(t))\n\nfor t in 0 40 with history function\n\nphi_1(t) = 5 \n\nphi_2(t) = 01 \n\nphi_3(t) = 0 \n\nphi_4(t) = 0\n\nfor t leq 0, where r_1 = 002, r_2 = 0005, alpha = 3, and delta = 001.\n\nReferences\n\nGatica, J. and Waltman, P. (1982). A threshold model of antigen antibody dynamics with fading memory, in Lakshmikantham (ed.), Nonlinear phenomena in mathematical science, Academic Press, New York, pp. 425-439.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_E1","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_E1","text":"prob_dde_DDETST_E1\n\nDelay differential equation model of a food-limited population, given by\n\nu(t) = r u(t) (1 - u(t - 1) - c u(t - 1))\n\nfor t in 0 40 with history function phi(t) = 2 + t for t leq 0, where r = pi  sqrt3 + 120 and c = sqrt3  (2 pi) - 1  25.\n\nReferences\n\nKuang, Y. and Feldstein, A. (1991). Boundedness of solutions of a nonlinear nonautonomous neutral delay equation, J. Math. Anal. Appl. (156), pp. 293-304.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_E2","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_E2","text":"prob_dde_DDETST_E2\n\nDelay differential equation model of a logistic Gauss-type predator-prey system, given by\n\nu_1(t) = u_1(t) (1 - u_1(t - tau) - rho u_1(t - tau)) - fracu_2(t) u_1(t)^2u_1(t)^2 + 1 \n\nu_2(t) = u_2(t) left(fracu_1(t)^2u_1(t)^2 + 1 - alpharight)\n\nfor t in 0 2 with history function\n\nphi_1(t) = 033 - t  10 \n\nphi_2(t) = 222 + t  10\n\nfor t leq 0, where alpha = 01, rho = 29, and tau = 042.\n\nReferences\n\nKuang, Y. (1991). On neutral delay logistics Gauss-type predator-prey systems, Dyn. Stab. Systems (6), pp. 173-189.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_F1","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_F1","text":"prob_dde_DDETST_F1\n\nDelay differential equation\n\nu(t) = 2 cos(2t) u(t  2)^2 cos t + log(u(t  2)) - log(2 cos t) - sin t\n\nfor t in 0 1 with history function phi(0) = 1 and phi(0) = 2.\n\nSolution\n\nThe analytical solution for t in 0 1 is\n\nu(t) = exp(sin(2t))\n\nReferences\n\nJackiewicz, Z. (1981). One step methods for the numerical solution of Volterra functional differential equations of neutral type, Applicable Anal. (12), pp. 1-11.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_F2","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_F2","text":"prob_dde_DDETST_F2\n\nDelay differential equation\n\nu(t) = u(2t - 05)\n\nfor t in 025 0499 with history function phi(t) = exp(-t^2) and phi(t) = -2t exp(-t^2) for t leq 025.\n\nSolution\n\nThe analytical solution for t in 025 0499 is\n\nu(t) = u_i(t) = exp(-4^i t^2 + B_i t + C_i)  2^i + K_i\n\nif t in x_i x_i + 1, where\n\nx_i = (1 - 2^-i)  2 \n\nB_i = 2 (4^i-1 + B_i-1) \n\nC_i = - 4^i-2 - B_i-1  2 + C_i-1 \n\nK_i = - exp(-4^i x_i^2 + B_i x_i + C_i)  2^i + u_i-1(x_i)\n\nand B_0 = C_0 = K_0 = 0.\n\nReferences\n\nNeves, K. W. and Thompson, S. (1992). Solution of systems of functional differential equations with state dependent delays, Technical Report TR-92-009, Computer Science, Radford University.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_F3","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_F3","text":"prob_dde_DDETST_F3\n\nDelay differential equation\n\nu(t) = exp(-u(t)) + L_3 leftsin(u(alpha(t))) - sinleft(frac13 + alpha(t)right)right\n\nfor t in 0 10 with history function phi(0) = log 3 and phi(0) = 1  3, where alpha(t) = 05 t (1 - cos(2 pi t)) and L_3 = 02.\n\nSolution\n\nThe analytical solution for t in 0 10 is\n\nu(t) = log(t + 3)\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_F4","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_F4","text":"prob_dde_DDETST_F4\n\nSame delay differential equation as prob_dde_DDETST_F3 with L_3 = 04.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_F5","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_F5","text":"prob_dde_DDETST_F5\n\nSame delay differential equation as prob_dde_DDETST_F3 with L_3 = 06.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_G1","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_G1","text":"prob_dde_DDETST_G1\n\nDelay differential equation\n\nu(t) = - u(t - u(t)^2  4)\n\nfor t in 0 1 with history function phi(t) = 1 - t for t leq 0 and phi(t) = -1 for t  0.\n\nSolution\n\nThe analytical solution for t in 0 1 is\n\nu(t) = t + 1\n\nReferences\n\nEl'sgol'ts, L. E. and Norkin, S. B. (1973). Introduction to the Theory and Application of Differential Equations with Deviating Arguments, Academic Press, New York, p. 44.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_G2","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_G2","text":"prob_dde_DDETST_G2\n\nDelay differential equation\n\nu(t) = - u(u(t) - 2)\n\nfor t in 0 1 with history function phi(t) = 1 - t for t leq 0 and phi(t) = -1 for t  0.\n\nSolution\n\nThe analytical solution for t in 0 1 is\n\nu(t) = t + 1\n\nEl'sgol'ts, L. E. and Norkin, S. B. (1973). Introduction to the Theory and Application of Differential Equations with Deviating Arguments, Academic Press, New York, pp. 44-45.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_H1","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_H1","text":"prob_dde_DDETST_H1\n\nDelay differential equation\n\nu(t) = - frac4 t u(t)^24 + log(cos(2t))^2 + tan(2t) + 05 arctanleft(uleft(fract u(t)^21 + u(t)^2right)right)\n\nfor t in 0 0225 pi with history function phi(0) = 0 and phi(0) = 0.\n\nSolution\n\nThe analytical solution for t in 0 0225 pi is\n\nu(t) = - log(cos(2t))  2\n\nReferences\n\nCastleton, R. N. and Grimm, L. J. (1973). A first order method for differential equations of neutral type, Math. Comput. (27), pp. 571-577.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_H2","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_H2","text":"prob_dde_DDETST_H2\n\nDelay differential equation\n\nu(t) = cos(t) (1 + u(t u(t)^2)) + L_3 u(t) u(t u(t)^2) + (1 - L_3) sin(t) cos(t sin(t)^2) - sin(t + t sin(t)^2)\n\nfor t in 0 pi with history function phi(0) = 0 and phi(0) = 1, where L_3 = 01.\n\nSolution\n\nThe analytical solution for t in 0 pi is\n\nu(t) = sin(t)\n\nReferences\n\nHayashi, H. (1996). Numerical solution of retarded and neutral delay differential equations using continuous Runge-Kutta methods, PhD thesis, Department of Computer Science, University of Toronto, Toronto, Canada.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_H3","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_H3","text":"prob_dde_DDETST_H3\n\nSame delay differential equation as prob_dde_DDETST_H2 with L_3 = 03.\n\nReferences\n\nHayashi, H. (1996). Numerical solution of retarded and neutral delay differential equations using continuous Runge-Kutta methods, PhD thesis, Department of Computer Science, University of Toronto, Toronto, Canada.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_H4","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_DDETST_H4","text":"prob_dde_DDETST_H4\n\nSame delay differential equation as prob_dde_DDETST_H2 with L_3 = 05.\n\nReferences\n\nHayashi, H. (1996). Numerical solution of retarded and neutral delay differential equations using continuous Runge-Kutta methods, PhD thesis, Department of Computer Science, University of Toronto, Toronto, Canada.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#Radar5-Test-Problems","page":"DDE Problems","title":"Radar5 Test Problems","text":"","category":"section"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"prob_dde_RADAR5_oregonator\nprob_dde_RADAR5_robertson\nprob_dde_RADAR5_waltman","category":"page"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_RADAR5_oregonator","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_RADAR5_oregonator","text":"prob_dde_RADAR5_oregonator\n\nDelay differential equation model from chemical kinetics, given by\n\n  u_1(t) = - k_1 A u_2(t) - k_2 u_1(t) u_2(t - tau) + k_3 B u_1(t) - 2 k_4 u_1(t)^2 \n\n  u_2(t) = - k_1 A u_2(t) - k_2 u_1(t) u_2(t - tau) + f k_3 B u_1(t)\n\nfor t in 0 1005 with history function\n\n  phi_1(t) = 1e-10 \n\n  phi_2(t) = 1e-5\n\nfor t leq 0, where k_1 = 134, k_2 = 16e9, k_3 = 8000, k_4 = 4e7, k_5 = 1, f = 1, A = 006, B = 006, and tau = 015.\n\nReferences\n\nEpstein, I. and Luo, Y. (1991). Differential delay equations in chemical kinetics. Nonlinear models, Journal of Chemical Physics (95), pp. 244-254.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_RADAR5_robertson","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_RADAR5_robertson","text":"prob_dde_RADAR5_robertson\n\nDelay differential equation model of a chemical reaction with steady state solution, given by\n\n  u_1(t) = - a u_1(t) + b u_2(t - tau) u_3(t) \n\n  u_2(t) = a u_1(t) - b u_2(t - tau) u_3(t) - c u_2(t)^2 \n\n  u_3(t) = c u_2(t)^2\n\nfor t in 0 10e10 with history function phi_1(0) = 1, phi_2(t) = 0 for t in -tau 0, and phi_3(0) = 0, where a = 004, b = 10_000, c = 3e7, and tau = 001.\n\nReferences\n\nGuglielmi, N. and Hairer, E. (2001). Implementing Radau IIA methods for stiff delay differential equations, Computing (67), pp. 1-12.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_RADAR5_waltman","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_RADAR5_waltman","text":"prob_dde_RADAR5_waltman\n\nDelay differential equation model of antibody production, given by\n\n  u_1(t) = - r u_1(t) u_2(t) - s u_1(t) u_4(t) \n\n  u_2(t) = - r u_1(t) u_2(t) + alpha r u_1(u_5(t)) u_2(u_5(t)) t geq t_0 \n\n  u_3(t) = r u_1(t) u_2(t) \n\n  u_4(t) = - s u_1(t) u_4(t) - gamma u_4(t) + beta r u_1(u_6(t)) u_2(u_6(t)) t  t_1 \n\n  u_5(t) = t geq t_0 fracu_1(t) u_2(t) + u_3(t)u_1(u_5(t)) u_2(u_5(t)) + u_3(u_5(t)) \n\n  u_6(t) = t geq t_1 frac1e-12 + u_2(t) + u_3(t)1e-12 + u_2(u_6(t)) + u_3(u_6(t))\n\nfor t in 0 300 with history function\n\n  phi_1(t) = phi_0 \n\n  phi_2(t) = 1e-15 \n\n  phi_3(t) = 0 \n\n  phi_4(t) = 0 \n\n\n\n  phi_6(t) = 0\n\nfor t leq 0, where alpha = 18, beta = 20, gamma = 0002, r = 5e4, s = 1e5, t_0 = 32, t_1 = 119, and phi_0 = 075e-4.\n\nReferences\n\nWaltman, P. (1978). A threshold model of antigen-stimulated antibody production, Theoretical Immunology (8), pp. 437-453.\n\n\n\n\n\n","category":"constant"},{"location":"types/dde_types/#QS-Example","page":"DDE Problems","title":"QS Example","text":"","category":"section"},{"location":"types/dde_types/","page":"DDE Problems","title":"DDE Problems","text":"prob_dde_qs","category":"page"},{"location":"types/dde_types/#DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_qs","page":"DDE Problems","title":"DiffEqProblemLibrary.DDEProblemLibrary.prob_dde_qs","text":"prob_dde_qs\n\nDelay differential equation model of Quorum Sensing (QS) of Pseudomonas putida IsoF in continuous cultures.\n\nReferences\n\nBuddrus-Schiemann et al. (2014). Analysis of N-Acylhomoserine Lactone Dynamics in Continuous Cultures of Pseudomonas Putida IsoF By Use of ELISA and UHPLC/qTOF-MS-derived Measurements and Mathematical Models, Analytical and Bioanalytical Chemistry.\n\n\n\n\n\n","category":"constant"},{"location":"basics/compatibility_chart/#Solver-Compatibility-Chart","page":"Solver Compatibility Chart","title":"Solver Compatibility Chart","text":"","category":"section"},{"location":"basics/compatibility_chart/","page":"Solver Compatibility Chart","title":"Solver Compatibility Chart","text":"This chart is for documenting the compatibility of the component solver packages to the common interface. An x means that the option is implemented or the add-on functionality will work with the given solver. A blank means that the option has not been implemented or that a given add-on has not been tested with a given package. If there are any errors in this chart, please file an issue or submit a pull-request.","category":"page"},{"location":"basics/compatibility_chart/","page":"Solver Compatibility Chart","title":"Solver Compatibility Chart","text":"Option OrdinaryDiffEq.jl Sundials.jl ODE.jl ODEInterface.jl LSODA.jl StochasticDiffEq.jl DelayDiffEq.jl DASKR.jl DASSL.jl\nNonlinear Dense (continuous) output x x    x x x \nTolerance control x x x x x x x x x\nAdvanced stepsize control x 0  x 0 x x 0 \nMass Matrices^ x 0  x 0 x x 0 \nAnalytical Jacobians^† x x  x  x x x \nGeneral Performance Overloads^† x 0  0 0 x x 0 \ninternalnorm x 0 x 0 0 x x 0 \nInitial dt x x x x  x x x \nsave_everystep x x x x x x x x \nsaveat x x x x x x x x \ntstops x x  0  x x x \nd_discontinuities x   0  x x  \nisoutofdomain x  x   x x  \nAllows reverse time direction x x x x x x x  \nUnitful numbers x 0  0 0  x 0 \nArbitrary dimension arrays x x x x x x x x x\nComplex numbers p     x p  \nArbitrary precision x 0 x 0 0 x x 0 x\nApproxFun types x 0  0 0  x 0 \nProgress monitoring x     x x  \nIntegrator interface x x  0  x x  \nResizability x 0  0 0 x x 0 \nCache iterator x 0  0 0 x x 0 \nCan choose linear solvers x s    x x s x\nCan choose nonlinear solvers x 0  0 0 x x 0 x\nCan use out of place natively x 0 x 0 0 x x 0 x\nCan use inplace natively x x  x x x x x \nCompatible with DiffEqDevTools x x x x x x x x \nCompatible with ParameterizedFunctions x x x x x x x x \nContinuous Callbacks x x  x  x x  x\nDiscrete Callbacks x x  x  x x  \nMonte Carlo Simulations x x x x x x x x \nParameter Estimation x n n n n x x n x\nParameter Sensitivity Analysis x x x x x  x  \nPlotting and solution handling x x x x x x x x x","category":"page"},{"location":"basics/compatibility_chart/","page":"Solver Compatibility Chart","title":"Solver Compatibility Chart","text":"x: Full compatibility\np: Partial compatibility, only in nonstiff methods unless the Jacobian is provided.\nn: General compatibility, but not compatible with routines which. require being able to autodifferentiate through the entire solver.\n0: Not possible. This is generally due to underlying inflexibility in a wrapped library.\ns: Special, Sundials has its own linear solver choices.\n^: Only stiff (implicit) methods.\n†: For packages with compatibility, no warning is given when a specific algorithm does not need to use this feature.","category":"page"},{"location":"basics/compatibility_chart/","page":"Solver Compatibility Chart","title":"Solver Compatibility Chart","text":"All blank spaces are possible future additions.","category":"page"},{"location":"types/dae_types/#DAE-Problems","page":"DAE Problems","title":"DAE Problems","text":"","category":"section"},{"location":"types/dae_types/#Mathematical-Specification-of-an-DAE-Problem","page":"DAE Problems","title":"Mathematical Specification of an DAE Problem","text":"","category":"section"},{"location":"types/dae_types/","page":"DAE Problems","title":"DAE Problems","text":"To define a DAE Problem, you simply need to give the function f and the initial condition u₀ which define an ODE:","category":"page"},{"location":"types/dae_types/","page":"DAE Problems","title":"DAE Problems","text":"0 = f(duupt)","category":"page"},{"location":"types/dae_types/","page":"DAE Problems","title":"DAE Problems","text":"f should be specified as f(du,u,p,t) (or in-place as f(resid,du,u,p,t)). Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher dimension tensors as well.","category":"page"},{"location":"types/dae_types/#Problem-Type","page":"DAE Problems","title":"Problem Type","text":"","category":"section"},{"location":"types/dae_types/#Constructors","page":"DAE Problems","title":"Constructors","text":"","category":"section"},{"location":"types/dae_types/","page":"DAE Problems","title":"DAE Problems","text":"DAEProblem(f::DAEFunction,du0,u0,tspan,p=NullParameters();kwargs...)\nDAEProblem{isinplace}(f,du0,u0,tspan,p=NullParameters();kwargs...) : Defines the DAE with the specified functions. isinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred.","category":"page"},{"location":"types/dae_types/","page":"DAE Problems","title":"DAE Problems","text":"Parameters are optional, and if not given then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.","category":"page"},{"location":"types/dae_types/","page":"DAE Problems","title":"DAE Problems","text":"For specifying Jacobians and mass matrices, see the DiffEqFunctions page.","category":"page"},{"location":"types/dae_types/#Fields","page":"DAE Problems","title":"Fields","text":"","category":"section"},{"location":"types/dae_types/","page":"DAE Problems","title":"DAE Problems","text":"f: The function in the ODE.\ndu0: The initial condition for the derivative.\nu0: The initial condition.\ntspan: The timespan for the problem.\ndifferential_vars: A logical array which declares which variables are the differential (non algebraic) vars (i.e. du' is in the equations for this variable). Defaults to nothing. Some solvers may require this be set if an initial condition needs to be determined.\np: The parameters for the problem. Defaults to NullParameters\nkwargs: The keyword arguments passed onto the solves.","category":"page"},{"location":"types/dae_types/#Example-Problems","page":"DAE Problems","title":"Example Problems","text":"","category":"section"},{"location":"types/dae_types/","page":"DAE Problems","title":"DAE Problems","text":"Examples problems can be found in DiffEqProblemLibrary.jl.","category":"page"},{"location":"types/dae_types/","page":"DAE Problems","title":"DAE Problems","text":"To use a sample problem, such as prob_dae_resrob, you can do something like:","category":"page"},{"location":"types/dae_types/","page":"DAE Problems","title":"DAE Problems","text":"#] add DiffEqProblemLibrary\nusing DiffEqProblemLibrary.DAEProblemLibrary\n# load problems\nDAEProblemLibrary.importdaeproblems()\nprob = DAEProblemLibrary.prob_dae_resrob\nsol = solve(prob,IDA())","category":"page"},{"location":"types/dae_types/","page":"DAE Problems","title":"DAE Problems","text":"CurrentModule = DAEProblemLibrary","category":"page"},{"location":"types/dae_types/","page":"DAE Problems","title":"DAE Problems","text":"prob_dae_resrob","category":"page"},{"location":"types/dae_types/#DiffEqProblemLibrary.DAEProblemLibrary.prob_dae_resrob","page":"DAE Problems","title":"DiffEqProblemLibrary.DAEProblemLibrary.prob_dae_resrob","text":"The Robertson biochemical reactions in DAE form\n\nfracdy₁dt = -k₁y₁+k₃y₂y₃\n\nfracdy₂dt =  k₁y₁-k₂y₂^2-k₃y₂y₃\n\n1 = y₁ + y₂ + y₃\n\nwhere k₁=004, k₂=3times10^7, k₃=10^4. For details, see: Hairer Norsett Wanner Solving Ordinary Differential Equations I - Nonstiff Problems Page 129 Usually solved on 01e11\n\n\n\n","category":"constant"},{"location":"basics/solution/#solution","page":"Solution Handling","title":"Solution Handling","text":"","category":"section"},{"location":"basics/solution/#Accessing-the-Values","page":"Solution Handling","title":"Accessing the Values","text":"","category":"section"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"The solution type has a lot of built in functionality to help analysis. For example, it has an array interface for accessing the values. Internally, the solution type has two important fields:","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"u which holds the Vector of values at each timestep\nt which holds the times of each timestep.","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"Different solution types may add extra information as necessary, such as the derivative at each timestep du or the spatial discretization x, y, etc.","category":"page"},{"location":"basics/solution/#Array-Interface","page":"Solution Handling","title":"Array Interface","text":"","category":"section"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"Instead of working on the Vector{uType} directly, we can use the provided array interface.","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"sol[j]","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"to access the value at timestep j (if the timeseries was saved), and","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"sol.t[j]","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"to access the value of t at timestep j. For multi-dimensional systems, this will address first by component and lastly by time, and thus","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"sol[i,j]","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"will be the ith component at timestep j. Hence, sol[j][i] == sol[i, j]. This is done because Julia is column-major, so the leading dimension should be contiguous in memory. If the independent variables had shape (for example, was a matrix), then i is the linear index. We can also access solutions with shape:","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"sol[i,k,j]","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"gives the [i,k] component of the system at timestep j. The colon operator is supported, meaning that","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"sol[i,:]","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"gives the timeseries for the ith component.","category":"page"},{"location":"basics/solution/#Using-the-AbstractArray-Interface","page":"Solution Handling","title":"Using the AbstractArray Interface","text":"","category":"section"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"The AbstractArray interface can be directly used. For example, for a vector system of variables sol[i,j] is a matrix with rows being the variables and columns being the timepoints. Operations like sol' will transpose the solution type. Functionality written for AbstractArrays can directly use this. For example, the Base cov function computes correlations amongst columns, and thus:","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"cov(sol)","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"computes the correlation of the system state in time, whereas","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"cov(sol,2)","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"computes the correlation between the variables. Similarly, mean(sol,2) is the mean of the variable in time, and var(sol,2) is the variance. Other statistical functions and packages which work on AbstractArray types will work on the solution type.","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"At anytime, a true Array can be created using Array(sol).","category":"page"},{"location":"basics/solution/#Interpolations-and-Calculating-Derivatives","page":"Solution Handling","title":"Interpolations and Calculating Derivatives","text":"","category":"section"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"If the solver allows for dense output and dense=true was set for the solving (which is the default), then we can access the approximate value at a time t using the command","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"sol(t)","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"Note that the interpolating function allows for t to be a vector and uses this to speed up the interpolation calculations. The full API for the interpolations is","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"sol(t,deriv=Val{0};idxs=nothing,continuity=:left)","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"The optional argument deriv lets you choose the number n derivative to solve the interpolation for, defaulting with n=0. Note that most of the derivatives have not yet been implemented (though it's not hard, it just has to be done by hand for each algorithm. Open an issue if there's a specific one you need). continuity describes whether to satisfy left or right continuity when a discontinuity is saved. The default is :left, i.e. grab the value before the callback's change, but can be changed to :right. idxs allows you to choose the indices the interpolation should solve for. For example,","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"sol(t,idxs=1:2:5)","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"will return a Vector of length 3 which is the interpolated values at t for components 1, 3, and 5. idxs=nothing, the default, means it will return every component. In addition, we can do","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"sol(t,idxs=1)","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"and it will return a Number for the interpolation of the single value. Note that this interpolation only computes the values which are requested, and thus it's much faster on large systems to use this rather than computing the full interpolation and using only a few values.","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"In addition, there is an inplace form:","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"sol(out,t,deriv=Val{0};idxs=nothing,continuity=:left)","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"which will write the output to out. This allows one to use pre-allocated vectors for the output to improve the speed even more.","category":"page"},{"location":"basics/solution/#Comprehensions","page":"Solution Handling","title":"Comprehensions","text":"","category":"section"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"The solver interface also gives tools for using comprehensions over the solution. Using the tuples(sol) function, we can get a tuple for the output at each timestep. This allows one to do the following:","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"[t+2u for (u,t) in tuples(sol)]","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"One can use the extra components of the solution object as well as using zip. For example, say the solution type holds du, the derivative at each timestep. One can comprehend over the values using:","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"[t+3u-du for (t,u,du) in zip(sol.t,sol.u,sol.du)]","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"Note that the solution object acts as a vector in time, and so its length is the number of saved timepoints.","category":"page"},{"location":"basics/solution/#Special-Fields","page":"Solution Handling","title":"Special Fields","text":"","category":"section"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"The solution interface also includes some special fields. The problem object prob and the algorithm used to solve the problem alg are included in the solution. Additionally, the field dense is a boolean which states whether the interpolation functionality is available. Further, the field destats contains the internal statistics for the solution process such as the number of linear solves and convergence failures. Lastly, there is a mutable state tslocation which controls the plot recipe behavior. By default, tslocation=0. Its values have different meanings between partial and ordinary differential equations:","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"tslocation=0  for non-spatial problems (ODEs) means that the plot recipe will plot the full solution. tslocation=i means that it will only plot the timepoint i.\ntslocation=0 for spatial problems (PDEs) means the plot recipe will plot the final timepoint. tslocation=i means that the plot recipe will plot the ith timepoint.","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"What this means is that for ODEs, the plots will default to the full plot and PDEs will default to plotting the surface at the final timepoint. The iterator interface simply iterates the value of tslocation, and the animate function iterates the solution calling solve at each step.","category":"page"},{"location":"basics/solution/#retcodes","page":"Solution Handling","title":"Return Codes (RetCodes)","text":"","category":"section"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"The solution types have a retcode field which returns a symbol signifying the error state of the solution. The retcodes are as follows:","category":"page"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":":Default: The solver did not set retcodes.\n:Success: The integration completed without erroring or the steady state solver from SteadyStateDiffEq found the steady state.\n:Terminated: The integration is terminated with terminate!(integrator). Note that this may occur by using TerminateSteadyState from the callback library DiffEqCallbacks.\n:MaxIters: The integration exited early because it reached its maximum number of iterations.\n:DtLessThanMin: The timestep method chose a stepsize which is smaller than the allowed minimum timestep, and exited early.\n:Unstable: The solver detected that the solution was unstable and exited early.\n:InitialFailure: The DAE solver could not find consistent initial conditions.\n:ConvergenceFailure: The internal implicit solvers failed to converge.\n:Failure: General uncategorized failures or errors.","category":"page"},{"location":"basics/solution/#Problem-Specific-Features","page":"Solution Handling","title":"Problem-Specific Features","text":"","category":"section"},{"location":"basics/solution/","page":"Solution Handling","title":"Solution Handling","text":"Extra fields for solutions of specific problems are specified in the appropriate problem definition page.  ","category":"page"},{"location":"types/sde_types/#SDE-Problems","page":"SDE Problems","title":"SDE Problems","text":"","category":"section"},{"location":"types/sde_types/#Mathematical-Specification-of-a-SDE-Problem","page":"SDE Problems","title":"Mathematical Specification of a SDE Problem","text":"","category":"section"},{"location":"types/sde_types/","page":"SDE Problems","title":"SDE Problems","text":"To define an SDE Problem, you simply need to give the forcing function f, the noise function g, and the initial condition u₀ which define an SDE:","category":"page"},{"location":"types/sde_types/","page":"SDE Problems","title":"SDE Problems","text":"du = f(upt)dt + Σgᵢ(upt)dWⁱ","category":"page"},{"location":"types/sde_types/","page":"SDE Problems","title":"SDE Problems","text":"f and g should be specified as f(u,p,t) and  g(u,p,t) respectively, and u₀ should be an AbstractArray whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher dimension tensors as well. A vector of gs can also be defined to determine an SDE of higher Ito dimension.","category":"page"},{"location":"types/sde_types/#Problem-Type","page":"SDE Problems","title":"Problem Type","text":"","category":"section"},{"location":"types/sde_types/","page":"SDE Problems","title":"SDE Problems","text":"Wraps the data which defines an SDE problem","category":"page"},{"location":"types/sde_types/","page":"SDE Problems","title":"SDE Problems","text":"u = f(upt)dt + Σgᵢ(upt)dWⁱ","category":"page"},{"location":"types/sde_types/","page":"SDE Problems","title":"SDE Problems","text":"with initial condition u0.","category":"page"},{"location":"types/sde_types/#Constructors","page":"SDE Problems","title":"Constructors","text":"","category":"section"},{"location":"types/sde_types/","page":"SDE Problems","title":"SDE Problems","text":"SDEProblem(f::SDEFunction,g,u0,tspan,p=NullParameters();noise=WHITE_NOISE,noise_rate_prototype=nothing)\nSDEProblem{isinplace}(f,g,u0,tspan,p=NullParameters();noise=WHITE_NOISE,noise_rate_prototype=nothing) : Defines the SDE with the specified functions. The default noise is WHITE_NOISE. isinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred.","category":"page"},{"location":"types/sde_types/","page":"SDE Problems","title":"SDE Problems","text":"Parameters are optional, and if not given then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.","category":"page"},{"location":"types/sde_types/","page":"SDE Problems","title":"SDE Problems","text":"For specifying Jacobians and mass matrices, see the DiffEqFunctions page.","category":"page"},{"location":"types/sde_types/#Fields","page":"SDE Problems","title":"Fields","text":"","category":"section"},{"location":"types/sde_types/","page":"SDE Problems","title":"SDE Problems","text":"f: The drift function in the SDE.\ng: The noise function in the SDE.\nu0: The initial condition.\ntspan: The timespan for the problem.\np: The optional parameters for the problem. Defaults to NullParameters.\nnoise: The noise process applied to the noise upon generation. Defaults to Gaussian white noise. For information on defining different noise processes, see the noise process documentation page\nnoise_rate_prototype: A prototype type instance for the noise rates, that is the output g. It can be any type which overloads A_mul_B! with itself being the middle argument. Commonly, this is a matrix or sparse matrix. If this is not given, it defaults to nothing, which means the problem should be interpreted as having diagonal noise.  \nkwargs: The keyword arguments passed onto the solves.","category":"page"},{"location":"types/sde_types/#Example-Problems","page":"SDE Problems","title":"Example Problems","text":"","category":"section"},{"location":"types/sde_types/","page":"SDE Problems","title":"SDE Problems","text":"Examples problems can be found in DiffEqProblemLibrary.jl.","category":"page"},{"location":"types/sde_types/","page":"SDE Problems","title":"SDE Problems","text":"To use a sample problem, such as prob_sde_linear, you can do something like:","category":"page"},{"location":"types/sde_types/","page":"SDE Problems","title":"SDE Problems","text":"#] add DiffEqProblemLibrary\nusing DiffEqProblemLibrary.SDEProblemLibrary\n# load problems\nSDEProblemLibrary.importsdeproblems()\nprob = SDEProblemLibrary.prob_sde_linear\nsol = solve(prob)","category":"page"},{"location":"types/sde_types/","page":"SDE Problems","title":"SDE Problems","text":"CurrentModule = SDEProblemLibrary","category":"page"},{"location":"types/sde_types/","page":"SDE Problems","title":"SDE Problems","text":"prob_sde_linear\nprob_sde_2Dlinear\nprob_sde_wave\nprob_sde_lorenz\nprob_sde_cubic\nprob_sde_additive\nprob_sde_additivesystem\nprob_sde_nltest\noval2ModelExample\nprob_sde_stiffquadstrat\nprob_sde_stiffquadito\ngenerate_stiff_stoch_heat\nprob_sde_bistable\nprob_sde_bruss\nprob_sde_oscilreact","category":"page"},{"location":"types/sde_types/#DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_linear","page":"SDE Problems","title":"DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_linear","text":"du_t = αudt + βudW_t\n\nwhere α=101, β=087, and initial condtion u_0=12, with solution\n\nu(u_0ptW_t)=u_0exp((α-fracβ^22)t+βW_t)\n\n\n\n","category":"constant"},{"location":"types/sde_types/#DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_2Dlinear","page":"SDE Problems","title":"DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_2Dlinear","text":"8 linear SDEs (as a 4x2 matrix):\n\ndu_t = αudt + βudW_t\n\nwhere α=101, β=087, and initial condtion u_0=frac12 with solution\n\nu(u_0ptW_t)=u_0exp((α-fracβ^22)t+βW_t)\n\n\n\n","category":"constant"},{"location":"types/sde_types/#DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_wave","page":"SDE Problems","title":"DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_wave","text":"du_t = -frac1100sin(u)cos^3(u)dt + frac110cos^2(u_t) dW_t\n\nand initial condition u_0=1 with solution\n\nu(u_0ptW_t)=arctan(fracW_t10 + tan(u_0))\n\n\n\n","category":"constant"},{"location":"types/sde_types/#DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_lorenz","page":"SDE Problems","title":"DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_lorenz","text":"Lorenz Attractor with additive noise\n\ndx = σ(y-x)dt + αdW_t\n\ndy = (x(ρ-z) - y)dt + αdW_t\n\ndz = (xy - βz)dt + αdW_t\n\nwith σ=10, ρ=28, β=83, α=30 and inital condition u_0=111.\n\n\n\n","category":"constant"},{"location":"types/sde_types/#DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_cubic","page":"SDE Problems","title":"DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_cubic","text":"du_t = frac14u(1-u^2)dt + frac12(1-u^2)dW_t\n\nand initial condtion u_0=frac12, with solution\n\nu(u0ptW_t)=frac(1+u_0)exp(W_t)+u)0-1(1+u_0)exp(W_t)+1-u_0\n\n\n\n","category":"constant"},{"location":"types/sde_types/#DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_additive","page":"SDE Problems","title":"DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_additive","text":"Additive noise problem\n\nu_t = (fracβsqrt1+t-frac12(1+t)u_t)dt + fracαβsqrt1+tdW_t\n\nand initial condition u_0=1 with α=01 and β=005, with solution\n\nu(u_0ptW_t)=fracu_0sqrt1+t + fracβ(t+αW_t)sqrt1+t\n\n\n\n","category":"constant"},{"location":"types/sde_types/#DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_additivesystem","page":"SDE Problems","title":"DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_additivesystem","text":"A multiple dimension extension of additiveSDEExample\n\n\n\n","category":"constant"},{"location":"types/sde_types/#DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_nltest","page":"SDE Problems","title":"DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_nltest","text":"Runge–Kutta methods for numerical solution of stochastic differential equations Tocino and Ardanuy\n\n\n\n","category":"constant"},{"location":"types/sde_types/#DiffEqProblemLibrary.SDEProblemLibrary.oval2ModelExample","page":"SDE Problems","title":"DiffEqProblemLibrary.SDEProblemLibrary.oval2ModelExample","text":"oval2ModelExample(;largeFluctuations=false,useBigs=false,noiseLevel=1)\n\nA function which generates the Oval2 Epithelial-Mesenchymal Transition model from:\n\nRackauckas, C., & Nie, Q. (2017). Adaptive methods for stochastic differential equations  via natural embeddings and rejection sampling with memory. Discrete and continuous  dynamical systems. Series B, 22(7), 2731.\n\n19 SDEs which are only stiff during transitions between biological states.\n\n\n\n","category":"function"},{"location":"types/sde_types/#DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_stiffquadstrat","page":"SDE Problems","title":"DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_stiffquadstrat","text":"The composite Euler method for stiff stochastic differential equations\n\nKevin Burrage, Tianhai Tian\n\nAnd\n\nS-ROCK: CHEBYSHEV METHODS FOR STIFF STOCHASTIC DIFFERENTIAL EQUATIONS\n\nASSYR ABDULLE AND STEPHANE CIRILLI\n\nStiffness of Euler is determined by α+β²<1 Higher α or β is stiff, with α being deterministic stiffness and β being noise stiffness (and grows by square).\n\n\n\n","category":"constant"},{"location":"types/sde_types/#DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_stiffquadito","page":"SDE Problems","title":"DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_stiffquadito","text":"The composite Euler method for stiff stochastic differential equations\n\nKevin Burrage, Tianhai Tian\n\nAnd\n\nS-ROCK: CHEBYSHEV METHODS FOR STIFF STOCHASTIC DIFFERENTIAL EQUATIONS\n\nASSYR ABDULLE AND STEPHANE CIRILLI\n\nStiffness of Euler is determined by α+β²<1 Higher α or β is stiff, with α being deterministic stiffness and β being noise stiffness (and grows by square).\n\n\n\n","category":"constant"},{"location":"types/sde_types/#DiffEqProblemLibrary.SDEProblemLibrary.generate_stiff_stoch_heat","page":"SDE Problems","title":"DiffEqProblemLibrary.SDEProblemLibrary.generate_stiff_stoch_heat","text":"Stochastic Heat Equation with scalar multiplicative noise\n\nS-ROCK: CHEBYSHEV METHODS FOR STIFF STOCHASTIC DIFFERENTIAL EQUATIONS\n\nASSYR ABDULLE AND STEPHANE CIRILLI\n\nRaising D or k increases stiffness\n\n\n\n","category":"function"},{"location":"types/sde_types/#DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_bistable","page":"SDE Problems","title":"DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_bistable","text":"Bistable chemical reaction network with a semi-stable lower state.\n\n\n\n\n\n","category":"constant"},{"location":"types/sde_types/#DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_bruss","page":"SDE Problems","title":"DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_bruss","text":"Stochastic Brusselator\n\n\n\n\n\n","category":"constant"},{"location":"types/sde_types/#DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_oscilreact","page":"SDE Problems","title":"DiffEqProblemLibrary.SDEProblemLibrary.prob_sde_oscilreact","text":"An oscillatory chemical reaction system\n\n\n\n\n\n","category":"constant"},{"location":"types/nonautonomous_linear_ode/#nonauto_dynamical_prob","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Non-autonomous Linear ODE / Lie Group Problems","text":"","category":"section"},{"location":"types/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Non-autonomous Linear ODE / Lie Group Problems","text":"Non-autonomous linear ODEs show up in a lot of scientific problems where the differential equation lives on a manifold such as Lie Group. In these situtations, specialized solvers can be utilized to enforce physical bounds on the solution and enhance the solving.","category":"page"},{"location":"types/nonautonomous_linear_ode/#Mathematical-Specification-of-a-Non-autonomous-Linear-ODE","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Mathematical Specification of a Non-autonomous Linear ODE","text":"","category":"section"},{"location":"types/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Non-autonomous Linear ODE / Lie Group Problems","text":"These algorithms require a Non-autonomous linear ODE of the form:","category":"page"},{"location":"types/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Non-autonomous Linear ODE / Lie Group Problems","text":"u^prime = A(upt)u","category":"page"},{"location":"types/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Non-autonomous Linear ODE / Lie Group Problems","text":"Where A is an AbstractDiffEqOperator that is  multiplied against u. Many algorithms specialize on the form of A,  such as A being a constant or A being only time-dependent (A(t)). ","category":"page"},{"location":"types/nonautonomous_linear_ode/#Construction","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Construction","text":"","category":"section"},{"location":"types/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Non-autonomous Linear ODE / Lie Group Problems","text":"Creating a non-autonomous linear ODE is the same as an ODEProblem, except f is represented by an AbstractDiffEqOperator (note: this means that any standard ODE solver can also be applied to problems written in this form). As an example:","category":"page"},{"location":"types/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Non-autonomous Linear ODE / Lie Group Problems","text":"function update_func(A,u,p,t)\n    A[1,1] = cos(t)\n    A[2,1] = sin(t)\n    A[1,2] = -sin(t)\n    A[2,2] = cos(t)\nend\nA = DiffEqArrayOperator(ones(2,2),update_func=update_func)\nprob = ODEProblem(A, ones(2), (10, 50.))","category":"page"},{"location":"types/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Non-autonomous Linear ODE / Lie Group Problems","text":"defines a quasi-linear ODE u^prime = A(t)u where the components of A are the given functions. Using that formulation, we can see that the general form is u^prime = A(upt)u, for example:","category":"page"},{"location":"types/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Non-autonomous Linear ODE / Lie Group Problems","text":"function update_func(A,u,p,t)\n    A[1,1] = 0\n    A[2,1] = 1\n    A[1,2] = -2*(1 - cos(u[2]) - u[2]*sin(u[2]))\n    A[2,2] = 0\nend","category":"page"},{"location":"types/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Non-autonomous Linear ODE / Lie Group Problems","text":"has a state-dependent linear operator. Note that many other AbstractDiffEqOperators can be used and DiffEqArrayOperator is just one version that represents A via a matrix (other choices are matrix-free).","category":"page"},{"location":"types/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Non-autonomous Linear ODE / Lie Group Problems","text":"Note that if A is a constant, then it is sufficient to supply A directly without an update_func.","category":"page"},{"location":"types/nonautonomous_linear_ode/#Note-About-Affine-Equations","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Note About Affine Equations","text":"","category":"section"},{"location":"types/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Non-autonomous Linear ODE / Lie Group Problems","text":"Note that the affine equation","category":"page"},{"location":"types/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Non-autonomous Linear ODE / Lie Group Problems","text":"u^prime = A(upt)u + g(upt)","category":"page"},{"location":"types/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group Problems","title":"Non-autonomous Linear ODE / Lie Group Problems","text":"can be written as a linear form by extending the size of the system by one to have a constant term of 1. This is done by extending A with a new row, containing only zeros, and giving this new state an initial value of 1. Then extend A to have a new column containing the values of g(u,p,t). In this way, these types of equations can be handled by these specialized integrators.","category":"page"},{"location":"features/diffeq_operator/#DiffEqOperators","page":"DiffEqOperators","title":"DiffEqOperators","text":"","category":"section"},{"location":"features/diffeq_operator/","page":"DiffEqOperators","title":"DiffEqOperators","text":"The AbstractDiffEqOperator interface is an interface for declaring parts of a differential equation as linear or affine. This then allows the solvers to exploit linearity to achieve maximal performance.","category":"page"},{"location":"features/diffeq_operator/#Using-DiffEqOperators","page":"DiffEqOperators","title":"Using DiffEqOperators","text":"","category":"section"},{"location":"features/diffeq_operator/","page":"DiffEqOperators","title":"DiffEqOperators","text":"AbstractDiffEqOperators act like functions. When defined, A has function calls A(u,p,t) and A(du,u,p,t) that act like A*u. These operators update via a function update_coefficients!(A,u,p,t).","category":"page"},{"location":"features/diffeq_operator/#Constructors","page":"DiffEqOperators","title":"Constructors","text":"","category":"section"},{"location":"features/diffeq_operator/#Wrapping-an-Array:-DiffEqArrayOperator","page":"DiffEqOperators","title":"Wrapping an Array: DiffEqArrayOperator","text":"","category":"section"},{"location":"features/diffeq_operator/","page":"DiffEqOperators","title":"DiffEqOperators","text":"DiffEqArrayOperator is for defining an operator directly from an array. The operator is of the form:","category":"page"},{"location":"features/diffeq_operator/","page":"DiffEqOperators","title":"DiffEqOperators","text":"A(upt)","category":"page"},{"location":"features/diffeq_operator/","page":"DiffEqOperators","title":"DiffEqOperators","text":"for some scalar α and time plus possibly state dependent A. The constructor is:","category":"page"},{"location":"features/diffeq_operator/","page":"DiffEqOperators","title":"DiffEqOperators","text":"DiffEqArrayOperator(A::AbstractMatrix{T},update_func = DEFAULT_UPDATE_FUNC)","category":"page"},{"location":"features/diffeq_operator/","page":"DiffEqOperators","title":"DiffEqOperators","text":"A is the operator array. update_func(A,u,p,t) is the function called by  update_coefficients!(A,u,p,t). If left as its default, then update_func  is trivial which signifies A is a constant.","category":"page"},{"location":"features/diffeq_operator/#AffineDiffEqOperator","page":"DiffEqOperators","title":"AffineDiffEqOperator","text":"","category":"section"},{"location":"features/diffeq_operator/","page":"DiffEqOperators","title":"DiffEqOperators","text":"For As = (A1,A2,...,An) and Bs = (B1,B2,...,Bm) where each of the Ai and Bi are AbstractDiffEqOperators, the following constructor:","category":"page"},{"location":"features/diffeq_operator/","page":"DiffEqOperators","title":"DiffEqOperators","text":"function AffineDiffEqOperator{T}(As,Bs,u_cache=nothing)","category":"page"},{"location":"features/diffeq_operator/","page":"DiffEqOperators","title":"DiffEqOperators","text":"builds an operator L = (A1 + A2 + ... An)*u + B1 + B2 + ... + Bm. u_cache is for designating a type of internal cache for non-allocating evaluation of L(du,u,p,t). If not given, the function L(du,u,p,t) is not available. Note that in solves which exploit this structure, this function call is not necessary. It's only used as the fallback in ODE solvers which were not developed for this structure.","category":"page"},{"location":"features/diffeq_operator/#Formal-Properties-of-DiffEqOperators","page":"DiffEqOperators","title":"Formal Properties of DiffEqOperators","text":"","category":"section"},{"location":"features/diffeq_operator/","page":"DiffEqOperators","title":"DiffEqOperators","text":"These are the formal properties that an AbstractDiffEqOperator should obey for it to work in the solvers.","category":"page"},{"location":"features/diffeq_operator/#AbstractDiffEqOperator-Interface-Description","page":"DiffEqOperators","title":"AbstractDiffEqOperator Interface Description","text":"","category":"section"},{"location":"features/diffeq_operator/","page":"DiffEqOperators","title":"DiffEqOperators","text":"Function call and multiplication: L(du,u,p,t) for inplace and du = L(u,p,t) for out-of-place, meaning L*u and mul!.\nIf the operator is not a constant, update it with (u,p,t). A mutating form, i.e. update_coefficients!(A,u,p,t) that changes the internal coefficients, and a out-of-place form B = update_coefficients(A,u,p,t).\nisconstant(A) trait for whether the operator is constant or not.","category":"page"},{"location":"features/diffeq_operator/#AbstractDiffEqLinearOperator-Interface-Description","page":"DiffEqOperators","title":"AbstractDiffEqLinearOperator Interface Description","text":"","category":"section"},{"location":"features/diffeq_operator/","page":"DiffEqOperators","title":"DiffEqOperators","text":"AbstractDiffEqLinearOperator <: AbstractDiffEqOperator\nCan absorb under multiplication by a scalar. In all algorithms things like dt*L show up all the time, so the linear operator must be able to absorb such constants.\nisconstant(A) trait for whether the operator is constant or not.\nOptional: diagonal, symmetric, etc traits from LinearMaps.jl.\nOptional: exp(A). Required for simple exponential integration.\nOptional: expv(A,u,t) = exp(t*A)*u and expv!(v,A::DiffEqOperator,u,t) Required for sparse-saving exponential integration.\nOptional: factorizations. ldiv!, factorize et. al. This is only required for algorithms which use the factorization of the operator (Crank-Nicolson), and only for when the default linear solve is used.","category":"page"},{"location":"analysis/parameterized_functions/#paremeterized_functions","page":"ParameterizedFunctions","title":"ParameterizedFunctions","text":"","category":"section"},{"location":"analysis/parameterized_functions/#Installation","page":"ParameterizedFunctions","title":"Installation","text":"","category":"section"},{"location":"analysis/parameterized_functions/","page":"ParameterizedFunctions","title":"ParameterizedFunctions","text":"This functionality does not come standard with DifferentialEquations.jl. To use this functionality, you must install ParameterizedFunctions.jl:","category":"page"},{"location":"analysis/parameterized_functions/","page":"ParameterizedFunctions","title":"ParameterizedFunctions","text":"]add ParameterizedFunctions\nusing ParameterizedFunctions","category":"page"},{"location":"analysis/parameterized_functions/#Function-Definition-Macros","page":"ParameterizedFunctions","title":"Function Definition Macros","text":"","category":"section"},{"location":"analysis/parameterized_functions/","page":"ParameterizedFunctions","title":"ParameterizedFunctions","text":"DifferentialEquations.jl provides a set of macros for more easily and legibly defining your differential equations. It exploits the standard notation for mathematically writing differential equations and the notation for \"punching differential equations into the computer\"; effectively doing the translation step for you. This is best shown by an example. Say we want to solve the ROBER model. Using the @ode_def macro from ParameterizedFunctions.jl, we can do this by writing:","category":"page"},{"location":"analysis/parameterized_functions/","page":"ParameterizedFunctions","title":"ParameterizedFunctions","text":"using ParameterizedFunctions\nf = @ode_def begin\n  dy₁ = -k₁*y₁+k₃*y₂*y₃\n  dy₂ =  k₁*y₁-k₂*y₂^2-k₃*y₂*y₃\n  dy₃ =  k₂*y₂^2\nend k₁ k₂ k₃","category":"page"},{"location":"analysis/parameterized_functions/","page":"ParameterizedFunctions","title":"ParameterizedFunctions","text":"This looks just like pseudocode! The macro will expand this to the \"standard form\", i.e. the ugly computer form:","category":"page"},{"location":"analysis/parameterized_functions/","page":"ParameterizedFunctions","title":"ParameterizedFunctions","text":"function f(du,u,p,t)\n  du[1] = -p[1]*u[1] + p[3]*u[2]*u[3]\n  du[2] = p[1]*u[1] - p[2]*u[2]^2 - p[3]*u[2]*u[3]\n  du[3] = p[2]*u[2]^2\nend","category":"page"},{"location":"analysis/parameterized_functions/","page":"ParameterizedFunctions","title":"ParameterizedFunctions","text":"Note that one doesn't need to use numbered variables: DifferentialEquations.jl will number the variables for you. For example, the following defines the function for the Lotka-Volterra model, with full Unicode support to boot:","category":"page"},{"location":"analysis/parameterized_functions/","page":"ParameterizedFunctions","title":"ParameterizedFunctions","text":"f = @ode_def begin\n  d🐁  = α*🐁  - β*🐁*🐈\n  d🐈 = -γ*🐈 + δ*🐁*🐈\nend α β γ δ","category":"page"},{"location":"analysis/parameterized_functions/#Limitations","page":"ParameterizedFunctions","title":"Limitations","text":"","category":"section"},{"location":"analysis/parameterized_functions/","page":"ParameterizedFunctions","title":"ParameterizedFunctions","text":"The macro is a Domain-Specific Language (DSL) and thus has different internal semantics than standard Julia functions. In particular:","category":"page"},{"location":"analysis/parameterized_functions/","page":"ParameterizedFunctions","title":"ParameterizedFunctions","text":"Control sequences and conditionals (while, for, if) will not work in the macro.\nIntermediate calculations (lines that don't start with d_) are incompatible with the Jacobian etc. calculations.\nThe macro has to use t for the independent variable.","category":"page"},{"location":"analysis/parameterized_functions/#Extra-Optimizations","page":"ParameterizedFunctions","title":"Extra Optimizations","text":"","category":"section"},{"location":"analysis/parameterized_functions/","page":"ParameterizedFunctions","title":"ParameterizedFunctions","text":"Because the ParameterizedFunction defined by the macro holds the definition at a symbolic level, optimizations are provided by SymEngine. Using the symbolic calculator, in-place functions for many things such as Jacobians, Hessians, etc. are symbolically pre-computed. In addition, functions for the inverse Jacobian, Hessian, etc. are also pre-computed. In addition, parameter gradients and Jacobians are also used.","category":"page"},{"location":"analysis/parameterized_functions/","page":"ParameterizedFunctions","title":"ParameterizedFunctions","text":"Normally these will be computed fast enough that the user doesn't have to worry. However, in some cases you may want to restrict the number of functions (or get rid of a warning). For more information, please see the ParameterizedFunctions.jl documentation.","category":"page"},{"location":"features/callback_library/#callback_library","page":"Callback Library","title":"Callback Library","text":"","category":"section"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"DiffEqCallbacks.jl provides a library of various helpful callbacks which can be used with any component solver which implements the callback interface. It adds the following callbacks which are available to users of DifferentialEquations.jl.","category":"page"},{"location":"features/callback_library/#Manifold-Conservation-and-Projection","page":"Callback Library","title":"Manifold Conservation and Projection","text":"","category":"section"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"In many cases, you may want to declare a manifold on which a solution lives. Mathematically, a manifold M is defined by a function g as the set of points where g(u)=0. An embedded manifold can be a lower dimensional object which constrains the solution. For example, g(u)=E(u)-C where E is the energy of the system in state u, meaning that the energy must be constant (energy preservation). Thus by defining the manifold the solution should live on, you can retain desired properties of the solution.","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"It is a consequence of convergence proofs both in the deterministic and stochastic cases that post-step projection to manifolds keep the same convergence rate (stochastic requires a truncation in the proof, details details), thus any algorithm can be easily extended to conserve properties. If the solution is supposed to live on a specific manifold or conserve such property, this guarantees the conservation law without modifying the convergence properties.","category":"page"},{"location":"features/callback_library/#Constructor","page":"Callback Library","title":"Constructor","text":"","category":"section"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"ManifoldProjection(g; nlsolve=NLSOLVEJL_SETUP(), save=true, autonomous=numargs(g)==2, nlopts=Dict{Symbol,Any}())","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"g: The residual function for the manifold. This is an inplace function of form g(resid, u) or g(resid, u, p, t) which writes to the residual resid the  difference from the manifold components. Here, it is assumed that resid is of the same shape as u.\nnlsolve: A nonlinear solver as defined in the nlsolve format.\nsave: Whether to do the save after the callback is applied. Standard saving is unchanged.\nautonomous: Whether g is an autonomous function of the form g(resid, u).\nnlopts: Optional arguments to nonlinear solver which can be any of the NLsolve keywords.","category":"page"},{"location":"features/callback_library/#Example","page":"Callback Library","title":"Example","text":"","category":"section"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"Here we solve the harmonic oscillator:","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"u0 = ones(2)\nfunction f(du,u,p,t)\n  du[1] = u[2]\n  du[2] = -u[1]\nend\nprob = ODEProblem(f,u0,(0.0,100.0))","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"However, this problem is supposed to conserve energy, and thus we define our manifold to conserve the sum of squares:","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"function g(resid,u,p,t)\n  resid[1] = u[2]^2 + u[1]^2 - 2\n  resid[2] = 0\nend","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"To build the callback, we just call","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"cb = ManifoldProjection(g)","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"Using this callback, the Runge-Kutta method Vern7 conserves energy. Note that the standard saving occurs after the step and before the callback, and thus we set save_everystep=false to turn off all standard saving and let the callback save after the projection is applied.","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"sol = solve(prob,Vern7(),save_everystep=false,callback=cb)\n@test sol[end][1]^2 + sol[end][2]^2 ≈ 2","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"(Image: manifold_projection)","category":"page"},{"location":"features/callback_library/#Saveat-Warning","page":"Callback Library","title":"Saveat Warning","text":"","category":"section"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"Note that the ManifoldProjection callback modifies the endpoints of the integration intervals and thus breaks assumptions of internal interpolations. Because of this, the values for given by saveat will not be order-matching. However, the interpolation error can be proportional to the change by the projection, so if the projection is making small changes then one is still safe. However, if there are large changes from each projection, you should consider only saving at stopping/projection times. To do this, set tstops to the same values as saveat. There is a performance hit by doing so because now the integrator is forced to stop at every saving point, but this is guerenteed to match the order of the integrator even with the ManifoldProjection.","category":"page"},{"location":"features/callback_library/#AutoAbstol","page":"Callback Library","title":"AutoAbstol","text":"","category":"section"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"Many problem solving environments such as MATLAB provide a way to automatically adapt the absolute tolerance to the problem. This helps the solvers automatically \"learn\" what appropriate limits are. Via the callback interface, DiffEqCallbacks.jl implements a callback AutoAbstol which has the same behavior as the MATLAB implementation, that is the absolute tolerance starts and at each iteration it is set to the maximum value that the state has thus far reached times the relative tolerance. If init_curmax is zero, then the initial value is determined by the abstol of the solver. Otherwise this is the initial value for the current maximum abstol.","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"To generate the callback, use the constructor:","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"AutoAbstol(save=true;init_curmax=0.0)","category":"page"},{"location":"features/callback_library/#PositiveDomain","page":"Callback Library","title":"PositiveDomain","text":"","category":"section"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"Especially in biology and other natural sciences, a desired property of dynamical systems is the positive invariance of the positive cone, i.e. non-negativity of variables at time t_0 ensures their non-negativity at times t geq t_0 for which the solution is defined. However, even if a system satisfies this property mathematically it can be difficult for ODE solvers to ensure it numerically, as these MATLAB examples show.","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"In order to deal with this problem one can specify isoutofdomain=(u,p,t) -> any(x -> x < 0, u) as additional solver option, which will reject any step that leads to non-negative values and reduce the next time step. However, since this approach only rejects steps and hence calculations might be repeated multiple times until a step is accepted, it can be computationally expensive.","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"Another approach is taken by a PositiveDomain callback in DiffEqCallbacks.jl, which is inspired by Shampine's et al. paper about non-negative ODE solutions. It reduces the next step by a certain scale factor until the extrapolated value at the next time point is non-negative with a certain tolerance. Extrapolations are cheap to compute but might be inaccurate, so if a time step is changed it is additionally reduced by a safety factor of 0.9. Since extrapolated values are only non-negative up to a certain tolerance and in addition actual calculations might lead to negative values, also any negative values at the current time point are set to 0. Hence by this callback non-negative values at any time point are ensured in a computationally cheap way, but the quality of the solution depends on how accurately extrapolations approximate next time steps.","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"Please note that the system should be defined also outside the positive domain, since even with these approaches negative variables might occur during the calculations. Moreover, one should follow Shampine's et. al. advice and set the derivative x_i of a negative component x_i to max 0 f_i(x t), where t denotes the current time point with state vector x and f_i is the i-th component of function f in an ODE system x = f(x t).","category":"page"},{"location":"features/callback_library/#Constructor-2","page":"Callback Library","title":"Constructor","text":"","category":"section"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"PositiveDomain(u=nothing; save=true, abstol=nothing, scalefactor=nothing)","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"u: A prototype of the state vector of the integrator. A copy of it is saved and extrapolated values are written to it. If it is not specified every application of the callback allocates a new copy of the state vector.\nsave: Whether to do the standard saving (applied after the callback).\nabstol: Tolerance up to which negative extrapolated values are accepted. Element-wise tolerances are allowed. If it is not specified every application of the callback uses the current absolute tolerances of the integrator.\nscalefactor: Factor by which an unaccepted time step is reduced. If it is not specified time steps are halved.","category":"page"},{"location":"features/callback_library/#GeneralDomain","page":"Callback Library","title":"GeneralDomain","text":"","category":"section"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"A GeneralDomain callback in DiffEqCallbacks.jl generalizes the concept of a PositiveDomain callback to arbitrary domains. Domains are specified by in-place functions g(u, resid) or g(t, u, resid) that calculate residuals of a state vector u at time t relative to that domain. As for PositiveDomain, steps are accepted if residuals of the extrapolated values at the next time step are below a certain tolerance. Moreover, this callback is automatically coupled with a ManifoldProjection that keeps all calculated state vectors close to the desired domain, but in contrast to a PositiveDomain callback the nonlinear solver in a ManifoldProjection can not guarantee that all state vectors of the solution are actually inside the domain. Thus a PositiveDomain callback should in general be preferred.","category":"page"},{"location":"features/callback_library/#Constructor-3","page":"Callback Library","title":"Constructor","text":"","category":"section"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"function GeneralDomain(g, u=nothing; nlsolve=NLSOLVEJL_SETUP(), save=true,\n                       abstol=nothing, scalefactor=nothing, autonomous=numargs(g)==2,\n                       nlopts=Dict(:ftol => 10*eps()))","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"g: The residual function for the domain. This is an inplace function of form g(resid, u, p, t) which writes to the residual the difference from the domain.\nu: A prototype of the state vector of the integrator and the residuals. Two copies of it are saved, and extrapolated values and residuals are written to them. If it is not specified every application of the callback allocates two new copies of the state vector.\nnlsolve: A nonlinear solver as defined in the nlsolve format which is passed to a ManifoldProjection.\nsave: Whether to do the standard saving (applied after the callback).\nabstol: Tolerance up to which residuals are accepted. Element-wise tolerances are allowed. If it is not specified every application of the callback uses the current absolute tolerances of the integrator.\nscalefactor: Factor by which an unaccepted time step is reduced. If it is not specified time steps are halved.\nautonomous: Whether g is an autonomous function of the form g(u, resid).\nnlopts: Optional arguments to nonlinear solver of a ManifoldProjection which can be any of the NLsolve keywords. The default value of ftol = 10*eps() ensures that convergence is only declared if the infinite norm of residuals is very small and hence the state vector is very close to the domain.","category":"page"},{"location":"features/callback_library/#Stepsize-Limiters","page":"Callback Library","title":"Stepsize Limiters","text":"","category":"section"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"In many cases there is a known maximal stepsize for which the computation is stable and produces correct results. For example, in hyperbolic PDEs one normally needs to ensure that the stepsize stays below some Delta t_FE determined by the CFL condition. For nonlinear hyperbolic PDEs this limit can be a function dtFE(u,p,t) which changes throughout the computation. The stepsize limiter lets you pass a function which will adaptively limit the stepsizes to match these constraints.","category":"page"},{"location":"features/callback_library/#Constructor-4","page":"Callback Library","title":"Constructor","text":"","category":"section"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"StepsizeLimiter(dtFE;safety_factor=9//10,max_step=false,cached_dtcache=0.0)","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"dtFE: The function for the maximal timestep, called as dtFE(u,p,t) using the previous values of u, p, and t.\nsafety_factor: The factor below the true maximum that will be stepped to which defaults to 9//10.\nmax_step: Makes every step equal to safety_factor*dtFE(u,p,t) when the solver is set to adaptive=false.\ncached_dtcache: Should be set to match the type for time when not using Float64 values.","category":"page"},{"location":"features/callback_library/#FunctionCallingCallback","page":"Callback Library","title":"FunctionCallingCallback","text":"","category":"section"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"The function calling callback lets you define a function func(u,t,integrator) which gets calls at the time points of interest. The constructor is:","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"  FunctionCallingCallback(func;\n                 funcat=Vector{Float64}(),\n                 func_everystep=isempty(funcat),\n                 func_start = true,\n                 tdir=1)","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"func(u, t, integrator) is the function to be called.\nfuncat values that the function is sure to be evaluated at.\nfunc_everystep whether to call the function after each integrator step.\nfunc_start whether the function is called the initial condition.\ntdir should be sign(tspan[end]-tspan[1]). It defaults to 1 and should   be adapted if tspan[1] > tspan[end].","category":"page"},{"location":"features/callback_library/#saving_callback","page":"Callback Library","title":"SavingCallback","text":"","category":"section"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"The saving callback lets you define a function save_func(u, t, integrator) which returns quantities of interest that shall be saved.","category":"page"},{"location":"features/callback_library/#Constructor-5","page":"Callback Library","title":"Constructor","text":"","category":"section"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"SavingCallback(save_func, saved_values::SavedValues;\n               saveat=Vector{eltype(saved_values.t)}(),\n               save_everystep=isempty(saveat),\n               tdir=1)","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"save_func(u, t, integrator) returns the quantities which shall be saved. Note that this should allocate the output (not as a view to u).\nsaved_values::SavedValues is the types that save_func will return, i.e. save_func(u, t, integrator)::savevalType. It's specified via SavedValues(typeof(t),savevalType), i.e. give the type for time and the type that save_func will output (or higher compatible type).\nsaveat mimicks saveat in solve from solve.\nsave_everystep mimicks save_everystep from solve.\nsave_start mimicks save_start from solve.\ntdir should be sign(tspan[end]-tspan[1]). It defaults to 1 and should be adapted if tspan[1] > tspan[end].","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"The outputted values are saved into saved_values. Time points are found via saved_values.t and the values are saved_values.saveval.","category":"page"},{"location":"features/callback_library/#Example-2","page":"Callback Library","title":"Example","text":"","category":"section"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"In this example we will solve a matrix equation and at each step save a tuple of values which contains the current trace and the norm of the matrix. We build the SavedValues cache to use Float64 for time and Tuple{Float64,Float64} for the saved values, and then call the solver with the callback.","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"using DiffEqCallbacks, OrdinaryDiffEq, LinearAlgebra\nprob = ODEProblem((du,u,p,t) -> du .= u, rand(4,4), (0.0,1.0))\nsaved_values = SavedValues(Float64, Tuple{Float64,Float64})\ncb = SavingCallback((u,t,integrator)->(tr(u),norm(u)), saved_values)\nsol = solve(prob, Tsit5(), callback=cb)\n\nprint(saved_values.saveval)\n#=\nTuple{Float64,Float64}[(2.23186, 2.49102), (2.46675, 2.75318), (3.16138, 3.52847), (4.42011, 4.93337), (6.06683, 6.77129)]\n=#","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"Note that the values are retrieved from the cache as .saveval, and the time points are found as .t. If we want to control the saved times, we use saveat in the callback. The save controls like saveat act analogously to how they act in the solve function.","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"saved_values = SavedValues(Float64, Tuple{Float64,Float64})\ncb = SavingCallback((u,t,integrator)->(tr(u),norm(u)), saved_values, saveat=0.0:0.1:1.0)\nsol = solve(prob, Tsit5(), callback=cb)\nprint(saved_values.saveval)\nprint(saved_values.t)\n\n#=\nTuple{Float64,Float64}[(2.23186, 2.49102), (2.46659, 2.753), (2.726, 3.04254), (3.0127, 3.36253),\n(3.32955, 3.71617), (3.67972, 4.107), (4.06672, 4.53893), (4.49442, 5.0163), (4.9671, 5.54387),\n(5.48949, 6.12692), (6.06683, 6.77129)]\n[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n=#","category":"page"},{"location":"features/callback_library/#PresetTimeCallback","page":"Callback Library","title":"PresetTimeCallback","text":"","category":"section"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"PresetTimeCallback is a callback that adds callback affect! calls at preset times. No playing around with tstops or anything is required: this callback adds the triggers for you to make it automatic.","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"PresetTimeCallback(tstops,user_affect!;\n                            initialize = DiffEqBase.INITIALIZE_DEFAULT,\n                            filter_tstops = true,\n                            kwargs...)","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"tstops: the times for the affect! to trigger at.\nuser_affect!: an affect!(integrator) function to use at the time points.\nfilter_tstops: Whether to filter out tstops beyond the end of the integration timespan. Defaults to true. If false, then tstops can extend the interval of integration.","category":"page"},{"location":"features/callback_library/#IterativeCallback","page":"Callback Library","title":"IterativeCallback","text":"","category":"section"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"IterativeCallback is a callback to be used to iteratively apply some effect. For example, if given the first effect at t₁, you can define t₂ to apply the next effect.","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"A IterativeCallback is constructed as follows:","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"function IterativeCallback(time_choice, user_affect!,tType = Float64;\n                           initial_affect = false, kwargs...)","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"where time_choice(integrator) determines the time of the next callback and user_affect! is the effect applied to the integrator at the stopping points. If nothing is returned for the time choice then the iterator ends. initial_affect is whether to apply the affect at t=0 which defaults to false. kwargs are keyword arguments accepted by the DiscreteCallback constructor.","category":"page"},{"location":"features/callback_library/#PeriodicCallback","page":"Callback Library","title":"PeriodicCallback","text":"","category":"section"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"PeriodicCallback can be used when a function should be called periodically in terms of integration time (as opposed to wall time), i.e. at t = tspan[1], t = tspan[1] + Δt, t = tspan[1] + 2Δt, and so on. This callback can, for example, be used to model a digital controller for an analog system, running at a fixed rate.","category":"page"},{"location":"features/callback_library/#Constructor-6","page":"Callback Library","title":"Constructor","text":"","category":"section"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"PeriodicCallback(f, Δt::Number; initial_affect = false, kwargs...)","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"where f is the function to be called periodically, Δt is the period, initial_affect is whether to apply the affect at t=0 which defaults to false, and kwargs are keyword arguments accepted by the DiscreteCallback constructor (see the DiscreteCallback section).","category":"page"},{"location":"features/callback_library/#TerminateSteadyState","page":"Callback Library","title":"TerminateSteadyState","text":"","category":"section"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"TerminateSteadyState can be used to solve the problem for the steady-state by running the solver until the derivatives of the problem converge to 0 or tspan[2] is reached. This is an alternative approach to root finding (see the Steady State Solvers section). The constructor of this callback is:","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"TerminateSteadyState(abstol = 1e-8, reltol = 1e-6, test = allDerivPass)","category":"page"},{"location":"features/callback_library/","page":"Callback Library","title":"Callback Library","text":"where abstol and reltol are the absolute and relative tolerance, respectively. These tolerances may be specified as scalars or as arrays of the same length as the states of the problem. test represents the function that evaluates the condition for termination. The default condition is that all derivatives should become smaller than abstol and the states times reltol. The user can pass any other function to implement a different termination condition. Such function should take four arguments: integrator (see Integrator Interface for details), abstol and reltol.","category":"page"},{"location":"tutorials/rode_example/#rode_example","page":"Random Ordinary Differential Equations","title":"Random Ordinary Differential Equations","text":"","category":"section"},{"location":"tutorials/rode_example/","page":"Random Ordinary Differential Equations","title":"Random Ordinary Differential Equations","text":"This tutorial will introduce you to the functionality for solving RODEs. Other introductions can be found by checking out DiffEqTutorials.jl.","category":"page"},{"location":"tutorials/rode_example/","page":"Random Ordinary Differential Equations","title":"Random Ordinary Differential Equations","text":"note: Note\nThis tutorial assumes you have read the Ordinary Differential Equations tutorial.","category":"page"},{"location":"tutorials/rode_example/#Example-1:-Scalar-RODEs","page":"Random Ordinary Differential Equations","title":"Example 1: Scalar RODEs","text":"","category":"section"},{"location":"tutorials/rode_example/","page":"Random Ordinary Differential Equations","title":"Random Ordinary Differential Equations","text":"In this example we will solve the equation","category":"page"},{"location":"tutorials/rode_example/","page":"Random Ordinary Differential Equations","title":"Random Ordinary Differential Equations","text":"du = f(uptW)dt","category":"page"},{"location":"tutorials/rode_example/","page":"Random Ordinary Differential Equations","title":"Random Ordinary Differential Equations","text":"where f(uptW)=2usin(W) and W(t) is a Wiener process (Gaussian process).","category":"page"},{"location":"tutorials/rode_example/","page":"Random Ordinary Differential Equations","title":"Random Ordinary Differential Equations","text":"using DifferentialEquations\nfunction f(u,p,t,W)\n  2u*sin(W)\nend\nu0 = 1.00\ntspan = (0.0,5.0)\nprob = RODEProblem(f,u0,tspan)\nsol = solve(prob,RandomEM(),dt=1/100)","category":"page"},{"location":"tutorials/rode_example/","page":"Random Ordinary Differential Equations","title":"Random Ordinary Differential Equations","text":"(Image: intro_rode)","category":"page"},{"location":"tutorials/rode_example/","page":"Random Ordinary Differential Equations","title":"Random Ordinary Differential Equations","text":"The random process defaults to a Gaussian/Wiener process, so there is nothing else required here! See the documentation on NoiseProcesses for details on how to define other noise proceses.","category":"page"},{"location":"tutorials/rode_example/#Example-2:-Systems-of-RODEs","page":"Random Ordinary Differential Equations","title":"Example 2: Systems of RODEs","text":"","category":"section"},{"location":"tutorials/rode_example/","page":"Random Ordinary Differential Equations","title":"Random Ordinary Differential Equations","text":"As with the other problem types, there is an in-place version which is more efficient for systems. The signature is f(du,u,p,t,W). For example,","category":"page"},{"location":"tutorials/rode_example/","page":"Random Ordinary Differential Equations","title":"Random Ordinary Differential Equations","text":"using DifferentialEquations\nfunction f(du,u,p,t,W)\n  du[1] = 2u[1]*sin(W[1] - W[2])\n  du[2] = -2u[2]*cos(W[1] + W[2])\nend\nu0 = [1.00;1.00]\ntspan = (0.0,5.0)\nprob = RODEProblem(f,u0,tspan)\nsol = solve(prob,RandomEM(),dt=1/100)","category":"page"},{"location":"tutorials/rode_example/","page":"Random Ordinary Differential Equations","title":"Random Ordinary Differential Equations","text":"(Image: rode_system)","category":"page"},{"location":"tutorials/rode_example/","page":"Random Ordinary Differential Equations","title":"Random Ordinary Differential Equations","text":"By default, the size of the noise process matches the size of u0. However, you can use the rand_prototype keyword to explicitly set the size of the random process:","category":"page"},{"location":"tutorials/rode_example/","page":"Random Ordinary Differential Equations","title":"Random Ordinary Differential Equations","text":"function f(du,u,p,t,W)\n  du[1] = -2W[3]*u[1]*sin(W[1] - W[2])\n  du[2] = -2u[2]*cos(W[1] + W[2])\nend\nu0 = [1.00;1.00]\ntspan = (0.0,5.0)\nprob = RODEProblem(f,u0,tspan,rand_prototype=zeros(3))\nsol = solve(prob,RandomEM(),dt=1/100)","category":"page"},{"location":"tutorials/rode_example/","page":"Random Ordinary Differential Equations","title":"Random Ordinary Differential Equations","text":"(Image: noise_choice)","category":"page"},{"location":"solvers/sde_solve/#sde_solve","page":"SDE Solvers","title":"SDE Solvers","text":"","category":"section"},{"location":"solvers/sde_solve/#Recommended-Methods","page":"SDE Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"For most Ito diagonal and scalar noise problems where a good amount of accuracy is required and mild stiffness may be an issue, the SOSRI algorithm should do well. If the problem has additive noise, then SOSRA will be the optimal algorithm. At low tolerances (<1e-4?) SRA3 will be more efficient, though SOSRA is more robust to stiffness. For commutative noise, RKMilCommute is a strong order 1.0 method which utilizes the commutivity property to greatly speed up the Wiktorsson approximation and can choose between Ito and Stratonovich. For non-commutative noise, difficult problems usually require adaptive time stepping in order to be efficient. In this case, LambaEM and LambaEulerHeun are adaptive and handle general non-diagonal problems (for Ito and Stratonovich interpretations respectively). If adaptivity isn't necessary, the EM and EulerHeun are good choices (for Ito and Stratonovich interpretations respectively).","category":"page"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"For stiff problems with additive noise, the high order adaptive method SKenCarp is highly preferred and will solve problems with similar efficiency as ODEs. If possible, stiff problems should be converted to make use of this additive noise solver. If the noise term is large/stiff, then the split-step methods are required in order for the implicit methods to be stable. For Ito in this case, use ISSEM and for Stratonovich use ISSEulerHeun. These two methods can handle any noise form.","category":"page"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"If the noise term is not too large, for stiff problems with diagonal noise, ImplicitRKMil is the most efficient method and can choose between Ito and Stratonovich. For each of the theta methods, the parameter theta can be chosen. The default is theta=1/2 which will not dampen numerical oscillations and thus is symmetric (and almost symplectic) and will lead to less error when noise is sufficiently small. However, theta=1/2 is not L-stable in the drift term, and thus one can receive more stability (L-stability in the drift term) with theta=1, but with a tradeoff of error efficiency in the low noise case. In addition, the option symplectic=true will turns these methods into an implicit Midpoint extension which is symplectic in distribution but has an accuracy tradeoff.","category":"page"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"If only an estimation for the expected value of the solution is required, i.e., if one is only interested in an accurate draw from the distribution induced by a given SDE, the use of high weak order solvers is recommended. Specifically, DRI1 is preferred for a high number of Wiener processes. The weak stochastic Runge-Kutta solvers with weak order 2 due to Roessler are adaptive. All other high weak order solvers currently require a fixed step size.","category":"page"},{"location":"solvers/sde_solve/#Special-Noise-Forms","page":"SDE Solvers","title":"Special Noise Forms","text":"","category":"section"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"Some solvers are for specialized forms of noise. Diagonal noise is the default setup. Non-diagonal noise is specified via setting noise_rate_prototype to a matrix in the SDEProblem type. A special form of non-diagonal noise, commutative noise, occurs when the noise satisfies the following condition:","category":"page"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"sum_i=1^d g_ij_1(tx) fracpartial g_kj_2(tx)partial x_i = sum_i=1^d g_ij_2(tx) fracpartial g_kj_1(tx)partial x_i","category":"page"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"for every j_1j_2 and k. Additive noise is when g(tu)=g(t), i.e. is independent of u. Multiplicative noise is g_i(tu)=a_i u.","category":"page"},{"location":"solvers/sde_solve/#Iterated-Integral-Approximations","page":"SDE Solvers","title":"Iterated Integral Approximations","text":"","category":"section"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"The difficulty of higher strong order integrators is the resolution of the stochastic iterated integral expressions, i.e.","category":"page"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"iint dW_t dZ_s","category":"page"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"Most methods are specific noise cases, like diagonal noise or commutative noise, because of how this iterated integral approximation is performed within the method. However, the methods for general noise, like RKMilGeneral, perform a direct approximation of the iterated integrals. For those methods, the algorithms have an ii_approx keyword argument which allows for specifying the method for the approximation. The choices are:","category":"page"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"IICommutative: a simplification of the integral which assumes the noise commutativity property. If used on a non-commutative noise problem this will limit the strong convergence to 0.5.\nIIWiktorsson: approximation of the due to Wiktorsson with a approximation of the truncation term","category":"page"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"Example: RKMilGeneral(ii_approx=IIWiktorsson()).","category":"page"},{"location":"solvers/sde_solve/#Special-Keyword-Arguments","page":"SDE Solvers","title":"Special Keyword Arguments","text":"","category":"section"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"save_noise: Determines whether the values of W are saved whenever the timeseries is saved. Defaults to true.\ndelta: The delta adaptivity parameter for the natural error estimator. Determines the balance between drift and diffusion error. For more details, see the publication.\nseed: Sets the seed for the random number generator. This overrides any seed set in the SDEProblem.","category":"page"},{"location":"solvers/sde_solve/#Full-List-of-Methods","page":"SDE Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"solvers/sde_solve/#StochasticDiffEq.jl","page":"SDE Solvers","title":"StochasticDiffEq.jl","text":"","category":"section"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"Each of the StochasticDiffEq.jl solvers come with a linear interpolation. Orders are given in terms of strong order.","category":"page"},{"location":"solvers/sde_solve/#Nonstiff-Methods","page":"SDE Solvers","title":"Nonstiff Methods","text":"","category":"section"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"EM- The Euler-Maruyama method. Strong Order 0.5 in the Ito sense. Has an optional argument split=true for controlling step splitting. When splitting is enabled, the stability with large diffusion eigenvalues is improved. Can handle all forms of noise, including non-diagonal, scalar, and colored noise. Fixed time step only.†\nLambaEM- A modified Euler-Maruyama method with adaptive time stepping with an error estimator based on Lamba and Rackauckas. Has an optional argument split=true for controlling step splitting. When splitting is enabled, the stability with   large diffusion eigenvalues is improved. Strong Order 0.5 in the Ito sense. Can handle all forms of noise, including non-diagonal, scalar, and colored noise.†\nEulerHeun - The Euler-Heun method. Strong Order 0.5 in the Stratonovich sense. Can handle all forms of noise, including non-diagonal, scalar, and colored noise. Fixed time step only.†\nLambaEulerHeun - A modified Euler-Heun method with adaptive time stepping with an error estimator based on Lamba due to Rackauckas. Strong order 0.5 in the Stratonovich sense. Can handle all forms of noise, including non-diagonal, scalar, and colored noise.†\nRKMil - An explicit Runge-Kutta discretization of the strong order 1.0 Milstein method. Defaults to solving the Ito problem, but RKMil(interpretation=:Stratonovich) makes it solve the Stratonovich problem. Only handles scalar and diagonal noise.†\nRKMilCommute - An explicit Runge-Kutta discretization of the strong order 1.0 Milstein method for commutative noise problems. Defaults to solving the Ito problem, but RKMilCommute(interpretation=:Stratonovich) makes it solve the Stratonovich problem. Uses a 1.5/2.0 error estimate for adaptive time stepping.†\nRKMilGeneral(;interpretation=:Ito, ii_approx=IIWiktorsson() - An explicit  Runge-Kutta discretization of the strong order 1.0 Milstein method for general  non-commutative noise problems. Allows for a choice of interpretation between :Ito and :Stratonovich. Allows for a choice of iterated integral approximation.\nWangLi3SMil_A - fixed step-size explicit 3-stage Milstein methods for Ito problem with strong and weak order 1.0\nWangLi3SMil_B - fixed step-size explicit 3-stage Milstein methods for Ito problem with strong and weak order 1.0\nWangLi3SMil_C - fixed step-size explicit 3-stage Milstein methods for Ito problem with strong and weak order 1.0\nWangLi3SMil_D - fixed step-size explicit 3-stage Milstein methods for Ito problem with strong and weak order 1.0\nWangLi3SMil_E - fixed step-size explicit 3-stage Milstein methods for Ito problem with strong and weak order 1.0\nWangLi3SMil_F - fixed step-size explicit 3-stage Milstein methods for Ito problem with strong and weak order 1.0\nSRA - Adaptive strong order 1.5 methods for additive Ito and Stratonovich SDEs. Default tableau is for SRA1. Can handle diagonal, non-diagonal and scalar additive noise.\nSRI - Adaptive strong order 1.5 methods for diagonal/scalar Ito SDEs. Default tableau is for SRIW1.\nSRIW1 - Adaptive strong order 1.5 and weak order 2.0 for diagonal/scalar Ito SDEs.†\nSRIW2 - Adaptive strong order 1.5 and weak order 3.0 for diagonal/scalar Ito SDEs.†\nSOSRI - Stability-optimized adaptive strong order 1.5 and weak order 2.0 for diagonal/scalar Ito SDEs. Stable at high tolerances and robust to stiffness.†\nSOSRI2 - Stability-optimized adaptive strong order 1.5 and weak order 2.0 for diagonal/scalar Ito SDEs. Stable at high tolerances and robust to stiffness.†\nSRA1 - Adaptive strong order 1.5 for additive Ito and Stratonovich SDEs with weak order 2. Can handle diagonal, non-diagonal, and scalar additive noise.†\nSRA2 - Adaptive strong order 1.5 for additive Ito and Stratonovich SDEs with weak order 2. Can handle diagonal, non-diagonal, and scalar additive noise.†\nSRA3 - Adaptive strong order 1.5 for additive Ito and Stratonovich SDEs with weak order 3. Can handle non-diagonal and scalar additive noise.†\nSOSRA - A stability-optimized adaptive SRA. Strong order 1.5 for additive Ito and Stratonovich SDEs with weak order 2. Can handle diagonal, non-diagonal, and scalar additive noise. Stable at high tolerances and robust to stiffness.†\nSOSRA2 - A stability-optimized adaptive SRA. Strong order 1.5 for additive Ito and Stratonovich SDEs with weak order 2. Can handle diagonal, non-diagonal, and scalar additive noise. Stable at high tolerances and robust to stiffness.†","category":"page"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"Example usage:","category":"page"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"sol = solve(prob,SRIW1())","category":"page"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"3-stage Milstein Methods WangLi3SMil_A, WangLi3SMil_B, WangLi3SMil_D, WangLi3SMil_E and WangLi3SMil_F are currently implemented for 1-dimensional and diagonal noise only.","category":"page"},{"location":"solvers/sde_solve/#Tableau-Controls","page":"SDE Solvers","title":"Tableau Controls","text":"","category":"section"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"For SRA and SRI, the following option is allowed:","category":"page"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"tableau: The tableau for an :SRA or :SRI algorithm. Defaults to SRIW1 or SRA1.","category":"page"},{"location":"solvers/sde_solve/#S-ROCK-Methods","page":"SDE Solvers","title":"S-ROCK Methods","text":"","category":"section"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"SROCK1 - is a fixed step size stabilized explicit method for stiff problems. Defaults to solving th Ito problem but SROCK1(interpretation=:Stratonovich) can make it solve the Stratonovich problem. Strong order of convergence is 0.5 and weak order 1, but is optimised to get order 1 in case os scalar/diagonal noise.\nSROCKEM - is fixed step Euler-Mayurama with first order ROCK stabilization thus can handle stiff problems. Only for Ito problems. Defaults to strong and weak order 1.0, but can solve with weak order 0.5 as SROCKEM(strong_order_1=false). This method can handle 1-dimensional, diagonal and multi-dimensional noise.\nSROCK2 - is a weak second order and strong first order fixed step stabilized method for stiff Ito problems.This method can handle 1-dimensional, diagonal and multi-dimensional noise.\nSKSROCK - is fixed step stabilized explicit method for stiff Ito problems. Strong order 0.5 and weak order 1. This method has a better stability domain then SROCK1. Also it allows special post-processing techniques in case of ergodic dynamical systems, in the context of ergodic Brownian dynamics, to achieve order 2 accuracy. SKSROCK(;post_processing=true) will make use of post processing. By default it doesn't use post processing. Post processing is optional and under development. The rest of the method is completely functional and can handle 1-dimensional, diagonal and multi-dimensional noise.  \nTangXiaoSROCK2 - is a fixed step size stabilized expicit method for stiff problems. Only for Ito problems. Weak order of 2 and strog order of 1. Has 5 versions with different stability domains which can be used as TangXiaoSROCK2(version_num=i) where i is 1-5. Under Development.","category":"page"},{"location":"solvers/sde_solve/#Stiff-Methods","page":"SDE Solvers","title":"Stiff Methods","text":"","category":"section"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"ImplicitEM - An order 0.5 Ito drift-implicit method. This is a theta method which defaults to theta=1 or the Trapezoid method on the drift term. This method defaults to symplectic=false, but when true and theta=1/2 this is the implicit Midpoint method on the drift term and is symplectic in distribution. Can handle all forms of noise, including non-diagonal, scalar, and colored noise. Uses a 1.0/1.5 heuristic for adaptive time stepping.\nSTrapezoid - An alias for ImplicitEM with theta=1/2\nSImplicitMidpoint - An alias for ImplicitEM with theta=1/2 and symplectic=true\nImplicitEulerHeun - An order 0.5 Stratonovich drift-implicit method. This is a theta method which defaults to theta=1/2 or the Trapezoid method on the drift term. This method defaults to symplectic=false, but when true and theta=1 this is the implicit Midpoint method on the drift term and is symplectic in distribution. Can handle all forms of noise, including non-diagonal, scalar, and colored noise. Uses a 1.0/1.5 heuristic for adaptive time stepping.\nImplicitRKMil - An order 1.0 drift-implicit method. This is a theta method which defaults to theta=1 or the Trapezoid method on the drift term. Defaults to solving the Ito problem, but ImplicitRKMil(interpretation=:Stratonovich) makes it solve the Stratonovich problem. This method defaults to symplectic=false, but when true and theta=1/2 this is the implicit Midpoint method on the drift term and is symplectic in distribution. Handles diagonal and scalar noise. Uses a 1.5/2.0 heuristic for adaptive time stepping.\nISSEM - An order 0.5 split-step Ito implicit method. It is fully implicit, meaning it can handle stiffness in the noise term. This is a theta method which defaults to theta=1 or the Trapezoid method on the drift term. This method defaults to symplectic=false, but when true and theta=1/2 this is the implicit Midpoint method on the drift term and is symplectic in distribution. Can handle all forms of noise, including non-diagonal, scalar, and colored noise. Uses a 1.0/1.5 heuristic for adaptive time stepping.\nISSEulerHeun - An order 0.5 split-step Stratonovich implicit method. It is fully implicit, meaning it can handle stiffness in the noise term. This is a theta method which defaults to theta=1 or the Trapezoid method on the drift term. This method defaults to symplectic=false, but when true and theta=1/2 this is the implicit Midpoint method on the drift term and is symplectic in distribution. Can handle all forms of noise, including non-diagonal,Q scalar, and colored noise. Uses a 1.0/1.5 heuristic for adaptive time stepping.\nSKenCarp - Adaptive L-stable drift-implicit strong order 1.5 for additive Ito and Stratonovich SDEs with weak order 2. Can handle diagonal, non-diagonal and scalar additive noise.*†","category":"page"},{"location":"solvers/sde_solve/#Derivative-Based-Methods","page":"SDE Solvers","title":"Derivative-Based Methods","text":"","category":"section"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"The following methods require analytic derivatives of the diffusion term.","category":"page"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"PCEuler - The predictor corrector euler method. Strong Order 0.5 in the Ito sense. Requires the ggprime function, which is defined as\n  textggprime^k(tx) = sum_j=1^m sum_i=1^d g_ij(tx) fracpartial g_kj(tx)partial x_i\nThis can also be understood more intuitively in vector/matrix form as,\ntextggprime(tx) = sum_j=1^m barmathcalJvec g^(j)(tx) vec g^(j)(tx)\nwhere vec g^(j) is the noise vector for the j'th noise channel and barmathcalJ is the Jacobian of the j'th   noise vector.\nThe default settings for the drift implicitness is theta=0.5 and the diffusion implicitness is eta=0.5.  ","category":"page"},{"location":"solvers/sde_solve/#High-Weak-Order-Methods","page":"SDE Solvers","title":"High Weak Order Methods","text":"","category":"section"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"Note that none of the following methods are adaptive.","category":"page"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"SimplifiedEM - A simplified Euler-Maruyama method with weak order 1.0 and fixed step size. Can handle all forms of noise, including non-diagonal, scalar, and colored noise.†\nDRI1 - Adaptive step weak order 2.0 for Ito SDEs with minimized error constants (deterministic order 3). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†\nDRI1NM - Adaptive step weak order 2.0 for Ito SDEs with minimized error constants (deterministic order 3). Can handle non-mixing diagonal (i.e., du[k] = f(u[k])) and scalar additive noise.†  \nRI1 - Adaptive step weak order 2.0 for Ito SDEs (deterministic order 3). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†\nRI3 - Adaptive step weak order 2.0 for Ito SDEs (deterministic order 3). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†\nRI5 - Adaptive step weak order 2.0 for Ito SDEs (deterministic order 3). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†\nRI6 - Adaptive step weak order 2.0 for Ito SDEs (deterministic order 2). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†\nRDI1WM - Fixed step weak order 1.0 for Ito SDEs (deterministic order 2). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†\nRDI2WM - Adaptive step weak order 2.0 for Ito SDEs (deterministic order 2). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†  \nRDI3WM - Adaptive step weak order 2.0 for Ito SDEs (deterministic order 3). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†  \nRDI4WM - Adaptive step weak order 2.0 for Ito SDEs (deterministic order 3). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†\nRS1 - Fixed step weak order 2.0 for Stratonovich SDEs (deterministic order 2). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†\nRS2 - Fixed step weak order 2.0 for Stratonovich SDEs (deterministic order 3). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†  \nPL1WM - Fixed step weak order 2.0 for Ito SDEs (deterministic order 2). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†  \nPL1WMA - Fixed step weak order 2.0 for Ito SDEs (deterministic order 2). Can handle additive noise.†         \nNON - Fixed step weak order 2.0 for Stratonovich SDEs (deterministic order 4). Can handle diagonal, non-diagonal, non-commuting, and scalar additive noise.†\nSIEA - Fixed step weak order 2.0 for Ito SDEs (deterministic order 2). Can handle diagonal and scalar additive noise.†  Stochastic generalization of the improved Euler method.\nSIEB - Fixed step weak order 2.0 for Ito SDEs (deterministic order 2). Can handle diagonal and scalar additive noise.†  Stochastic generalization of the improved Euler method.   \nSMEA - Fixed step weak order 2.0 for Ito SDEs (deterministic order 2). Can handle diagonal and scalar additive noise.†  Stochastic generalization of the modified Euler method.  \nSMEB - Fixed step weak order 2.0 for Ito SDEs (deterministic order 2). Can handle diagonal and scalar additive noise.†  Stochastic generalization of the modified Euler method.              ","category":"page"},{"location":"solvers/sde_solve/#StochasticCompositeAlgorithm","page":"SDE Solvers","title":"StochasticCompositeAlgorithm","text":"","category":"section"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"One unique feature of StochasticDiffEq.jl is the StochasticCompositeAlgorithm, which allows you to, with very minimal overhead, design a multimethod which switches between chosen algorithms as needed. The syntax is StochasticCompositeAlgorithm(algtup,choice_function) where algtup is a tuple of StochasticDiffEq.jl algorithms, and choice_function is a function which declares which method to use in the following step. For example, we can design a multimethod which uses EM() but switches to RKMil() whenever dt is too small:","category":"page"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"choice_function(integrator) = (Int(integrator.dt<0.001) + 1)\nalg_switch = StochasticCompositeAlgorithm((EM(),RKMil()),choice_function)","category":"page"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"The choice_function takes in an integrator and thus all of the features available in the Integrator Interface can be used in the choice function.","category":"page"},{"location":"solvers/sde_solve/#SimpleDiffEq.jl","page":"SDE Solvers","title":"SimpleDiffEq.jl","text":"","category":"section"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"This setup provides access to simplified versions of a few SDE solvers. They mostly exist for experimentation, but offer shorter compile times. They have limitations compared to StochasticDiffEq.jl.","category":"page"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"SimpleEM - A fixed timestep solve method for Euler-Maruyama. Only works with non-colored Gaussian noise.","category":"page"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"Note that this setup is not automatically included with DifferentialEquaitons.jl. To use the following algorithms, you must install and use SimpleDiffEq.jl:","category":"page"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"]add SimpleDiffEq\nusing SimpleDiffEq","category":"page"},{"location":"solvers/sde_solve/#BridgeDiffEq.jl","page":"SDE Solvers","title":"BridgeDiffEq.jl","text":"","category":"section"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"Bridge.jl is a set of fixed timestep algorithms written in Julia. These methods are made and optimized for out-of-place functions on immutable (static vector) types. Note that this setup is not automatically included with DifferentialEquaitons.jl. To use the following algorithms, you must install and use BridgeDiffEq.jl:","category":"page"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"Pkg.clone(\"https://github.com/JuliaDiffEq/BridgeDiffEq.jl\")\nusing BridgeDiffEq","category":"page"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"BridgeEuler - Strong order 0.5 Euler-Maruyama method for Ito equations.†\nBridgeHeun - Strong order 0.5 Euler-Heun method for Stratonovich equations.†\nBridgeSRK - Strong order 1.0 derivative-free stochastic Runge-Kutta method for scalar (<:Number) Ito equations.†","category":"page"},{"location":"solvers/sde_solve/#Notes","page":"SDE Solvers","title":"Notes","text":"","category":"section"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"†: Does not step to the interval endpoint. This can cause issues with discontinuity detection, and discrete variables need to be updated appropriately.","category":"page"},{"location":"solvers/sde_solve/","page":"SDE Solvers","title":"SDE Solvers","text":"*:  Note that although SKenCarp uses the same table as KenCarp3, solving a ODE problem using SKenCarp by setting g(du,u,p,t) = du .= 0 will take much more steps than KenCarp3 because error estimator of SKenCarp is different (because of noise terms) and default value of qmax (maximum permissible ratio of relaxing/tightening dt for adaptive steps) is smaller for StochasticDiffEq algorithms.","category":"page"},{"location":"features/low_dep/#Low-Dependency-Usage","page":"Low Dependency Usage","title":"Low Dependency Usage","text":"","category":"section"},{"location":"features/low_dep/","page":"Low Dependency Usage","title":"Low Dependency Usage","text":"DifferentialEquations.jl is a large library containing the functionality of many different solver and addon packages. However in many cases you may want to cut down on the size of the dependency and only use the parts of the the library which are essential to your application. This is possible due to SciML's modular package structure.","category":"page"},{"location":"features/low_dep/#Common-Example:-Using-only-OrdinaryDiffEq.jl","page":"Low Dependency Usage","title":"Common Example: Using only OrdinaryDiffEq.jl","text":"","category":"section"},{"location":"features/low_dep/","page":"Low Dependency Usage","title":"Low Dependency Usage","text":"One common example is using only the ODE solvers OrdinaryDiffEq.jl. The solvers all reexport SciMLBase.jl (which holds the problem and solution types) and so OrdinaryDiffEq.jl is all that's needed. Thus replacing","category":"page"},{"location":"features/low_dep/","page":"Low Dependency Usage","title":"Low Dependency Usage","text":"using DifferentialEquations","category":"page"},{"location":"features/low_dep/","page":"Low Dependency Usage","title":"Low Dependency Usage","text":"with","category":"page"},{"location":"features/low_dep/","page":"Low Dependency Usage","title":"Low Dependency Usage","text":"#Add the OrdinaryDiffEq Package first!\n#using Pkg; Pkg.add(\"OrdinaryDiffEq\")\nusing OrdinaryDiffEq","category":"page"},{"location":"features/low_dep/","page":"Low Dependency Usage","title":"Low Dependency Usage","text":"will work if these are the only features you are using.","category":"page"},{"location":"features/low_dep/#Generalizing-the-Idea","page":"Low Dependency Usage","title":"Generalizing the Idea","text":"","category":"section"},{"location":"features/low_dep/","page":"Low Dependency Usage","title":"Low Dependency Usage","text":"In general, you will always need SciMLBase.jl, since it defines all of the fundamental types, but the solvers will automatically reexport it. For solvers, you typically only need that solver package. So SciMLBase+Sundials, SciMLBase+LSODA, etc. will get you the common interface with that specific solver setup. SciMLBase.jl is a very lightweight dependency, so there is no issue here! For PDEs, you normally need SciMLBase+DiffEqPDEBase in addition to the solver package.","category":"page"},{"location":"features/low_dep/","page":"Low Dependency Usage","title":"Low Dependency Usage","text":"For the addon packages, you will normally need SciMLBase, the solver package you choose, and the addon package. So for example, for parameter estimation you would likely want SciMLBase+OrdinaryDiffEq+DiffEqParamEstim. If you aren't sure which package a specific command is from, then use @which. For example, from the parameter estimation docs we have:","category":"page"},{"location":"features/low_dep/","page":"Low Dependency Usage","title":"Low Dependency Usage","text":"using DifferentialEquations\nfunction f(du,u,p,t)\n  dx = p[1]*u[1] - u[1]*u[2]\n  dy = -3*u[2] + u[1]*u[2]\nend\n\nu0 = [1.0;1.0]\ntspan = (0.0,10.0)\np = [1.5]\nprob = ODEProblem(f,u0,tspan,p)\nsol = solve(prob,Tsit5())\nt = collect(range(0, stop=10, length=200))\nrandomized = VectorOfArray([(sol(t[i]) + .01randn(2)) for i in 1:length(t)])\nusing RecursiveArrayTools\ndata = convert(Array,randomized)\ncost_function = build_loss_objective(prob,t,data,Tsit5(),maxiters=10000)","category":"page"},{"location":"features/low_dep/","page":"Low Dependency Usage","title":"Low Dependency Usage","text":"If we wanted to know where build_loss_objective came from, we can do:","category":"page"},{"location":"features/low_dep/","page":"Low Dependency Usage","title":"Low Dependency Usage","text":"@which build_loss_objective(prob,t,data,Tsit5(),maxiters=10000)\n\n(::DiffEqParamEstim.#kw##build_loss_objective)(::Array{Any,1}, ::DiffEqParamEstim.#build_loss_objective, prob::SciMLBase.DEProblem, t, data, alg)","category":"page"},{"location":"features/low_dep/","page":"Low Dependency Usage","title":"Low Dependency Usage","text":"This says it's in the DiffEqParamEstim.jl package. Thus in this case, we could have done","category":"page"},{"location":"features/low_dep/","page":"Low Dependency Usage","title":"Low Dependency Usage","text":"using OrdinaryDiffEq, DiffEqParamEstim","category":"page"},{"location":"features/low_dep/","page":"Low Dependency Usage","title":"Low Dependency Usage","text":"instead of the full using DifferentialEquations. Note that due to the way Julia dependencies work, any internal function in the package will work. The only dependencies you need to explicitly using are the functions you are specifically calling. Thus this method can be used to determine all of the DiffEq packages you are using.","category":"page"},{"location":"tutorials/sde_example/#Stochastic-Differential-Equations","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"","category":"section"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"This tutorial will introduce you to the functionality for solving SDEs. Other introductions can be found by checking out DiffEqTutorials.jl.","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"note: Note\nThis tutorial assumes you have read the Ordinary Differential Equations tutorial.","category":"page"},{"location":"tutorials/sde_example/#Example-1:-Scalar-SDEs","page":"Stochastic Differential Equations","title":"Example 1: Scalar SDEs","text":"","category":"section"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"In this example we will solve the equation","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"du = f(upt)dt + g(upt)dW","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"where f(upt)=αu and g(upt)=βu. We know via Stochastic Calculus that the solution to this equation is","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"u(tWₜ)=u₀exp((α-fracβ^22)t+βWₜ)","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"To solve this numerically, we define a problem type by giving it the equation and the initial condition:","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"using DifferentialEquations\nα=1\nβ=1\nu₀=1/2\nf(u,p,t) = α*u\ng(u,p,t) = β*u\ndt = 1//2^(4)\ntspan = (0.0,1.0)\nprob = SDEProblem(f,g,u₀,(0.0,1.0))","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"The solve interface is then the same as with ODEs. Here we will use the classic Euler-Maruyama algorithm EM and plot the solution:","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"sol = solve(prob,EM(),dt=dt)\nusing Plots; plotly() # Using the Plotly backend\nplot(sol)","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"(Image: Basic Solution)","category":"page"},{"location":"tutorials/sde_example/#Using-Higher-Order-Methods","page":"Stochastic Differential Equations","title":"Using Higher Order Methods","text":"","category":"section"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"One unique feature of DifferentialEquations.jl is that higher-order methods for stochastic differential equations are included. For reference, let's also give the SDEProblem the analytical solution. We can do this by making a test problem. This can be a good way to judge how accurate the algorithms are, or is used to test convergence of the algorithms for methods developers. Thus we define the problem object with:","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"f_analytic(u₀,p,t,W) = u₀*exp((α-(β^2)/2)*t+β*W)\nff = SDEFunction(f,g,analytic=f_analytic)\nprob = SDEProblem(ff,g,u₀,(0.0,1.0))","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"and then we pass this information to the solver and plot:","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"#We can plot using the classic Euler-Maruyama algorithm as follows:\nsol = solve(prob,EM(),dt=dt)\nplot(sol,plot_analytic=true)","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"(Image: SDE Solution)","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"We can choose a higher-order solver for a more accurate result:","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"sol = solve(prob,SRIW1(),dt=dt,adaptive=false)\nplot(sol,plot_analytic=true)","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"(Image: Better SDE Solution)","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"By default, the higher order methods have adaptivity. Thus one can use","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"sol = solve(prob,SRIW1())\nplot(sol,plot_analytic=true)","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"(Image: Better Automatic Solution)","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"Here we allowed the solver to automatically determine a starting dt. This estimate at the beginning is conservative (small) to ensure accuracy. We can instead start the method with a larger dt by passing in a value for the starting dt:","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"sol = solve(prob,SRIW1(),dt=dt)\nplot(sol,plot_analytic=true)","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"(Image: Better Automatic Solution)","category":"page"},{"location":"tutorials/sde_example/#Ensemble-Simulations","page":"Stochastic Differential Equations","title":"Ensemble Simulations","text":"","category":"section"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"Instead of solving single trajectories, we can turn our problem into a EnsembleProblem to solve many trajectories all at once. This is done by the EnsembleProblem constructor:","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"ensembleprob = EnsembleProblem(prob)","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"The solver commands are defined at the Parallel Ensemble Simulations page. For example we can choose to have 1000 trajectories via trajectories=1000. In addition, this will automatically parallelize using Julia native parallelism if extra processes are added via addprocs(), but we can change this to use multithreading via EnsembleThreads(). Together, this looks like:","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"sol = solve(ensembleprob,EnsembleThreads(),trajectories=1000)","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"Many more controls are defined at the Ensemble simulations page,  including analysis tools. A very simple analysis can be done with the EnsembleSummary, which builds mean/var statistics and has an associated plot recipe. For example, we can get the statistics at every 0.01 timesteps and plot the average + error using:","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"using DifferentialEquations.EnsembleAnalysis\nsumm = EnsembleSummary(sol,0:0.01:1)\nplot(summ,labels=\"Middle 95%\")\nsumm = EnsembleSummary(sol,0:0.01:1;quantiles=[0.25,0.75])\nplot!(summ,labels=\"Middle 50%\",legend=true)","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"(Image: sde_tutorial_monte)","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"Additionally we can easily calculate the correlation between the values at t=0.2 and t=0.7 via","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"timepoint_meancor(sim,0.2,0.7) # Gives both means and then the correlation coefficient","category":"page"},{"location":"tutorials/sde_example/#Example-2:-Systems-of-SDEs-with-Diagonal-Noise","page":"Stochastic Differential Equations","title":"Example 2: Systems of SDEs with Diagonal Noise","text":"","category":"section"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"More generally, an SDE","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"du = f(upt)dt + g(upt)dW","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"generalizes to systems of equations is done in the same way as ODEs. Here, g is now a matrix of values. One common case, and the default for DifferentialEquations.jl, is diagonal noise where g is a diagonal matrix. This means that every function in the system gets a different random number. Instead of handling matrices in this case, we simply define both f and g as in-place functions. Thus f(du,u,p,t) gives a vector of du which is the deterministic change, and g(du2,u,p,t) gives a vector du2 for which du2.*W is the stochastic portion of the equation.","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"For example, the Lorenz equation with additive noise has the same deterministic portion as the Lorenz equations, but adds an additive noise, which is simply 3*N(0,dt) where N is the normal distribution dt is the time step, to each step of the equation. This is done via:","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"function lorenz(du,u,p,t)\n  du[1] = 10.0(u[2]-u[1])\n  du[2] = u[1]*(28.0-u[3]) - u[2]\n  du[3] = u[1]*u[2] - (8/3)*u[3]\nend\n\nfunction σ_lorenz(du,u,p,t)\n  du[1] = 3.0\n  du[2] = 3.0\n  du[3] = 3.0\nend\n\nprob_sde_lorenz = SDEProblem(lorenz,σ_lorenz,[1.0,0.0,0.0],(0.0,10.0))\nsol = solve(prob_sde_lorenz)\nplot(sol,vars=(1,2,3))","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"(Image: stochastic_3d_lorenz)","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"Note that it's okay for the noise function to mix terms. For example","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"function σ_lorenz(du,u,p,t)\n  du[1] = sin(u[3])*3.0\n  du[2] = u[2]*u[1]*3.0\n  du[3] = 3.0\nend","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"is a valid noise function, which will once again give diagonal noise by du2.*W.","category":"page"},{"location":"tutorials/sde_example/#Example-3:-Systems-of-SDEs-with-Scalar-Noise","page":"Stochastic Differential Equations","title":"Example 3: Systems of SDEs with Scalar Noise","text":"","category":"section"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"In this example we'll solve a system of SDEs with scalar noise. This means that the same noise process is applied to all SDEs. First we need to define a scalar noise process using the Noise Process interface. Since we want a WienerProcess that starts at 0.0 at time 0.0, we use the command W = WienerProcess(0.0,0.0,0.0) to define the Brownian motion we want, and then give this to the noise option in the SDEProblem. For a full example, let's solve a linear SDE with scalar noise using a high order algorithm:","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"f(du,u,p,t) = (du .= u)\ng(du,u,p,t) = (du .= u)\nu0 = rand(4,2)\n\nW = WienerProcess(0.0,0.0,0.0)\nprob = SDEProblem(f,g,u0,(0.0,1.0),noise=W)\nsol = solve(prob,SRIW1())","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"(Image: Scalar Noise)","category":"page"},{"location":"tutorials/sde_example/#Example-4:-Systems-of-SDEs-with-Non-Diagonal-Noise","page":"Stochastic Differential Equations","title":"Example 4: Systems of SDEs with Non-Diagonal Noise","text":"","category":"section"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"In the previous examples we had diagonal noise, that is a vector of random numbers dW whose size matches the output of g where the noise is applied element-wise, and scalar noise where a single random variable is applied to all dependent variables. However, a more general type of noise allows for the terms to linearly mixed via g being a matrix.","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"(Note that nonlinear mixings are not SDEs but fall under the more general class of random ordinary differential equations (RODEs) which have a separate set of solvers.","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"Let's define a problem with four Wiener processes and two dependent random variables. In this case, we will want the output of g to be a 2x4 matrix, such that the solution is g(u,p,t)*dW, the matrix multiplication. For example, we can do the following:","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"f(du,u,p,t) = du .= 1.01u\nfunction g(du,u,p,t)\n  du[1,1] = 0.3u[1]\n  du[1,2] = 0.6u[1]\n  du[1,3] = 0.9u[1]\n  du[1,4] = 0.12u[1]\n  du[2,1] = 1.2u[2]\n  du[2,2] = 0.2u[2]\n  du[2,3] = 0.3u[2]\n  du[2,4] = 1.8u[2]\nend\nprob = SDEProblem(f,g,ones(2),(0.0,1.0),noise_rate_prototype=zeros(2,4))","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"In our g we define the functions for computing the values of the matrix. We can now think of the SDE that this solves as the system of equations","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"du_1 = f_1(upt)dt + g_11(upt)dW_1 + g_12(upt)dW_2 + g_13(upt)dW_3 + g_14(upt)dW_4 \ndu_2 = f_2(upt)dt + g_21(upt)dW_1 + g_22(upt)dW_2 + g_23(upt)dW_3 + g_24(upt)dW_4","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"meaning that for example du[1,1] and du[2,1] correspond to stochastic changes with the same random number in the first and second SDEs.","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"note: Note\nThis problem can only be solved my SDE methods which are compatible with non-diagonal noise. This is discussed in the SDE solvers page.","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"The matrix itself is determined by the keyword argument noise_rate_prototype in the SDEProblem constructor. This is a prototype for the type that du will be in g. This can be any AbstractMatrix type. Thus for example, we can define the problem as","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"\n# Define a sparse matrix by making a dense matrix and setting some values as not zero\nA = zeros(2,4)\nA[1,1] = 1\nA[1,4] = 1\nA[2,4] = 1\nA=sparse(A)\n\n# Make `g` write the sparse matrix values\nfunction g(du,u,p,t)\n  du[1,1] = 0.3u[1]\n  du[1,4] = 0.12u[2]\n  du[2,4] = 1.8u[2]\nend\n\n# Make `g` use the sparse matrix\nprob = SDEProblem(f,g,ones(2),(0.0,1.0),noise_rate_prototype=A)","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"and now g(u,p,t) writes into a sparse matrix, and g(u,p,t)*dW is sparse matrix multiplication.","category":"page"},{"location":"tutorials/sde_example/#Example-4:-Colored-Noise","page":"Stochastic Differential Equations","title":"Example 4: Colored Noise","text":"","category":"section"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"Colored noise can be defined using the Noise Process interface. In that portion of the docs, it is shown how to define your own noise process my_noise, which can be passed to the SDEProblem","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"SDEProblem(f,g,u0,tspan,noise=my_noise)","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"Note that general colored noise problems are only compatible with the EM and EulerHeun methods. This is discussed in the SDE solvers page.","category":"page"},{"location":"tutorials/sde_example/#Example:-Spatially-Colored-Noise-in-the-Heston-Model","page":"Stochastic Differential Equations","title":"Example: Spatially-Colored Noise in the Heston Model","text":"","category":"section"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"Let's define the Heston equation from financial mathematics:","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"dS = μSdt + sqrtvSdW_1 \ndv = κ(Θ-v)dt + σsqrtvdW_2 \ndW_1 dW_2 = ρ dt","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"In this problem, we have a diagonal noise problem given by:","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"function f(du,u,p,t)\n  du[1] = μ*u[1]\n  du[2] = κ*(Θ-u[2])\nend\nfunction g(du,u,p,t)\n  du[1] = √u[2]*u[1]\n  du[2] = Θ*√u[2]\nend","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"However, our noise has a correlation matrix for some constant ρ. Choosing ρ=0.2:","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"Γ = [1 ρ;ρ 1]","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"To solve this, we can define a CorrelatedWienerProcess which starts at zero (W(0)=0) via:","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"heston_noise = CorrelatedWienerProcess!(Γ,tspan[1],zeros(2),zeros(2))","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"This is then used to build the SDE:","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"SDEProblem(f,g,u0,tspan,noise=heston_noise)","category":"page"},{"location":"tutorials/sde_example/","page":"Stochastic Differential Equations","title":"Stochastic Differential Equations","text":"Of course, to fully define this problem we need to define our constants. Constructors for making common models like this easier to define can be found in the modeling toolkits. For example, the HestonProblem is pre-defined as part of the financial modeling tools.","category":"page"},{"location":"features/io/#io","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"","category":"section"},{"location":"features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"The ability to save and load solutions is important for handling large datasets and analyzing the results over multiple Julia sessions. This page explains the existing functionality for doing so.","category":"page"},{"location":"features/io/#Tabular-Data:-IterableTables","page":"I/O: Saving and Loading Solution Data","title":"Tabular Data: IterableTables","text":"","category":"section"},{"location":"features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"An interface to IterableTables.jl is provided. This IterableTables link allows you to use a solution type as the data source to convert to other tabular data formats. For example, let's solve a 4x2 system of ODEs and get the DataFrame:","category":"page"},{"location":"features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"using OrdinaryDiffEq, DataFrames\nf_2dlinear = (du,u,p,t) -> du.=1.01u;\nprob = ODEProblem(f_2dlinear,rand(2,2),(0.0,1.0));\nsol1 =solve(prob,Euler();dt=1//2^(4));\ndf = DataFrame(sol1)","category":"page"},{"location":"features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"│ Row │ timestamp │ value 1  │ value 2  │ value 3  │ value 4  │\n├─────┼───────────┼──────────┼──────────┼──────────┼──────────┤\n│ 1   │ 0.0       │ 0.110435 │ 0.569561 │ 0.918336 │ 0.508044 │\n│ 2   │ 0.0625    │ 0.117406 │ 0.605515 │ 0.976306 │ 0.540114 │\n│ 3   │ 0.125     │ 0.124817 │ 0.643738 │ 1.03794  │ 0.574208 │\n│ 4   │ 0.1875    │ 0.132696 │ 0.684374 │ 1.10345  │ 0.610455 │\n│ 5   │ 0.25      │ 0.141073 │ 0.727575 │ 1.17311  │ 0.64899  │\n│ 6   │ 0.3125    │ 0.149978 │ 0.773503 │ 1.24716  │ 0.689958 │\n│ 7   │ 0.375     │ 0.159445 │ 0.822331 │ 1.32589  │ 0.733511 │\n│ 8   │ 0.4375    │ 0.16951  │ 0.87424  │ 1.40959  │ 0.779814 │\n│ 9   │ 0.5       │ 0.18021  │ 0.929427 │ 1.49857  │ 0.82904  │\n│ 10  │ 0.5625    │ 0.191586 │ 0.988097 │ 1.59316  │ 0.881373 │\n│ 11  │ 0.625     │ 0.20368  │ 1.05047  │ 1.69373  │ 0.93701  │\n│ 12  │ 0.6875    │ 0.216537 │ 1.11678  │ 1.80065  │ 0.996159 │\n│ 13  │ 0.75      │ 0.230206 │ 1.18728  │ 1.91432  │ 1.05904  │\n│ 14  │ 0.8125    │ 0.244738 │ 1.26222  │ 2.03516  │ 1.12589  │\n│ 15  │ 0.875     │ 0.260187 │ 1.3419   │ 2.16363  │ 1.19697  │\n│ 16  │ 0.9375    │ 0.276611 │ 1.42661  │ 2.30021  │ 1.27252  │\n│ 17  │ 1.0       │ 0.294072 │ 1.51667  │ 2.44541  │ 1.35285  │","category":"page"},{"location":"features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"If we set syms in the DiffEqFunction, then those names will be used:","category":"page"},{"location":"features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"f = ODEFunction(f_2dlinear,syms=[:a,:b,:c,:d])\nprob = ODEProblem(f,rand(2,2),(0.0,1.0));\nsol1 =solve(prob,Euler();dt=1//2^(4));\ndf = DataFrame(sol1)","category":"page"},{"location":"features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"17×5 DataFrame\n│ Row │ timestamp │ a        │ b        │ c       │ d          │\n│     │ Float64   │ Float64  │ Float64  │ Float64 │ Float64    │\n├─────┼───────────┼──────────┼──────────┼─────────┼────────────┤\n│ 1   │ 0.0       │ 0.203202 │ 0.348326 │ 0.58971 │ 0.00606127 │\n⋮\n│ 16  │ 0.9375    │ 0.508972 │ 0.87247  │ 1.47708 │ 0.015182   │\n│ 17  │ 1.0       │ 0.541101 │ 0.927544 │ 1.57032 │ 0.0161403  │","category":"page"},{"location":"features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"Many modeling frameworks will automatically set syms for this feature. Additionally, this data can be saved to a CSV:","category":"page"},{"location":"features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"using CSV\nCSV.write(\"out.csv\",df)","category":"page"},{"location":"features/io/#JLD2-and-BSON.jl","page":"I/O: Saving and Loading Solution Data","title":"JLD2 and BSON.jl","text":"","category":"section"},{"location":"features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"JLD2.jl and BSON.jl will work with the full solution type if you bring the required functions back into scope before loading. For eaxmple, if we save the solution:","category":"page"},{"location":"features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"using OrdinaryDiffEq, JLD2\nf(u,p,t) = 1.01*u\nu0=1/2\ntspan = (0.0,1.0)\nprob = ODEProblem(f,u0,tspan)\nsol = solve(prob,Tsit5(),reltol=1e-8,abstol=1e-8)\n@save \"out.jld2\" sol","category":"page"},{"location":"features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"then we can get the full solution type back, interpolations and all, if we load the dependent functions first:","category":"page"},{"location":"features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"using JLD2\nusing OrdinaryDiffEq\nf(u,p,t) = 1.01*u\nJLD2.@load \"out.jld2\" sol","category":"page"},{"location":"features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"The example with BSON.jl is:","category":"page"},{"location":"features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"using OrdinaryDiffEq\nf_2dlinear = (du,u,p,t) -> du.=1.01u\nprob = ODEProblem(f_2dlinear,rand(2,2),(0.0,1.0))\nsol1 =solve(prob,Euler();dt=1//2^(4))\n\nusing BSON\nbson(\"test.bson\",Dict(:sol1=>sol1))\n\n# New session\nusing OrdinaryDiffEq, LinearAlgebra\nusing BSON\nBSON.load(\"test.bson\")","category":"page"},{"location":"features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"If you load it without the DE function then for some algorithms the interpolation may not work, and for all algorithms you'll need at least a solver package or SciMLBase.jl in scope in order for the solution interface (plot recipes, array indexing, etc.) to work. If none of these are put into scope, the solution type will still load and hold all of the values (so sol.u and sol.t will work), but none of the interface will be available.","category":"page"},{"location":"features/io/#JLD","page":"I/O: Saving and Loading Solution Data","title":"JLD","text":"","category":"section"},{"location":"features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"Don't use JLD. It's dead. Julia types can be saved via JLD.jl. However, they cannot save types which have functions, which means that the solution type is currently not compatible with JLD.","category":"page"},{"location":"features/io/","page":"I/O: Saving and Loading Solution Data","title":"I/O: Saving and Loading Solution Data","text":"using JLD\nJLD.save(\"out.jld\",\"sol\",sol)","category":"page"},{"location":"models/multiscale/#Multi-Scale-Models","page":"Multi-Scale Models","title":"Multi-Scale Models","text":"","category":"section"},{"location":"models/multiscale/","page":"Multi-Scale Models","title":"Multi-Scale Models","text":"The multi-scale modeling functionality is provided by MultiScaleArrays.jl. It allows for designing a multi-scale model as an extension of an array, which in turn can be directly used in the native Julia solvers of DifferentialEquations.jl.","category":"page"},{"location":"models/multiscale/#More-Information","page":"Multi-Scale Models","title":"More Information","text":"","category":"section"},{"location":"models/multiscale/","page":"Multi-Scale Models","title":"Multi-Scale Models","text":"For more information, please see the MultiScaleArrays.jl README.","category":"page"},{"location":"basics/integrator/#integrator","page":"Integrator Interface","title":"Integrator Interface","text":"","category":"section"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"The integrator interface gives one the ability to interactively step through the numerical solving of a differential equation. Through this interface, one can easily monitor results, modify the problem during a run, and dynamically continue solving as one sees fit.","category":"page"},{"location":"basics/integrator/#Initialization-and-Stepping","page":"Integrator Interface","title":"Initialization and Stepping","text":"","category":"section"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"To initialize an integrator, use the syntax:","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"integrator = init(prob,alg;kwargs...)","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"The keyword args which are accepted are the same as the solver options used by solve and the returned value is an integrator which satisfies typeof(integrator)<:DEIntegrator. One can manually choose to step via the step! command:","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"step!(integrator)","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"which will take one successful step. Additonally:","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"step!(integrator,dt[,stop_at_tdt=false])","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"passing a dt will make the integrator keep stepping until integrator.t+dt, and setting stop_at_tdt=true will add a tstop to force it to step to integrator.t+dt","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"To check whether or not the integration step was successful, you can call check_error(integrator) which returns one of the return codes.","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"This type also implements an iterator interface, so one can step n times (or to the last tstop) using the take iterator:","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"for i in take(integrator,n) end","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"One can loop to the end by using solve!(integrator) or using the iterator interface:","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"for i in integrator end","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"In addition, some helper iterators are provided to help monitor the solution. For example, the tuples iterator lets you view the values:","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"for (u,t) in tuples(integrator)\n  @show u,t\nend","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"and the intervals iterator lets you view the full interval:","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"for (uprev,tprev,u,t) in intervals(integrator)\n  @show tprev,t\nend","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"Additionally, you can make the iterator return specific time points via the TimeChoiceIterator:","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"ts = range(0, stop=1, length=11)\nfor (u,t) in TimeChoiceIterator(integrator,ts)\n  @show u,t\nend","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"Lastly, one can dynamically control the \"endpoint\". The initialization simply makes prob.tspan[2] the last value of tstop, and many of the iterators are made to stop at the final tstop value. However, step! will always take a step, and one can dynamically add new values of tstops by modifiying the variable in the options field: add_tstop!(integrator,new_t).","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"Finally, to solve to the last tstop, call solve!(integrator). Doing init and then solve! is equivalent to solve.","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"SciMLBase.step!\nSciMLBase.check_error\nSciMLBase.check_error!","category":"page"},{"location":"basics/integrator/#SciMLBase.step!","page":"Integrator Interface","title":"SciMLBase.step!","text":"step!(integ::DEIntegrator [, dt [, stop_at_tdt]])\n\nPerform one (successful) step on the integrator.\n\nAlternative, if a dt is given, then step! the integrator until there is a temporal difference ≥ dt in integ.t.  When true is passed to the optional third argument, the integrator advances exactly dt.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.check_error","page":"Integrator Interface","title":"SciMLBase.check_error","text":"check_error(integrator)\n\nCheck state of integrator and return one of the Return Codes\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.check_error!","page":"Integrator Interface","title":"SciMLBase.check_error!","text":"check_error!(integrator)\n\nSame as check_error but also set solution's return code (integrator.sol.retcode) and run postamble!.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#Handing-Integrators","page":"Integrator Interface","title":"Handing Integrators","text":"","category":"section"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"The integrator<:DEIntegrator type holds all of the information for the intermediate solution of the differential equation. Useful fields are:","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"t - time of the proposed step\nu - value at the proposed step\np - user-provided data\nopts - common solver options\nalg - the algorithm associated with the solution\nf - the function being solved\nsol - the current state of the solution\ntprev - the last timepoint\nuprev - the value at the last timepoint\ntdir - the sign for the direction of time","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"The p is the data which is provided by the user as a keyword arg in init. opts holds all of the common solver options, and can be mutated to change the solver characteristics. For example, to modify the absolute tolerance for the future timesteps, one can do:","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"integrator.opts.abstol = 1e-9","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"The sol field holds the current solution. This current solution includes the interpolation function if available, and thus integrator.sol(t) lets one interpolate efficiently over the whole current solution. Additionally, a a \"current interval interpolation function\" is provided on the integrator type via integrator(t,deriv::Type=Val{0};idxs=nothing,continuity=:left). This uses only the solver information from the interval [tprev,t] to compute the interpolation, and is allowed to extrapolate beyond that interval.","category":"page"},{"location":"basics/integrator/#Note-about-mutating","page":"Integrator Interface","title":"Note about mutating","text":"","category":"section"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"Be cautious: one should not directly mutate the t and u fields of the integrator. Doing so will destroy the accuracy of the interpolator and can harm certain algorithms. Instead if one wants to introduce discontinuous changes, one should use the callbacks. Modifications within a callback affect! surrounded by saves provides an error-free handling of the discontinuity.","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"As low-level alternative to the callbacks, one can use set_t!, set_u! and set_ut! to mutate integrator states.  Note that certain integrators may not have efficient ways to modify u and t.  In such case, set_*! are as inefficient as reinit!.","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"SciMLBase.set_t!\nSciMLBase.set_u!\nSciMLBase.set_ut!","category":"page"},{"location":"basics/integrator/#SciMLBase.set_t!","page":"Integrator Interface","title":"SciMLBase.set_t!","text":"set_t!(integrator::DEIntegrator, t)\n\nSet current time point of the integrator to t.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.set_u!","page":"Integrator Interface","title":"SciMLBase.set_u!","text":"set_u!(integrator::DEIntegrator, u)\n\nSet current state of the integrator to u.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.set_ut!","page":"Integrator Interface","title":"SciMLBase.set_ut!","text":"set_ut!(integrator::DEIntegrator, u, t)\n\nSet current state of the integrator to u and t\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#Integrator-vs-Solution","page":"Integrator Interface","title":"Integrator vs Solution","text":"","category":"section"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"The integrator and the solution have very different actions because they have very different meanings. The typeof(sol) <: DESolution type is a type with history: it stores all of the (requested) timepoints and interpolates/acts using the values closest in time. On the other hand, the typeof(integrator)<:DEIntegrator type is a local object. It only knows the times of the interval it currently spans, the current caches and values, and the current state of the solver (the current options, tolerances, etc.). These serve very different purposes:","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"The integrator's interpolation can extrapolate, both forward and backward in in time. This is used to estimate events and is internally used for predictions.\nThe integrator is fully mutable upon iteration. This means that every time an iterator affect is used, it will take timesteps from the current time. This means that first(integrator)!=first(integrator) since the integrator will step once to evaluate the left and then step once more (not backtracking). This allows the iterator to keep dynamically stepping, though one should note that it may violate some immutablity assumptions commonly made about iterators.","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"If one wants the solution object, then one can find it in integrator.sol.","category":"page"},{"location":"basics/integrator/#Function-Interface","page":"Integrator Interface","title":"Function Interface","text":"","category":"section"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"In addition to the type interface, a function interface is provided which allows for safe modifications of the integrator type, and allows for uniform usage throughout the ecosystem (for packages/algorithms which implement the functions). The following functions make up the interface:","category":"page"},{"location":"basics/integrator/#Saving-Controls","page":"Integrator Interface","title":"Saving Controls","text":"","category":"section"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"savevalues!","category":"page"},{"location":"basics/integrator/#SciMLBase.savevalues!","page":"Integrator Interface","title":"SciMLBase.savevalues!","text":"savevalues!(integrator::DEIntegrator,\n  force_save=false) -> Tuple{Bool, Bool}\n\nTry to save the state and time variables at the current time point, or the saveat point by using interpolation when appropriate. It returns a tuple that is (saved, savedexactly). If savevalues! saved value, then saved is true, and if savevalues! saved at the current time point, then savedexactly is true.\n\nThe saving priority/order is as follows:\n\nsave_on\nsaveat\nforce_save\nsave_everystep\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#Caches","page":"Integrator Interface","title":"Caches","text":"","category":"section"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"get_tmp_cache\nfull_cache","category":"page"},{"location":"basics/integrator/#SciMLBase.get_tmp_cache","page":"Integrator Interface","title":"SciMLBase.get_tmp_cache","text":"get_tmp_cache(i::DEIntegrator)\n\nReturns a tuple of internal cache vectors which are safe to use as temporary arrays. This should be used for integrator interface and callbacks which need arrays to write into in order to be non-allocating. The length of the tuple is dependent on the method.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.full_cache","page":"Integrator Interface","title":"SciMLBase.full_cache","text":"full_cache(i::DEIntegrator)\n\nReturns an iterator over the cache arrays of the method. This can be used to change internal values as needed.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#stepping_controls","page":"Integrator Interface","title":"Stepping Controls","text":"","category":"section"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"u_modified!\nget_proposed_dt\nset_proposed_dt!\nterminate!\nchange_t_via_interpolation!\nadd_tstop!\nadd_saveat!","category":"page"},{"location":"basics/integrator/#SciMLBase.u_modified!","page":"Integrator Interface","title":"SciMLBase.u_modified!","text":"u_modified!(i::DEIntegrator,bool)\n\nSets bool which states whether a change to u occurred, allowing the solver to handle the discontinuity. By default, this is assumed to be true if a callback is used. This will result in the re-calculation of the derivative at t+dt, which is not necessary if the algorithm is FSAL and u does not experience a discontinuous change at the end of the interval. Thus if u is unmodified in a callback, a single call to the derivative calculation can be eliminated by u_modified!(integrator,false).\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.get_proposed_dt","page":"Integrator Interface","title":"SciMLBase.get_proposed_dt","text":"get_proposed_dt(i::DEIntegrator)\n\nGets the proposed dt for the next timestep.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.set_proposed_dt!","page":"Integrator Interface","title":"SciMLBase.set_proposed_dt!","text":"set_proposed_dt(i::DEIntegrator,dt)\nset_proposed_dt(i::DEIntegrator,i2::DEIntegrator)\n\nSets the proposed dt for the next timestep. If second argument isa DEIntegrator then it sets the timestepping of first argument to match that of second one. Note that due to PI control and step acceleration this is more than matching the factors in most cases.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.terminate!","page":"Integrator Interface","title":"SciMLBase.terminate!","text":"terminate!(i::DEIntegrator[, retcode = :Terminated])\n\nTerminates the integrator by emptying tstops. This can be used in events and callbacks to immediately end the solution process.  Optionally, retcode may be specified (see: Return Codes (RetCodes)).\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.change_t_via_interpolation!","page":"Integrator Interface","title":"SciMLBase.change_t_via_interpolation!","text":"change_t_via_interpolation!(integrator::DEIntegrator,t,modify_save_endpoint=Val{false})\n\nModifies the current t and changes all of the corresponding values using the local interpolation. If the current solution has already been saved, one can provide the optional value modify_save_endpoint to also modify the endpoint of sol in the same manner.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.add_tstop!","page":"Integrator Interface","title":"SciMLBase.add_tstop!","text":"add_tstop!(i::DEIntegrator,t)\n\nAdds a tstop at time t.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.add_saveat!","page":"Integrator Interface","title":"SciMLBase.add_saveat!","text":"add_saveat!(i::DEIntegrator,t)\n\nAdds a saveat time point at t.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#Resizing","page":"Integrator Interface","title":"Resizing","text":"","category":"section"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"resize!\ndeleteat!\naddat!\nresize_non_user_cache!\ndeleteat_non_user_cache!\naddat_non_user_cache!","category":"page"},{"location":"basics/integrator/#Base.resize!","page":"Integrator Interface","title":"Base.resize!","text":"resize!(integrator::DEIntegrator,k::Int)\n\nResizes the DE to a size k. This chops off the end of the array, or adds blank values at the end, depending on whether k > length(integrator.u).\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#Base.deleteat!","page":"Integrator Interface","title":"Base.deleteat!","text":"deleteat!(integrator::DEIntegrator,idxs)\n\nShrinks the ODE by deleting the idxs components.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.addat!","page":"Integrator Interface","title":"SciMLBase.addat!","text":"addat!(integrator::DEIntegrator,idxs,val)\n\nGrows the ODE by adding the idxs components. Must be contiguous indices.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.resize_non_user_cache!","page":"Integrator Interface","title":"SciMLBase.resize_non_user_cache!","text":"resize_non_user_cache!(integrator::DEIntegrator,k::Int)\n\nResizes the non-user facing caches to be compatible with a DE of size k. This includes resizing Jacobian caches.\n\nnote: Note\nIn many cases, resize! simply resizes full_cache variables and then calls this function. This finer control is required for some AbstractArray operations.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.deleteat_non_user_cache!","page":"Integrator Interface","title":"SciMLBase.deleteat_non_user_cache!","text":"deleteat_non_user_cache!(integrator::DEIntegrator,idxs)\n\ndeleteat!s the non-user facing caches at indices idxs. This includes resizing Jacobian caches.\n\nnote: Note\nIn many cases, deleteat! simply deleteat!s full_cache variables and then calls this function. This finer control is required for some AbstractArray operations.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.addat_non_user_cache!","page":"Integrator Interface","title":"SciMLBase.addat_non_user_cache!","text":"addat_non_user_cache!(i::DEIntegrator,idxs)\n\naddat!s the non-user facing caches at indices idxs. This includes resizing Jacobian caches.\n\nnote: Note\nIn many cases, addat! simply addat!s full_cache variables and then calls this function. This finer control is required for some AbstractArray operations.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#Reinitialization","page":"Integrator Interface","title":"Reinitialization","text":"","category":"section"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"reinit!\nauto_dt_reset!","category":"page"},{"location":"basics/integrator/#SciMLBase.reinit!","page":"Integrator Interface","title":"SciMLBase.reinit!","text":"reinit!(integrator::DEIntegrator,args...; kwargs...)\n\nThe reinit function lets you restart the integration at a new value.\n\nArguments\n\nu0: Value of u to start at. Default value is integrator.sol.prob.u0\n\nKeyword Arguments\n\nt0: Starting timepoint. Default value is integrator.sol.prob.tspan[1]\ntf: Ending timepoint. Default value is integrator.sol.prob.tspan[2]\nerase_sol=true: Whether to start with no other values in the solution, or keep the previous solution.\ntstops, d_discontinuities, & saveat: Cache where these are stored. Default is the original cache.\nreset_dt: Set whether to reset the current value of dt using the automatic dt determination algorithm. Default is (integrator.dtcache == zero(integrator.dt)) && integrator.opts.adaptive\nreinit_callbacks: Set whether to run the callback initializations again (and initialize_save is for that). Default is true.\nreinit_cache: Set whether to re-run the cache initialization function (i.e. resetting FSAL, not allocating vectors) which should usually be true for correctness. Default is true.\n\nAdditionally, once can access auto_dt_reset! which will run the auto dt initialization algorithm.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.auto_dt_reset!","page":"Integrator Interface","title":"SciMLBase.auto_dt_reset!","text":"auto_dt_reset!(integrator::DEIntegrator)\n\nRun the auto dt initialization algorithm.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#Misc","page":"Integrator Interface","title":"Misc","text":"","category":"section"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"get_du\nget_du!","category":"page"},{"location":"basics/integrator/#SciMLBase.get_du","page":"Integrator Interface","title":"SciMLBase.get_du","text":"get_du(i::DEIntegrator)\n\nReturns the derivative at t.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/#SciMLBase.get_du!","page":"Integrator Interface","title":"SciMLBase.get_du!","text":"get_du!(out,i::DEIntegrator)\n\nWrite the current derivative at t into out.\n\n\n\n\n\n","category":"function"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"warning: Warning\nNote that not all of these functions will be implemented for every algorithm. Some have hard limitations. For example, Sundials.jl cannot resize problems. When a function is not limited, an error will be thrown.","category":"page"},{"location":"basics/integrator/#Additional-Options","page":"Integrator Interface","title":"Additional Options","text":"","category":"section"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"The following options can additionally be specified in init (or be mutated in the opts) for further control of the integrator:","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"advance_to_tstop: This makes step! continue to the next value in tstop.\nstop_at_next_tstop: This forces the iterators to stop at the next value of tstop.","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"For example, if one wants to iterate but only stop at specific values, one can choose:","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"integrator = init(prob,Tsit5();dt=1//2^(4),tstops=[0.5],advance_to_tstop=true)\nfor (u,t) in tuples(integrator)\n  @test t ∈ [0.5,1.0]\nend","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"which will only enter the loop body at the values in tstops (here, prob.tspan[2]==1.0 and thus there are two values of tstops which are hit). Addtionally, one can solve! only to 0.5 via:","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"integrator = init(prob,Tsit5();dt=1//2^(4),tstops=[0.5])\nintegrator.opts.stop_at_next_tstop = true\nsolve!(integrator)","category":"page"},{"location":"basics/integrator/#Plot-Recipe","page":"Integrator Interface","title":"Plot Recipe","text":"","category":"section"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"Like the DESolution type, a plot recipe is provided for the DEIntegrator type. Since the DEIntegrator type is a local state type on the current interval, plot(integrator) returns the solution on the current interval. The same options for the plot recipe are provided as for sol, meaning one can choose variables via the vars keyword argument, or change the plotdensity / turn on/off denseplot.","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"Additionally, since the integrator is an iterator, this can be used in the Plots.jl animate command to iteratively build an animation of the solution while solving the differential equation.","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"For an example of manually chaining together the iterator interface and plotting, one should try the following:","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"using DifferentialEquations, DiffEqProblemLibrary, Plots\n\n# Linear ODE which starts at 0.5 and solves from t=0.0 to t=1.0\nprob = ODEProblem((u,p,t)->1.01u,0.5,(0.0,1.0))\n\nusing Plots\nintegrator = init(prob,Tsit5();dt=1//2^(4),tstops=[0.5])\npyplot(show=true)\nplot(integrator)\nfor i in integrator\n  display(plot!(integrator,vars=(0,1),legend=false))\nend\nstep!(integrator); plot!(integrator,vars=(0,1),legend=false)\nsavefig(\"iteratorplot.png\")","category":"page"},{"location":"basics/integrator/","page":"Integrator Interface","title":"Integrator Interface","text":"(Image: Iterator Plot)","category":"page"},{"location":"features/linear_nonlinear/#linear_nonlinear","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"","category":"section"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"One of the key features of DifferentialEquations.jl is its flexibility. Keeping with this trend, many of the native Julia solvers provided by DifferentialEquations.jl allow you to choose the method for linear and nonlinear solving. This section details how to make that choice.","category":"page"},{"location":"features/linear_nonlinear/#Linear-Solvers:-linsolve-Specification","page":"Specifying (Non)Linear Solvers","title":"Linear Solvers: linsolve Specification","text":"","category":"section"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"For differential equation integrators which use linear solvers, an argument to the method linsolve determines the linear solver which is used. The signature is:","category":"page"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"linsolve! = linsolve(Val{:init},f,x;kwargs...)\nlinsolve!(x,A,b,matrix_updated=false;kwargs...)","category":"page"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"This is an in-place function which updates x by solving Ax=b. The user should specify the function linsolve(Val{:init},f,x) which returns a linsolve! function. The setting matrix_updated determines whether the matrix A has changed from the last call. This can be used to smartly cache factorizations.","category":"page"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"Note that linsolve! needs to accept splatted keyword arguments. The possible arguments passed to the linear solver are as follows:","category":"page"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"Pl, a pre-specified left preconditioner which utilizes the internal adaptive norm estimates\nPr, a pre-specified right preconditioner which utilizes the internal adaptive norm estimates\ntol, a linear solver tolerance specified from the ODE solver's implicit handling","category":"page"},{"location":"features/linear_nonlinear/#Pre-Built-Linear-Solver-Choices","page":"Specifying (Non)Linear Solvers","title":"Pre-Built Linear Solver Choices","text":"","category":"section"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"The following choices of pre-built linear solvers exist:","category":"page"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"DefaultLinSolve\nLinSolveFactorize\nLinSolveGPUFactorize\nLinSolveGMRES\nLinSolveCG\nLinSolveBiCGStabl\nLinSolveChebyshev\nLinSolveMINRES\nLinSolveIterativeSolvers","category":"page"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"Additionally, by adding Pardiso.jl the following exist:","category":"page"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"MKLPardisoFactorize\nPardisoFactorize\nPardisoIterate","category":"page"},{"location":"features/linear_nonlinear/#DefaultLinSolve","page":"Specifying (Non)Linear Solvers","title":"DefaultLinSolve","text":"","category":"section"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"The default linear solver is DefaultLinSolve. This method is adaptive, and automatically chooses an LU factorization choose for dense and sparse arrays, and is compatible with GPU-based arrays. When the Jacobian is an AbstractDiffEqOperator, i.e. is matrix-free, DefaultLinSolve defaults to using a gmres iterative solver.","category":"page"},{"location":"features/linear_nonlinear/#Basic-linsolve-method-choice:-Factorization-by-LinSolveFactorize","page":"Specifying (Non)Linear Solvers","title":"Basic linsolve method choice: Factorization by LinSolveFactorize","text":"","category":"section"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"The easiest way to specify a linsolve is by a factorization function which generates a type on which \\ (or A_ldiv_B!) is called.  This is done through the helper function LinSolveFactorize which makes the appropriate function. For example, the  Rosenbrock23 takes in a linsolve function, which we can choose to be a QR-factorization from the standard library LinearAlgebra by:","category":"page"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"Rosenbrock23(linsolve=LinSolveFactorize(qr!))","category":"page"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"LinSolveFactorize takes in a function which returns an object that can \\. Direct methods like qr! will automatically cache the factorization, making it efficient for small dense problems.","category":"page"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"However, for large sparse problems, you can let \\ be an iterative method. For example, using PETSc.jl, we can define our factorization function to be:","category":"page"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"linsolve = LinSolveFactorize((A) -> KSP(A, ksp_type=\"gmres\", ksp_rtol=1e-6))","category":"page"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"This function creates a KSP type which makes \\ perform the GMRES iterative method provided by PETSc.jl. Thus if we pass this function into the algorithm as the factorization method, all internal linear solves will happen by PETSc.jl.","category":"page"},{"location":"features/linear_nonlinear/#GPU-offloading-of-factorization-with-LinSolveGPUFactorize","page":"Specifying (Non)Linear Solvers","title":"GPU offloading of factorization with LinSolveGPUFactorize","text":"","category":"section"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"If one has a problem with a sufficiently large Jacobian (~100x100) and a sufficiently powerful GPU, it can make sense to offload the factorization and backpropogation steps to the GPU. For this, the LinSolveGPUFactorize linear solver is provided. It works similarly to LinSolveFactorize, but the matrix is automatically sent to the GPU as a CuArray and the ldiv! is performed against a CUDA QR factorization of the matrix.","category":"page"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"Note that this method requires that you have done using CuArrays in your script. A working installation of CuArrays.jl is required, which requires an installation of CUDA Toolkit.","category":"page"},{"location":"features/linear_nonlinear/#iterativesolvers-jl","page":"Specifying (Non)Linear Solvers","title":"IterativeSolvers.jl-Based Methods","text":"","category":"section"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"The signature for LinSolveIterativeSolvers is:","category":"page"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"LinSolveIterativeSolvers(generate_iterator,args...;\n                         Pl=IterativeSolvers.Identity(),\n                         Pr=IterativeSolvers.Identity(),\n                         kwargs...)","category":"page"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"where Pl is the left preconditioner, Pr is the right preconditioner, and the other args... and kwargs... are passed into the iterative solver chosen in generate_iterator which designates the construction of an iterator from IterativeSolvers.jl. For example, using gmres_iterable! would make a version that uses IterativeSolvers.gmres. The following are aliases to common choices:","category":"page"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"LinSolveGMRES – GMRES\nLinSolveCG – CG (Conjugate Gradient)\nLinSolveBiCGStabl – BiCGStabl Stabilized Bi-Conjugate Gradient\nLinSolveChebyshev – Chebyshev\nLinSolveMINRES – MINRES","category":"page"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"which all have the same arguments as LinSolveIterativeSolvers except with generate_iterator pre-specified.","category":"page"},{"location":"features/linear_nonlinear/#Implementing-Your-Own-LinSolve:-How-LinSolveFactorize-Was-Created","page":"Specifying (Non)Linear Solvers","title":"Implementing Your Own LinSolve: How LinSolveFactorize Was Created","text":"","category":"section"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"In order to make your own linsolve functions, let's look at how the LinSolveFactorize function is created. For example, for an LU-Factorization, we would like to use lufact! to do our linear solving. We can directly write this as:","category":"page"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"using LinearAlgebra\nfunction linsolve!(::Type{Val{:init}},f,u0; kwargs...)\n  function _linsolve!(x,A,b,update_matrix=false; kwargs...)\n    _A = lu(A)\n    ldiv!(x,_A,b)\n  end\nend","category":"page"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"This initialization function returns a linear solving function that always computes the LU-factorization and then does the solving. This method works fine and you can pass it to the methods like","category":"page"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"Rosenbrock23(linsolve=linsolve!)","category":"page"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"and it will work, but this method does not cache _A, the factorization. This means that, even if A has not changed, it will re-factorize the matrix.","category":"page"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"To change this, we can instead create a call-overloaded type. The generalized form of this is:","category":"page"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"mutable struct LinSolveFactorize{F}\n  factorization::F\n  A\nend\nLinSolveFactorize(factorization) = LinSolveFactorize(factorization,nothing)\nfunction (p::LinSolveFactorize)(x,A,b,matrix_updated=false)\n  if matrix_updated\n    p.A = p.factorization(A)\n  end\n  A_ldiv_B!(x,p.A,b)\nend\nfunction (p::LinSolveFactorize)(::Type{Val{:init}},f,u0_prototype)\n  LinSolveFactorize(p.factorization,nothing)\nend\nlinsolve = LinSolveFactorize(lufact!)","category":"page"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"LinSolveFactorize is a type which holds the factorization method and the pre-factorized matrix. When linsolve is passed to the ODE/SDE/etc. solver, it will use the function linsolve(Val{:init},f,u0_prototype) to create a LinSolveFactorize object which holds the factorization method and a cache for holding a factorized matrix. Then","category":"page"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"function (p::LinSolveFactorize)(x,A,b,matrix_updated=false)\n  if matrix_updated\n    p.A = p.factorization(A)\n  end\n  A_ldiv_B!(x,p.A,b)\nend","category":"page"},{"location":"features/linear_nonlinear/","page":"Specifying (Non)Linear Solvers","title":"Specifying (Non)Linear Solvers","text":"is what's used in the solver's internal loop. If matrix_updated is true, it will re-compute the factorization. Otherwise it just solves the linear system with the cached factorization. This general idea of using a call-overloaded type can be employed to do many other things.","category":"page"},{"location":"extras/sensitivity_math/#sensitivity_math","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"","category":"section"},{"location":"extras/sensitivity_math/#Forward-Sensitivity-Analysis","page":"Mathematics of Sensitivity Analysis","title":"Forward Sensitivity Analysis","text":"","category":"section"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"The local sensitivity is computed using the sensitivity ODE:","category":"page"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"fracddtfracpartial upartial p_j=fracpartial fpartial ufracpartial upartial p_j+fracpartial fpartial p_j=Jcdot S_j+F_j","category":"page"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"where","category":"page"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"J=left(beginarraycccc\r\nfracpartial f_1partial u_1  fracpartial f_1partial u_2  cdots  fracpartial f_1partial u_k\r\nfracpartial f_2partial u_1  fracpartial f_2partial u_2  cdots  fracpartial f_2partial u_k\r\ncdots  cdots  cdots  cdots\r\nfracpartial f_kpartial u_1  fracpartial f_kpartial u_2  cdots  fracpartial f_kpartial u_k\r\nendarrayright)","category":"page"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"is the Jacobian of the system,","category":"page"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"F_j=left(beginarrayc\r\nfracpartial f_1partial p_j\r\nfracpartial f_2partial p_j\r\nvdots\r\nfracpartial f_kpartial p_j\r\nendarrayright)","category":"page"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"are the parameter derivatives, and","category":"page"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"S_j=left(beginarrayc\r\nfracpartial u_1partial p_j\r\nfracpartial u_2partial p_j\r\nvdots\r\nfracpartial u_kpartial p_j\r\nendarrayright)","category":"page"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"is the vector of sensitivities. Since this ODE is dependent on the values of the independent variables themselves, this ODE is computed simultaneously with the actual ODE system.","category":"page"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"Note that the Jacobian-vector product","category":"page"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"fracpartial fpartial ufracpartial upartial p_j","category":"page"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"can be computed without forming the Jacobian. With finite differences, this through using the following formula for the directional derivative","category":"page"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"Jv approx fracf(x+v epsilon) - f(x)epsilon","category":"page"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"or, alternatively and without truncation error, by using a dual number with a single partial dimension, d = x + v epsilon we get that","category":"page"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"f(d) = f(x) + Jv epsilon","category":"page"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"as a fast way to calcuate Jv. Thus, except when a sufficiently good function for J is given by the user, the Jacobian is never formed. For more details, consult the MIT 18.337 lecture notes on forward mode AD.","category":"page"},{"location":"extras/sensitivity_math/#Adjoint-Sensitivity-Analysis","page":"Mathematics of Sensitivity Analysis","title":"Adjoint Sensitivity Analysis","text":"","category":"section"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"This adjoint requires the definition of some scalar functional g(up) where u(tp) is the (numerical) solution to the differential equation ddt u(tp)=f(tup) with tin 0T and u(t_0p)=u_0. Adjoint sensitivity analysis finds the gradient of","category":"page"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"G(up)=G(u(cdotp))=int_t_0^Tg(u(tp)p)dt","category":"page"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"some integral of the solution. It does so by solving the adjoint problem","category":"page"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"fracdlambda^stardt=g_u(u(tp)p)-lambda^star(t)f_u(tu(tp)p)thinspacethinspacethinspacelambda^star(T)=0","category":"page"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"where f_u is the Jacobian of the system with respect to the state u while f_p is the Jacobian with respect to the parameters. The adjoint problem's solution gives the sensitivities through the integral:","category":"page"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"fracdGdp=int_t_0^Tlambda^star(t)f_p(t)+g_p(t)dt+lambda^star(t_0)u_p(t_0)","category":"page"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"Notice that since the adjoints require the Jacobian of the system at the state, it requires the ability to evaluate the state at any point in time. Thus it requires the continuous forward solution in order to solve the adjoint solution, and the adjoint solution is required to be continuous in order to calculate the resulting integral.","category":"page"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"There is one extra detail to consider. In many cases we would like to calculate the adjoint sensitivity of some discontinuous functional of the solution. One canonical function is the L2 loss against some data points, that is:","category":"page"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"L(up)=sum_i=1^nVerttildeu(t_i)-u(t_ip)Vert^2","category":"page"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"In this case, we can reinterpret our summation as the distribution integral:","category":"page"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"G(up)=int_0^Tsum_i=1^nVerttildeu(t_i)-u(t_ip)Vert^2delta(t_i-t)dt","category":"page"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"where δ is the Dirac distribution. In this case, the integral is continuous except at finitely many points. Thus it can be calculated between each t_i. At a given t_i, given that the t_i are unique, we have that","category":"page"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"g_u(t_i)=2left(tildeu(t_i)-u(t_ip)right)","category":"page"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"Thus the adjoint solution lambda^star(t) is given by integrating between the integrals and applying the jump function g_u at every data point t_i.","category":"page"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"We note that","category":"page"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"lambda^star(t)f_u(t)","category":"page"},{"location":"extras/sensitivity_math/","page":"Mathematics of Sensitivity Analysis","title":"Mathematics of Sensitivity Analysis","text":"is a vector-transpose Jacobian product, also known as a vjp, which can be efficiently computed using the pullback of backpropogation on the user function f with a forward pass at u with a pullback vector lambda^star. For more information, consult the MIT 18.337 lecture notes on reverse mode AD","category":"page"},{"location":"features/diffeq_arrays/#diffeq_arrays","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"","category":"section"},{"location":"features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"In many cases, a standard array may not be enough to fully hold the data for a model. Many of the solvers in DifferentialEquations.jl (only the native Julia methods) allow you to solve problems on AbstractArray types which allow you to extend the meaning of an array. This page describes some of the AbstractArray types which can be helpful for modeling differential equations problems.","category":"page"},{"location":"features/diffeq_arrays/#ArrayPartitions","page":"DiffEq-Specific Array Types","title":"ArrayPartitions","text":"","category":"section"},{"location":"features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"ArrayPartitions in DiffEq are used for heterogeneous arrays. For example, DynamicalODEProblem solvers use them internally to turn the separate parts into a single array. You can construct an ArrayPartition using RecursiveArrayTools.jl:","category":"page"},{"location":"features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"using RecursiveArrayTools\nA = ArrayPartition(x::AbstractArray...)","category":"page"},{"location":"features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"where x is an array of arrays. Then, A will act like a single array, and its broadcast will be type stable, allowing for it to be used inside of the native Julia DiffEq solvers in an efficient way. This is a good way to generate an array which has different units for different parts, or different amounts of precision.","category":"page"},{"location":"features/diffeq_arrays/#Usage","page":"DiffEq-Specific Array Types","title":"Usage","text":"","category":"section"},{"location":"features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"An ArrayPartition acts like a single array. A[i] indexes through the first array, then the second, etc. all linearly. But A.x is where the arrays are stored. Thus for","category":"page"},{"location":"features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"using RecursiveArrayTools\nA = ArrayPartition(y,z)","category":"page"},{"location":"features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"We would have A.x[1]==y and A.x[2]==z. Broadcasting like f.(A) is efficient.","category":"page"},{"location":"features/diffeq_arrays/#Example:-Dynamics-Equations","page":"DiffEq-Specific Array Types","title":"Example: Dynamics Equations","text":"","category":"section"},{"location":"features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"In this example we will show using heterogeneous units in dynamics equations. Our arrays will be:","category":"page"},{"location":"features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"using Unitful, RecursiveArrayTools, DiffEqBase, OrdinaryDiffEq\nusing LinearAlgebra\n\nr0 = [1131.340, -2282.343, 6672.423]u\"km\"\nv0 = [-5.64305, 4.30333, 2.42879]u\"km/s\"\nΔt = 86400.0*365u\"s\"\nμ = 398600.4418u\"km^3/s^2\"\nrv0 = ArrayPartition(r0,v0)","category":"page"},{"location":"features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"Here, r0 is the initial positions, and v0 are the initial velocities. rv0 is the ArrayPartition initial condition. We now write our update function in terms of the ArrayPartition:","category":"page"},{"location":"features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"function f(dy, y, μ, t)\n    r = norm(y.x[1])\n    dy.x[1] .= y.x[2]\n    dy.x[2] .= -μ .* y.x[1] / r^3\nend","category":"page"},{"location":"features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"Notice that y.x[1] is the r part of y, and y.x[2] is the v part of y. Using this kind of indexing is type stable, even though the array itself is heterogeneous. Note that one can also use things like 2y or y.+x and the broadcasting will be efficient.","category":"page"},{"location":"features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"Now to solve our equations, we do the same thing as always in DiffEq:","category":"page"},{"location":"features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"prob = ODEProblem(f, rv0, (0.0u\"s\", Δt), μ)\nsol = solve(prob, Vern8())","category":"page"},{"location":"features/diffeq_arrays/#MultiScaleArrays","page":"DiffEq-Specific Array Types","title":"MultiScaleArrays","text":"","category":"section"},{"location":"features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"The multi-scale modeling functionality is provided by MultiScaleArrays.jl. It allows for designing a multi-scale model as an extension of an array, which in turn can be directly used in the native Julia solvers of DifferentialEquations.jl.","category":"page"},{"location":"features/diffeq_arrays/","page":"DiffEq-Specific Array Types","title":"DiffEq-Specific Array Types","text":"For more information, please see the MultiScaleArrays.jl README.","category":"page"},{"location":"tutorials/advanced_ode_example/#stiff","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"","category":"section"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"This tutorial is for getting into the extra features for solving stiff ordinary differential equations in an efficient manner. Solving stiff ordinary differential equations requires specializing the linear solver on properties of the Jacobian in order to cut down on the mathcalO(n^3) linear solve and the mathcalO(n^2) back-solves. Note that these same functions and controls also extend to stiff SDEs, DDEs, DAEs, etc.","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"note: Note\nThis tutorial is for advanced users to dive into advanced features!","category":"page"},{"location":"tutorials/advanced_ode_example/#Code-Optimization-for-Differential-Equations","page":"Solving Stiff Equations","title":"Code Optimization for Differential Equations","text":"","category":"section"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"Solving stiff differential equations requires speed. Here's a few things to keep in mind.","category":"page"},{"location":"tutorials/advanced_ode_example/#Writing-Efficient-Code","page":"Solving Stiff Equations","title":"Writing Efficient Code","text":"","category":"section"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"For a detailed tutorial on how to optimize one's DifferentialEquations.jl code, please see the Optimizing DiffEq Code tutorial.","category":"page"},{"location":"tutorials/advanced_ode_example/#Choosing-a-Good-Solver","page":"Solving Stiff Equations","title":"Choosing a Good Solver","text":"","category":"section"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"Choosing a good solver is required for getting top notch speed. General recommendations can be found on the solver page (for example, the ODE Solver Recommendations). The current recommendations can be simplified to a Rosenbrock method (Rosenbrock23 or Rodas5) for smaller (<50 ODEs) problems, ESDIRK methods for slightly larger (TRBDF2 or KenCarp4 for <2000 ODEs), and QNDF for even larger problems. lsoda from LSODA.jl is sometimes worth a try.","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"More details on the solver to choose can be found by benchmarking. See the DiffEqBenchmarks to compare many solvers on many problems.","category":"page"},{"location":"tutorials/advanced_ode_example/#Check-Out-the-Speed-FAQ","page":"Solving Stiff Equations","title":"Check Out the Speed FAQ","text":"","category":"section"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"See this FAQ for information on common pitfalls and how to improve performance.","category":"page"},{"location":"tutorials/advanced_ode_example/#Setting-Up-Your-Julia-Installation-for-Speed","page":"Solving Stiff Equations","title":"Setting Up Your Julia Installation for Speed","text":"","category":"section"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"Julia uses an underlying BLAS implementation for its matrix multiplications and factorizations. This library is automatically multithreaded and accelerates the internal linear algebra of DifferentialEquations.jl. However, for optimality, you should make sure that the number of BLAS threads that you are using matches the number of physical cores and not the number of logical cores. See this issue for more details.","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"To check the number of BLAS threads, use:","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"ccall((:openblas_get_num_threads64_, Base.libblas_name), Cint, ())","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"If I want to set this directly to 4 threads, I would use:","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"using LinearAlgebra\r\nLinearAlgebra.BLAS.set_num_threads(4)","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"Additionally, in some cases Intel's MKL might be a faster BLAS than the standard BLAS that ships with Julia (OpenBLAS). To switch your BLAS implementation, you can use MKL.jl which will accelerate the linear algebra routines. Please see the package for the limitations.","category":"page"},{"location":"tutorials/advanced_ode_example/#Use-Accelerator-Hardware","page":"Solving Stiff Equations","title":"Use Accelerator Hardware","text":"","category":"section"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"When possible, use GPUs. If your ODE system is small and you need to solve it with very many different parameters, see the ensembles interface and DiffEqGPU.jl. If your problem is large, consider using a CuArray for the state to allow for GPU-parallelism of the internal linear algebra.","category":"page"},{"location":"tutorials/advanced_ode_example/#Speeding-Up-Jacobian-Calculations","page":"Solving Stiff Equations","title":"Speeding Up Jacobian Calculations","text":"","category":"section"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"When one is using an implicit or semi-implicit differential equation solver, the Jacobian must be built at many iterations and this can be one of the most expensive steps. There are two pieces that must be optimized in order to reach maximal efficiency when solving stiff equations: the sparsity pattern and the construction of the Jacobian. The construction is filling the matrix J with values, while the sparsity pattern is what J to use.","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"The sparsity pattern is given by a prototype matrix, the jac_prototype, which will be copied to be used as J. The default is for J to be a Matrix, i.e. a dense matrix. However, if you know the sparsity of your problem, then you can pass a different matrix type. For example, a SparseMatrixCSC will give a sparse matrix. Additionally, structured matrix types like Tridiagonal, BandedMatrix (from BandedMatrices.jl), BlockBandedMatrix (from BlockBandedMatrices.jl), and more can be given. DifferentialEquations.jl will internally use this matrix type, making the factorizations faster by utilizing the specialized forms.","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"For the construction, there are 3 ways to fill J:","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"The default, which uses normal finite/automatic differentiation\nA function jac(J,u,p,t) which directly computes the values of J\nA colorvec which defines a sparse differentiation scheme.","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"We will now showcase how to make use of this functionality with growing complexity.","category":"page"},{"location":"tutorials/advanced_ode_example/#Declaring-Jacobian-Functions","page":"Solving Stiff Equations","title":"Declaring Jacobian Functions","text":"","category":"section"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"Let's solve the Rober equations:","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"beginaligned\r\nfracdy_1dt = -004y₁ + 10^4 y_2 y_3 \r\nfracdy_2dt = 004 y_1 - 10^4 y_2 y_3 - 3*10^7 y_2^2 \r\nfracdy_3dt = 3*10^7 y_2^2 \r\nendaligned","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"In order to reduce the Jacobian construction cost, one can describe a Jacobian function by using the jac argument for the ODEFunction. First, let's do a standard ODEProblem:","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"using DifferentialEquations\r\nfunction rober(du,u,p,t)\r\n  y₁,y₂,y₃ = u\r\n  k₁,k₂,k₃ = p\r\n  du[1] = -k₁*y₁+k₃*y₂*y₃\r\n  du[2] =  k₁*y₁-k₂*y₂^2-k₃*y₂*y₃\r\n  du[3] =  k₂*y₂^2\r\n  nothing\r\nend\r\nprob = ODEProblem(rober,[1.0,0.0,0.0],(0.0,1e5),[0.04,3e7,1e4])\r\nsol = solve(prob)\r\nplot(sol,tspan=(1e-2,1e5),xscale=:log10)","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"(Image: IntroDAEPlot)","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"julia> using BenchmarkTools\r\njulia> @btime solve(prob)\r\n415.800 μs (3053 allocations: 161.64 KiB)","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"Now we want to add the Jacobian. First we have to derive the Jacobian fracdf_idu_j which is J[i,j]. From this we get:","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"function rober_jac(J,u,p,t)\r\n  y₁,y₂,y₃ = u\r\n  k₁,k₂,k₃ = p\r\n  J[1,1] = k₁ * -1\r\n  J[2,1] = k₁\r\n  J[3,1] = 0\r\n  J[1,2] = y₃ * k₃\r\n  J[2,2] = y₂ * k₂ * -2 + y₃ * k₃ * -1\r\n  J[3,2] = y₂ * 2 * k₂\r\n  J[1,3] = k₃ * y₂\r\n  J[2,3] = k₃ * y₂ * -1\r\n  J[3,3] = 0\r\n  nothing\r\nend\r\nf = ODEFunction(rober, jac=rober_jac)\r\nprob_jac = ODEProblem(f,[1.0,0.0,0.0],(0.0,1e5),(0.04,3e7,1e4))","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"julia> @btime solve(prob_jac)\r\n305.400 μs (2599 allocations: 153.11 KiB)","category":"page"},{"location":"tutorials/advanced_ode_example/#Automatic-Derivation-of-Jacobian-Functions","page":"Solving Stiff Equations","title":"Automatic Derivation of Jacobian Functions","text":"","category":"section"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"But that was hard! If you want to take the symbolic Jacobian of numerical code, we can make use of ModelingToolkit.jl to symbolicify the numerical code and do the symbolic calculation and return the Julia code for this.","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"using ModelingToolkit\r\nde = modelingtoolkitize(prob)\r\nModelingToolkit.generate_jacobian(de)[2] # Second is in-place","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"which outputs:","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":":((##MTIIPVar#376, u, p, t)->begin\r\n          #= C:\\Users\\accou\\.julia\\packages\\ModelingToolkit\\czHtj\\src\\utils.jl:65 =#\r\n          #= C:\\Users\\accou\\.julia\\packages\\ModelingToolkit\\czHtj\\src\\utils.jl:66 =#\r\n          let (x₁, x₂, x₃, α₁, α₂, α₃) = (u[1], u[2], u[3], p[1], p[2], p[3])\r\n              ##MTIIPVar#376[1] = α₁ * -1\r\n              ##MTIIPVar#376[2] = α₁\r\n              ##MTIIPVar#376[3] = 0\r\n              ##MTIIPVar#376[4] = x₃ * α₃\r\n              ##MTIIPVar#376[5] = x₂ * α₂ * -2 + x₃ * α₃ * -1\r\n              ##MTIIPVar#376[6] = x₂ * 2 * α₂\r\n              ##MTIIPVar#376[7] = α₃ * x₂\r\n              ##MTIIPVar#376[8] = α₃ * x₂ * -1\r\n              ##MTIIPVar#376[9] = 0\r\n          end\r\n          #= C:\\Users\\accou\\.julia\\packages\\ModelingToolkit\\czHtj\\src\\utils.jl:67 =#\r\n          nothing\r\n      end)","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"Now let's use that to give the analytical solution Jacobian:","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"jac = eval(ModelingToolkit.generate_jacobian(de)[2])\r\nf = ODEFunction(rober, jac=jac)\r\nprob_jac = ODEProblem(f,[1.0,0.0,0.0],(0.0,1e5),(0.04,3e7,1e4))","category":"page"},{"location":"tutorials/advanced_ode_example/#Declaring-a-Sparse-Jacobian","page":"Solving Stiff Equations","title":"Declaring a Sparse Jacobian","text":"","category":"section"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"Jacobian sparsity is declared by the jac_prototype argument in the ODEFunction. Note that you should only do this if the sparsity is high, for example, 0.1% of the matrix is non-zeros, otherwise the overhead of sparse matrices can be higher than the gains from sparse differentiation!","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"But as a demonstration, let's build a sparse matrix for the Rober problem. We can do this by gathering the I and J pairs for the non-zero components, like:","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"I = [1,2,1,2,3,1,2]\r\nJ = [1,1,2,2,2,3,3]\r\n\r\nusing SparseArrays\r\njac_prototype = sparse(I,J,1.0)","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"Now this is the sparse matrix prototype that we want to use in our solver, which we then pass like:","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"f = ODEFunction(rober, jac=jac, jac_prototype=jac_prototype)\r\nprob_jac = ODEProblem(f,[1.0,0.0,0.0],(0.0,1e5),(0.04,3e7,1e4))","category":"page"},{"location":"tutorials/advanced_ode_example/#Automatic-Sparsity-Detection","page":"Solving Stiff Equations","title":"Automatic Sparsity Detection","text":"","category":"section"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"One of the useful companion tools for DifferentialEquations.jl is SparsityDetection.jl. This allows for automatic declaration of Jacobian sparsity types. To see this in action, let's look at the 2-dimensional Brusselator equation:","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"const N = 32\r\nconst xyd_brusselator = range(0,stop=1,length=N)\r\nbrusselator_f(x, y, t) = (((x-0.3)^2 + (y-0.6)^2) <= 0.1^2) * (t >= 1.1) * 5.\r\nlimit(a, N) = a == N+1 ? 1 : a == 0 ? N : a\r\nfunction brusselator_2d_loop(du, u, p, t)\r\n  A, B, alpha, dx = p\r\n  alpha = alpha/dx^2\r\n  @inbounds for I in CartesianIndices((N, N))\r\n    i, j = Tuple(I)\r\n    x, y = xyd_brusselator[I[1]], xyd_brusselator[I[2]]\r\n    ip1, im1, jp1, jm1 = limit(i+1, N), limit(i-1, N), limit(j+1, N), limit(j-1, N)\r\n    du[i,j,1] = alpha*(u[im1,j,1] + u[ip1,j,1] + u[i,jp1,1] + u[i,jm1,1] - 4u[i,j,1]) +\r\n                B + u[i,j,1]^2*u[i,j,2] - (A + 1)*u[i,j,1] + brusselator_f(x, y, t)\r\n    du[i,j,2] = alpha*(u[im1,j,2] + u[ip1,j,2] + u[i,jp1,2] + u[i,jm1,2] - 4u[i,j,2]) +\r\n                A*u[i,j,1] - u[i,j,1]^2*u[i,j,2]\r\n    end\r\nend\r\np = (3.4, 1., 10., step(xyd_brusselator))","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"Given this setup, we can give and example input and output and call jacobian_sparsity on our function with the example arguments and it will kick out a sparse matrix with our pattern, that we can turn into our jac_prototype.","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"using SparsityDetection, SparseArrays\r\ninput = rand(32,32,2)\r\noutput = similar(input)\r\nsparsity_pattern = jacobian_sparsity(brusselator_2d_loop,output,input,p,0.0)\r\njac_sparsity = Float64.(sparse(sparsity_pattern))","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"Let's double check what our sparsity pattern looks like:","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"using Plots\r\nspy(jac_sparsity,markersize=1,colorbar=false,color=:deep)","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"(Image: Bruss Sparsity)","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"That's neat, and would be tedius to build by hand! Now we just pass it to the ODEFunction like as before:","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"f = ODEFunction(brusselator_2d_loop;jac_prototype=jac_sparsity)","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"Build the ODEProblem:","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"function init_brusselator_2d(xyd)\r\n  N = length(xyd)\r\n  u = zeros(N, N, 2)\r\n  for I in CartesianIndices((N, N))\r\n    x = xyd[I[1]]\r\n    y = xyd[I[2]]\r\n    u[I,1] = 22*(y*(1-y))^(3/2)\r\n    u[I,2] = 27*(x*(1-x))^(3/2)\r\n  end\r\n  u\r\nend\r\nu0 = init_brusselator_2d(xyd_brusselator)\r\nprob_ode_brusselator_2d = ODEProblem(brusselator_2d_loop,\r\n                                     u0,(0.,11.5),p)\r\n\r\nprob_ode_brusselator_2d_sparse = ODEProblem(f,\r\n                                     u0,(0.,11.5),p)","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"Now let's see how the version with sparsity compares to the version without:","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"@btime solve(prob_ode_brusselator_2d,save_everystep=false) # 51.714 s (7317 allocations: 70.12 MiB)\r\n@btime solve(prob_ode_brusselator_2d_sparse,save_everystep=false) # 2.880 s (55533 allocations: 885.09 MiB)\r\n@btime solve(prob_ode_brusselator_2d_sparse,TRBDF2(),save_everystep=false) # 1.185 s (55081 allocations: 347.79 MiB)","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"From some automated tooling an a choice of an algorithm, we went from almost a minute to almost a second!","category":"page"},{"location":"tutorials/advanced_ode_example/#Defining-Linear-Solver-Routines-and-Jacobian-Free-Newton-Krylov","page":"Solving Stiff Equations","title":"Defining Linear Solver Routines and Jacobian-Free Newton-Krylov","text":"","category":"section"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"A completely different way to optimize the linear solvers for large sparse matrices is to use a Krylov subpsace method. This requires choosing a linear solver for changing to a Krylov method. Optionally, one can use a Jacobian-free operator to reduce the memory requirements.","category":"page"},{"location":"tutorials/advanced_ode_example/#Declaring-a-Jacobian-Free-Newton-Krylov-Implementation","page":"Solving Stiff Equations","title":"Declaring a Jacobian-Free Newton-Krylov Implementation","text":"","category":"section"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"To swap the linear solver out, we use the linsolve command and choose the GMRES linear solver.","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"@btime solve(prob_ode_brusselator_2d,TRBDF2(linsolve=LinSolveGMRES()),save_everystep=false) # 469.174 s (1266049 allocations: 120.80 MiB)\r\n@btime solve(prob_ode_brusselator_2d_sparse,TRBDF2(linsolve=LinSolveGMRES()),save_everystep=false) # 10.928 s (1327264 allocations: 59.92 MiB)","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"For more information on linear solver choices, see the linear solver documentation.","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"We can also enhance this by using a Jacobian-Free implementation of f'(x)*v. To define the Jacobian-Free operator, we can use DiffEqOperators.jl to generate an operator JacVecOperator such that Jv*v performs f'(x)*v without building the Jacobian matrix.","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"using DiffEqOperators\r\nJv = JacVecOperator(brusselator_2d_loop,u0,p,0.0)","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"and then we can use this by making it our jac_prototype:","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"f2 = ODEFunction(brusselator_2d_loop;jac_prototype=Jv);\r\nprob_ode_brusselator_2d_jacfree = ODEProblem(f2,u0,(0.,11.5),p);\r\n@btime solve(prob_ode_brusselator_2d_jacfree,TRBDF2(linsolve=LinSolveGMRES()),save_everystep=false) # 8.352 s (1875298 allocations: 78.86 MiB)","category":"page"},{"location":"tutorials/advanced_ode_example/#Adding-a-Preconditioner","page":"Solving Stiff Equations","title":"Adding a Preconditioner","text":"","category":"section"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"The linear solver documentation shows how you can add a preconditioner to the GMRES. For example, you can use packages like AlgebraicMultigrid.jl to add an algebraic multigrid (AMG) or IncompleteLU.jl for an incomplete LU-factorization (iLU).","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"using AlgebraicMultigrid\r\npc = aspreconditioner(ruge_stuben(jac_sparsity));\r\n@btime solve(prob_ode_brusselator_2d_jacfree,TRBDF2(linsolve=LinSolveGMRES(Pl=pc)),save_everystep=false) # 5.247 s (233048 allocations: 139.27 MiB)","category":"page"},{"location":"tutorials/advanced_ode_example/#Using-Structured-Matrix-Types","page":"Solving Stiff Equations","title":"Using Structured Matrix Types","text":"","category":"section"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"If your sparsity pattern follows a specific structure, for example a banded matrix, then you can declare jac_prototype to be of that structure and then additional optimizations will come for free. Note that in this case, it is not necessary to provide a colorvec since the color vector will be analytically derived from the structure of the matrix.","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"The matrices which are allowed are those which satisfy the ArrayInterface.jl interface for automatically-colorable matrices. These include:","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"Bidiagonal\nTridiagonal\nSymTridiagonal\nBandedMatrix (BandedMatrices.jl)\nBlockBandedMatrix (BlockBandedMatrices.jl)","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"Matrices which do not satisfy this interface can still be used, but the matrix coloring will not be automatic, and an appropriate linear solver may need to be given (otherwise it will default to attempting an LU-decomposition).","category":"page"},{"location":"tutorials/advanced_ode_example/#Sundials-Specific-Handling","page":"Solving Stiff Equations","title":"Sundials-Specific Handling","text":"","category":"section"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"While much of the setup makes the transition to using Sundials automatic, there are some differences between the pure Julia implementations and the Sundials implementations which must be taken note of. These are all detailed in the Sundials solver documentation, but here we will highlight the main details which one should make note of.","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"Defining a sparse matrix and a Jacobian for Sundials works just like any other package. The core difference is in the choice of the linear solver. With Sundials, the linear solver choice is done with a Symbol in the linear_solver from a preset list. Particular choices of note are :Band for a banded matrix and :GMRES for using GMRES. If you are using Sundials, :GMRES will not require defining the JacVecOperator, and instead will always make use of a Jacobian-Free Newton Krylov (with numerical differentiation). Thus on this problem we could do:","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"using Sundials\r\n# Sparse Version\r\n@btime solve(prob_ode_brusselator_2d_sparse,CVODE_BDF(),save_everystep=false) # 42.804 s (51388 allocations: 3.20 MiB)\r\n# GMRES Version: Doesn't require any extra stuff!\r\n@btime solve(prob_ode_brusselator_2d,CVODE_BDF(linear_solver=:GMRES),save_everystep=false) # 485.671 ms (61058 allocations: 3.63 MiB)","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"Details for setting up a preconditioner with Sundials can be found at the Sundials solver page.","category":"page"},{"location":"tutorials/advanced_ode_example/#Handling-Mass-Matrices","page":"Solving Stiff Equations","title":"Handling Mass Matrices","text":"","category":"section"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"Instead of just defining an ODE as u = f(upt), it can be common to express the differential equation in the form with a mass matrix:","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"Mu = f(upt)","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"where M is known as the mass matrix. Let's solve the Robertson equation. At the top we wrote this equation as:","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"beginaligned\r\ndy_1 = -004 y_1 + 10^4 y_2 y_3 \r\ndy_2 =  004 y_1 - 10^4 y_2 y_3 - 3*10^7 y_2^2 \r\ndy_3 = 3*10^7 y_2^2 \r\nendaligned","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"But we can instead write this with a conservation relation:","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"beginaligned\r\nfracdy_1dt = -004 y_1 + 10^4 y_2 y_3 \r\nfracdy_2dt =  004 y_1 - 10^4 y_2 y_3 - 3*10^7 y_2^2 \r\n1 =  y_1 + y_2 + y_3 \r\nendaligned","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"In this form, we can write this as a mass matrix ODE where M is singular (this is another form of a differential-algebraic equation (DAE)). Here, the last row of M is just zero. We can implement this form as:","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"using DifferentialEquations\r\nfunction rober(du,u,p,t)\r\n  y₁,y₂,y₃ = u\r\n  k₁,k₂,k₃ = p\r\n  du[1] = -k₁*y₁ + k₃*y₂*y₃\r\n  du[2] =  k₁*y₁ - k₃*y₂*y₃ - k₂*y₂^2\r\n  du[3] =  y₁ + y₂ + y₃ - 1\r\n  nothing\r\nend\r\nM = [1. 0  0\r\n     0  1. 0\r\n     0  0  0]\r\nf = ODEFunction(rober,mass_matrix=M)\r\nprob_mm = ODEProblem(f,[1.0,0.0,0.0],(0.0,1e5),(0.04,3e7,1e4))\r\nsol = solve(prob_mm,Rodas5(),reltol=1e-8,abstol=1e-8)\r\n\r\nplot(sol, xscale=:log10, tspan=(1e-6, 1e5), layout=(3,1))","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"(Image: IntroDAEPlot)","category":"page"},{"location":"tutorials/advanced_ode_example/","page":"Solving Stiff Equations","title":"Solving Stiff Equations","text":"note: Note\nIf your mass matrix is singular, i.e. your system is a DAE, then you need to make sure you choose a solver that is compatible with DAEs","category":"page"},{"location":"models/chemical_reactions/#Chemical-Reactions","page":"Chemical Reactions","title":"Chemical Reactions","text":"","category":"section"},{"location":"models/chemical_reactions/","page":"Chemical Reactions","title":"Chemical Reactions","text":"Catalyst.jl is a domain specific  language (DSL) for the easy generation of models of chemical reaction systems,  which can be used with SciML tooling to enable high performance simulation and  analysis of chemical reaction networks. Catalyst generates ReactionSystems,  leveraging ModelingToolkit to  enable large-scale simulations through auto-vectorization and parallelism.  ReactionSystems can be used to generate ModelingToolkit-based models,  allowing the easy simulation and parameter estimation of mass action ODE  models, Chemical Langevin SDE models, stochastic chemical kinetics jump process  models, and more. These generated models can be used with  DifferentialEquations.jl solvers, but also with higher level SciML packages  (e.g. for sensitivity analysis, parameter estimation, machine learning  applications, etc). See the Catalyst.jl  documentation for more information.","category":"page"},{"location":"tutorials/discrete_stochastic_example/#Continuous-Time-Jump-Processes-and-Gillespie-Methods","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"","category":"section"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"In this tutorial we will describe how to define and simulate continuous-time jump processes, also known in biological fields as stochastic chemical kinetics (i.e. Gillespie) models. ","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"note: Note\nThis tutorial assumes you have read the Ordinary Differential Equations tutorial. ","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"The discrete stochastic simulations we consider are a form of jump equation with a \"trivial\" (non-existent) differential equation. We will first demonstrate how to build these types of models using the biological modeling functionality of Catalyst.jl, then describe how to build them directly and more generally using DiffEqJump.jl jump types, and finally show how to couple discrete stochastic simulations to differential equation models.","category":"page"},{"location":"tutorials/discrete_stochastic_example/#Illustrative-Model:-SIR-disease-dynamics","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Illustrative Model: SIR disease dynamics","text":"","category":"section"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"To illustrate the jump process solvers, we will build an SIR model which matches the tutorial from Gillespie.jl. SIR stands for susceptible, infected, and recovered, and is a model of disease spread. When a susceptible person comes in contact with an infected person, the disease has a chance of infecting the susceptible person. This \"chance\" is determined by the number of susceptible persons and the number of infected persons, since in larger populations there is a greater chance that two people come into contact. Every infected person will in turn have a rate at which they recover. In our model we'll assume there are no births or deaths, and a recovered individual is protected from reinfection.","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"We'll begin by giving the mathematical equations for the jump processes of the number of susceptible (S(t)), number of infected (I(t)), and number of recovered (R(t)). In the next section we give a more intuitive and biological description of the model for users that are less familiar with jump processes. Let Y_i(t), i = 12, denote independent unit Poisson processes. Our basic mathematical model for the evolution of (S(t)I(t)R(t)), written using Kurtz's time-change representation, is then","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"beginaligned\nS(t) = S(0) - Y_1left(  int_0^t beta S(s^-) I(s^-)  dsright) \nI(t) = I(0) + Y_1left(  int_0^t beta S(s^-) I(s^-)  dsright) \n        - Y_2 left( int_0^t nu I(s^-)   ds right) \nR(t) = R(0) + Y_2 left( int_0^t nu I(s^-)   ds right)\nendaligned","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Notice, our model involves two jumps with rate functions, also known as intensities or propensities, given by beta S(t) I(t) and nu I(t) respectively. These model the infection of susceptible individuals and recovery of infected individuals.","category":"page"},{"location":"tutorials/discrete_stochastic_example/#Defining-the-SIR-Model-using-Reactions-via-Catalyst","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Defining the SIR Model using Reactions via Catalyst","text":"","category":"section"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"For those less-familiar with the time-change representation, we now give a more intuitive explanation of the model, and then demonstrate how it can be written as a serious of chemical reactions in Catalyst.jl and seamlessly converted into a form that can be used with the DiffEqJump.jl solvers. Users interested in how to directly define jumps using the lower-level DiffEqJump interface can skip to Building and Simulating the Jump Process using the DiffEqJump Low-level Interface.","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"The SIR model described above involves two basic chemical reactions,","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"beginaligned\nS + I oversetbetato 2 I \nI oversetnuto R\nendaligned","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"where beta and nu are the rate constants of the reactions (with units of probability per time). In a jump process (stochastic chemical kinetics) model, we keep track of the non-negative integer number of each species at each time (i.e. (S(t) I(t) R(t)) above). Each reaction has an associated rate function (i.e. intensity or propensity) giving the probability per time it can occur when the system is in state (S(t)I(t)R(t)):","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"beginmatrix\ntextReaction  textRate Functions \nhline\nS + I oversetbetato 2 I  beta S(t) I(t) \nI oversetnuto R  nu I(t)\nendmatrix","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"beta is determined by factors like the type of the disease. It can be interpreted as the probability per time one pair of susceptible and infected people encounter each other, with the susceptible person becoming sick. The overall rate (i.e. probability per time) that some susceptible person gets sick is then given by the rate constant multiplied by the number of possible pairs of susceptible and infected people. This formulation is known as the law of mass action. Similarly, we have that each individual infected person is assumed to recover with probability per time nu, so that the probability per time some infected person becomes recovered is nu times the number of infected people, i.e. nu I(t). ","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Rate functions give the probability per time for each of the two types of jumps to occur, and hence determine when the state of our system changes. To fully specify our model we also need to specify how the state changes when a jump occurs, giving what are called affect functions in DiffEqJump. For example, when the S + I to 2 I reaction occurs and some susceptible person becomes infected, the subsequent (instantaneous) state change is that","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"beginaligned\nS to S - 1  I to I + 1\nendaligned","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Likewise, when the I to R reaction occurs so that some infected person becomes recovered the state change is","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"beginaligned\nI to I - 1  R to R + 1\nendaligned","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Using Catalyst.jl we can input our full reaction network in a form that can be easily used with DiffEqJump's solvers:","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"# ]add Catalyst\nusing Catalyst\nsir_model = @reaction_network begin\n    β, S + I --> 2I\n    ν, I --> R\nend β ν","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Notice that the order the variables are introduced in the model is S, then I, then R, and thus this is the canonical ordering of the variables.","category":"page"},{"location":"tutorials/discrete_stochastic_example/#Building-and-Simulating-the-Jump-Process-from-Catalyst-Models","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Building and Simulating the Jump Process from Catalyst Models","text":"","category":"section"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"First, we have to define some kind of differential equation that we can \"solve\" to simulate the jump process. Since we want integer, discrete changes in the numbers of the different types of people, we will build a DiscreteProblem. We do this by giving the constructor u0, the initial condition, and tspan, the timespan. Here, we will start with 999 susceptible people, 1 infected person, and 0 recovered people, and solve the problem from t=0.0 to t=250.0. We use the parameters β = 0.1/1000 and ν = 0.01. Thus we build the problem via:","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"p     = (0.1/1000,0.01)   \nu₀    = [999,1,0]\ntspan = (0.0,250.0)\nprob  = DiscreteProblem(sir_model, u₀, tspan, p)","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Notice, the initial populations are integers since we want the exact number of people in the different states.","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"The Catalyst reaction network can be converted into various DifferentialEquations.jl problem types, including JumpProblems, ODEProblems, or SDEProblems. To turn it into a jump problem representing the SIR jump process model, we load DiffEqJump and simply do:","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"using DiffEqJump\njump_prob = JumpProblem(sir_model, prob, Direct())","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Here Direct() indicates that we will determine the random times and types of reactions using Gillespie's Direct stochastic simulation algorithm (SSA). See Constant Rate Jump Aggregators below for other supported SSAs.","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"We now have a problem that can be evolved in time using the DiffEqJump solvers. Since our model is a pure jump process (no continuously-varying components), we will use SSAStepper() to handle time-stepping the Direct method from jump to jump:","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"sol = solve(jump_prob, SSAStepper())","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"This solve command takes the standard commands of the common interface, and the solution object acts just like any other differential equation solution. Thus there exists a plot recipe, which we can plot with:","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"using Plots; plot(sol)","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"(Image: SIR Solution)","category":"page"},{"location":"tutorials/discrete_stochastic_example/#Building-and-Simulating-the-Jump-Process-using-the-DiffEqJump-Low-level-Interface","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Building and Simulating the Jump Process using the DiffEqJump Low-level Interface","text":"","category":"section"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"We now show how to directly use DiffEqJump's low-level interface to construct and solve our jump process model for (S(t)I(t)R(t)). Each individual jump that can occur is represented through specifying two pieces of information; a rate function (i.e. intensity or propensity) for the jump and an affect function for the jump. The former gives the probability per time a particular jump can occur given the current state of the system, and hence determines the time at which jumps can happen. The later specifies the instantaneous change in the state of the system when the jump occurs. ","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"In our SIR model we have two possible jumps that can occur (one for susceptibles becoming infected and one for infected becoming recovered), with the corresponding (mathematical) rates and affects given by","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"beginmatrix\ntextRates  textAffects\nhline \nbeta S(t) I(t)  S to S - 1 I to I + 1 \nnu I(t)  I to I - 1  R to R + 1\nendmatrix","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"DiffEqJump offers three different ways to represent jumps: MassActionJump, ConstantRateJump, and VariableRateJump. Choosing which to use is a trade off between the desired generality of the rate and affect functions vs. the computational performance of the resulting simulated system. In general","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Jump Type Performance Generality\nMassActionJump Fastest Restrictive rates/affects\nConstantRateJump Somewhat Slower Much more general\nVariableRateJump Slowest Completely general","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"It is recommended to try to encode jumps using the most performant option that supports the desired generality of the underlying rate and affect functions. Below we describe the different jump types, and show how the SIR model can be formulated using first ConstantRateJumps and then MassActionJumps (VariableRateJumps are considered later).","category":"page"},{"location":"tutorials/discrete_stochastic_example/#ConstantRateJumpSect","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Defining the Jumps Directly: ConstantRateJump","text":"","category":"section"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"The constructor for a ConstantRateJump is:","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"jump = ConstantRateJump(rate, affect!)","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"where rate is a function rate(u,p,t) and affect! is a function of the integrator affect!(integrator) (for details on the integrator, see the integrator interface docs). Here u corresponds to the current state of the system; for our SIR model u[1]=S(t), u[2]=I(t) and u[3]=R(t). p corresponds to the parameters of the model, just as used for passing parameters to derivative functions in ODE solvers. Thus, to define the two possible jumps for our model we take (with β=.1/1000.0 and ν=.01).","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"using DiffEqJump\nβ = 0.1 / 1000.0; ν = .01;\np = (β,ν)\nrate1(u,p,t) = p[1]*u[1]*u[2]  # β*S*I\nfunction affect1!(integrator)\n  integrator.u[1] -= 1         # S -> S - 1\n  integrator.u[2] += 1         # I -> I + 1\nend\njump = ConstantRateJump(rate1,affect1!)\n\nrate2(u,p,t) = p[2]*u[2]      # ν*I\nfunction affect2!(integrator)\n  integrator.u[2] -= 1        # I -> I - 1\n  integrator.u[3] += 1        # R -> R + 1\nend\njump2 = ConstantRateJump(rate2,affect2!)","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"We will start with 999 susceptible people, 1 infected person, and 0 recovered people, and solve the problem from t=0.0 to t=250.0 so that","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"u₀    = [999,1,0]\ntspan = (0.0,250.0)","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Notice, the initial populations are integers since we want the exact number of people in the different states.","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Since we want integer, discrete changes in the numbers of the different types of people, we will build a DiscreteProblem. ","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"prob = DiscreteProblem(u₀, tspan, p)","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"We can then use JumpProblem from DiffEqJump to augment the discrete problem with jumps and select the stochastic simulation algorithm (SSA) to use in sampling the jump processes. To create a JumpProblem we would simply do:","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"jump_prob = JumpProblem(prob, Direct(), jump, jump2)","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Here Direct() indicates that we will determine the random times and types of jumps that occur using Gillespie's Direct stochastic simulation algorithm (SSA). See Constant Rate Jump Aggregators for other supported SSAs.","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"We now have a problem that can be evolved in time using the DiffEqJump solvers. Since our model is a pure jump process (no continuously-varying components), we will use SSAStepper() to handle time-stepping the Direct method from jump to jump:","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"sol = solve(jump_prob, SSAStepper())","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"This solve command takes the standard commands of the common interface, and the solution object acts just like any other differential equation solution. Thus there exists a plot recipe, which we can plot with:","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"using Plots; plot(sol)","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"(Image: SIR Solution)","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Note, in systems with more than a few jumps (more than ~10), it can be advantageous to use more sophisticated SSAs than Direct. For such systems it is recommended to use SortingDirect, RSSA or RSSACR, see the list of DiffEqJump SSAs at Constant Rate Jump Aggregators.","category":"page"},{"location":"tutorials/discrete_stochastic_example/#*Caution-about-Constant-Rate-Jumps*","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Caution about Constant Rate Jumps","text":"","category":"section"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"ConstantRateJumps are quite general, but they do have one restriction. They assume that the rate functions are constant at all times between two consecutive jumps of the system. i.e. any species/states or parameters that the rate function depends on must not change between the times at which two consecutive jumps occur. Such conditions are violated if one has a time dependent parameter like beta(t) or if some of the solution components, say u[2], may also evolve through a coupled ODE or SDE (see below for examples). For problems where the rate function may change between consecutive jumps, VariableRateJumps must be used.","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Thus in the examples above,","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"rate1(u,p,t) = p[1]*u[1]*u[2]\nrate2(u,p,t) = p[2]*u[2]","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"both must be constant other than changes due to some other ConstantRateJump or MassActionJump (the same restriction applies to MassActionJumps). Since these rates only change when u[1] or u[2] is changed, and u[1] and u[2] only change when one of the jumps occur, this setup is valid. However, a rate of t*p[1]*u[1]*u[2] would not be valid because the rate would change during the interval, as would p[2]*u[1]*u[4] when u[4] is the solution to a continuous problem such as an ODE or SDE. Thus one must be careful to follow this rule when choosing rates.","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"If your problem must have the rates depend on a continuously changing quantity, you need to use the VariableRateJump.","category":"page"},{"location":"tutorials/discrete_stochastic_example/#SSAStepper","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"SSAStepper","text":"","category":"section"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Any common interface algorithm can be used to perform the time-stepping since it is implemented over the callback interface. This allows for hybrid systems that mix ODEs, SDEs and jumps. In many cases we may have a pure jump system that only involves ConstantRateJumps and/or MassActionJumps (see below). When that's the case, a substantial performance benefit may be gained by using SSAStepper(). Note, SSAStepper is a more limited time-stepper which only supports discrete events, and does not allow simultaneous coupled ODEs or SDEs. It is, however, very efficient for pure jump problems.","category":"page"},{"location":"tutorials/discrete_stochastic_example/#save_positions_docs","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Reducing Memory Use: Controlling Saving Behavior","text":"","category":"section"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Note that jumps act via the callback interface which defaults to saving at each event. The reason is because this is required in order to accurately resolve every discontinuity exactly (and this is what allows for perfectly vertical lines in plots!). However, in many cases when using jump problems you may wish to decrease the saving pressure given by large numbers of jumps. To do this, you set save_positions in the JumpProblem. Just like for other callbacks, this is a tuple (bool1,bool2) which sets whether to save before or after a jump. If we do not want to save at every jump, we would thus pass:","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"jump_prob = JumpProblem(prob, Direct(), jump, jump2, save_positions=(false,false))","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Now the saving controls associated with the integrator are the only ones to note. For example, we can use saveat=0.5 to save at an evenly spaced grid:","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"sol = solve(jump_prob, SSAStepper(), saveat=0.5)","category":"page"},{"location":"tutorials/discrete_stochastic_example/#MassActionJumpSect","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Defining the Jumps Directly: MassActionJump","text":"","category":"section"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"For jumps that can be represented as mass action reactions, a further specialization of the jump type is possible that offers improved computational performance; MassActionJump. Suppose the system has N chemical species S_1dotsS_N. A general mass action reaction has the form","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"R_1 S_1 + R_2 S_2 + dots + R_N S_N oversetkrightarrow P_1 S_1 + P_2 S_2 + dots + P_N S_N","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"where the non-negative integers (R_1dotsR_N) denote the reactant stoichiometry of the reaction, and the non-negative integers (P_1dotsP_N) the product stoichiometry. The net stoichiometry is the net change in each chemical species from the reaction occurring one time, given by (P_1-R_1dotsP_N-R_N).","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"As an example, consider again the SIR model defined in the @reaction_network above. The species are then (S,I,R). The first reaction has rate β, reactant stoichiometry (1,1,0), product stoichiometry (0,2,0), and net stoichiometry (-1,1,0). The second reaction has rate ν, reactant stoichiometry (0,1,0), product stoichiometry (0,0,1), and net stoichiometry (0,-1,1).","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"We can manually encode this system as a mass action jump by specifying the indexes of the rate constants in p, the reactant stoichiometry, and the net stoichiometry as follows:","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"rateidxs = [1, 2]    # i.e. [β,ν]\nreactant_stoich =\n[\n  [1 => 1, 2 => 1],         # 1*S and 1*I\n  [2 => 1]                  # 1*I\n]\nnet_stoich =\n[\n  [1 => -1, 2 => 1],        # -1*S and 1*I\n  [2 => -1, 3 => 1]         # -1*I and 1*R\n]\nmass_act_jump = MassActionJump(reactant_stoich, net_stoich; param_idxs=rateidxs)","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Notice, one typically should define one MassActionJump that encodes each possible jump that can be represented via a mass action reaction. This is in contrast to ConstantRateJumps or VariableRateJumps where separate instances are created for each distinct jump type.","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Just like for ConstantRateJumps, to then simulate the system we create a JumpProblem and call solve:","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"jump_prob = JumpProblem(prob, Direct(), mass_act_jump)\nsol = solve(jump_prob, SSAStepper())","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"For more details about MassActionJumps see Defining a Mass Action Jump. ","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Note, for chemical reaction systems, Catalyst.jl automatically groups reactions into their optimal jump representation.","category":"page"},{"location":"tutorials/discrete_stochastic_example/#Defining-the-Jumps-Directly:-Mixing-ConstantRateJump-and-MassActionJump","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Defining the Jumps Directly: Mixing ConstantRateJump and MassActionJump","text":"","category":"section"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Suppose we now want to add in to the SIR model another jump that can not be represented as a mass action reaction. We can create a new ConstantRateJump and simulate a hybrid system using both the MassActionJump for the two previous reactions, and the new ConstantRateJump. Let's suppose we want to let susceptible people be born with the following jump rate:","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"birth_rate(u,p,t) = 10.0*u[1]/(200. + u[1]) + 10.\nfunction birth_affect!(integrator)\n  integrator.u[1] += 1\nend\nbirth_jump = ConstantRateJump(birth_rate, birth_affect!)","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"We can then simulate the hybrid system as","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"jump_prob = JumpProblem(prob, Direct(), mass_act_jump, birth_jump)\nsol = solve(jump_prob, SSAStepper())","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"(Image: gillespie_hybrid_jumps)","category":"page"},{"location":"tutorials/discrete_stochastic_example/#Adding-Jumps-to-a-Differential-Equation","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Adding Jumps to a Differential Equation","text":"","category":"section"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"If we instead used some form of differential equation instead of a DiscreteProblem, we would couple the jumps/reactions to the differential equation. Let's define an ODE problem, where the continuous part only acts on some new 4th component:","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"using OrdinaryDiffEq\nfunction f(du,u,p,t)\n  du[4] = u[2]*u[3]/100000 - u[1]*u[4]/100000\nend\nu₀   = [999.0,1.0,0.0,100.0]\nprob = ODEProblem(f,u₀,tspan,p)","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Notice we gave the 4th component a starting value of 100.0, and used floating point numbers for the initial condition since some solution components now evolve continuously. The same steps as above will allow us to solve this hybrid equation when using ConstantRateJumps (or MassActionJumps). For example, we can solve it using the Tsit5() method via:","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"jump_prob = JumpProblem(prob,Direct(),jump,jump2)\nsol = solve(jump_prob,Tsit5())","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"(Image: gillespie_ode)","category":"page"},{"location":"tutorials/discrete_stochastic_example/#VariableRateJumpSect","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Adding a VariableRateJump","text":"","category":"section"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Now let's consider adding a reaction whose rate changes continuously with the differential equation. To continue our example, let's let there be a new jump/reaction with rate depending on u[4]","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"rate3(u,p,t) = 1e-2*u[4]\nfunction affect3!(integrator)\n  integrator.u[2] += 1\nend\njump3 = VariableRateJump(rate3,affect3!)","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Notice, since rate3 depends on a variable that evolves continuously, and hence is not constant between jumps, we must use a VariableRateJump.","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Solving the equation is exactly the same:","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"u₀   = [999.0,1.0,0.0,1.0]\nprob = ODEProblem(f,u₀,tspan,p)\njump_prob = JumpProblem(prob,Direct(),jump,jump2,jump3)\nsol = solve(jump_prob,Tsit5())","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"(Image: variable_rate_gillespie)","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Note that VariableRateJumps require a continuous problem, like an ODE/SDE/DDE/DAE problem.","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Lastly, we are not restricted to ODEs. For example, we can solve the same jump problem except with multiplicative noise on u[4] by using an SDEProblem instead:","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"using StochasticDiffEq\nfunction g(du,u,p,t)\n  du[4] = 0.1u[4]\nend\n\nprob = SDEProblem(f,g,[999.0,1.0,0.0,1.0],(0.0,250.0), p)\njump_prob = JumpProblem(prob,Direct(),jump,jump2,jump3)\nsol = solve(jump_prob,SRIW1())","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"(Image: sde_gillespie)","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"For more details about VariableRateJumps see Defining a Variable Rate Jump.","category":"page"},{"location":"tutorials/discrete_stochastic_example/#RegularJumps-and-Tau-Leaping","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"RegularJumps and Tau-Leaping","text":"","category":"section"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"The previous parts described how to use ConstantRateJumps, MassActionJumps, and VariableRateJumps to add jumps to differential equation algorithms over the callback interface. However, in many cases you do not need to step to every jump time. Instead, regular jumping allows you to pool together jumps and perform larger updates in a statistically-correct but more efficient manner.","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"For RegularJumps, we pool together the jumps we wish to perform. Here our rate is a vector equation which computes the rates of each jump process together:","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"function rate(out,u,p,t)\n    out[1] = (0.1/1000.0)*u[1]*u[2]\n    out[2] = 0.01u[2]\nend","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"and then we compute the total change matrix c","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"function c(dc,u,p,t,mark)\n    dc[1,1] = -1\n    dc[2,1] = 1\n    dc[2,2] = -1\n    dc[3,2] = 1\nend","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"where each column is a different jump process. We then declare the form of dc and build a RegularJump:","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"dc = zeros(3,2)\nrj = RegularJump(rate,c,dc;constant_c=true)","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"From there we build a JumpProblem:","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"prob = DiscreteProblem([999.0,1.0,0.0],(0.0,250.0))\njump_prob = JumpProblem(prob,Direct(),rj)","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Note that when a JumpProblem has a RegularJump, special algorithms are required. This is detailed on the jump solvers page. One such algorithm is SimpleTauLeaping, which we use as follows:","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"sol = solve(jump_prob,SimpleTauLeaping();dt=1.0)","category":"page"},{"location":"tutorials/discrete_stochastic_example/#FAQ","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"FAQ","text":"","category":"section"},{"location":"tutorials/discrete_stochastic_example/#*1.-My-simulation-is-really-slow-and/or-using-a-lot-of-memory,-what-can-I-do?*","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"1. My simulation is really slow and/or using a lot of memory, what can I do?","text":"","category":"section"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"To reduce memory use, use save_positions=(false,false) in the JumpProblem constructor as described earlier to turn off saving the system state before and after every jump. Combined with use of saveat in the call to solve this can dramatically reduce memory usage.","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"While Direct is often fastest for systems with 10 or less ConstantRateJumps or MassActionJumps, if your system has many jumps or one jump occurs most frequently, other stochastic simulation algorithms may be faster. See Constant Rate Jump Aggregators and the subsequent sections there for guidance on choosing different SSAs (called aggregators in DiffEqJump).","category":"page"},{"location":"tutorials/discrete_stochastic_example/#*2.-When-running-many-consecutive-simulations,-for-example-within-an-EnsembleProblem-or-loop,-how-can-I-update-JumpProblems?*","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"2. When running many consecutive simulations, for example within an EnsembleProblem or loop, how can I update JumpProblems?","text":"","category":"section"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"In Remaking JumpProblems we show how to modify parameters, the initial condition, and other components of a generated JumpProblem. This can be useful when trying to call solve many times while avoiding reallocations of the internal aggregators for each new parameter value or initial condition.","category":"page"},{"location":"tutorials/discrete_stochastic_example/#*3.-How-do-I-use-callbacks-with-ConstantRateJump-or-MassActionJump-systems?*","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"3. How do I use callbacks with ConstantRateJump or MassActionJump systems?","text":"","category":"section"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Callbacks can be used with ConstantRateJumps and MassActionJumps. When solving a pure jump system with SSAStepper, only discrete callbacks can be used (otherwise a different time stepper is needed). ","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Note, when modifying u or p within a callback, you must call reset_aggregated_jumps!(integrator) after making updates. This ensures that the underlying jump simulation algorithms know to reinitialize their internal data structures. Leaving out this call will lead to incorrect behavior!","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"A simple example that uses a MassActionJump and changes the parameters at a specified time in the simulation using a DiscreteCallback is","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"using DiffEqJump\nrs = [[1 => 1],[2=>1]]\nns = [[1 => -1, 2 => 1],[1=>1,2=>-1]]\np  = [1.0,0.0]\nmaj = MassActionJump(rs, ns; param_idxs=[1,2])\nu₀ = [100,0]\ntspan = (0.0,40.0)\ndprob = DiscreteProblem(u₀,tspan,p)\njprob = JumpProblem(dprob,Direct(),maj)\npcondit(u,t,integrator) = t==20.0\nfunction paffect!(integrator)\n  integrator.p[1] = 0.0\n  integrator.p[2] = 1.0\n  reset_aggregated_jumps!(integrator)\nend\nsol = solve(jprob, SSAStepper(), tstops=[20.0], callback=DiscreteCallback(pcondit,paffect!))","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Here at time 20.0 we turn off production of u[2] while activating production of u[1], giving","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"(Image: callback_gillespie)","category":"page"},{"location":"tutorials/discrete_stochastic_example/#*4.-How-can-I-define-collections-of-many-different-jumps-and-pass-them-to-JumpProblem?*","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"4. How can I define collections of many different jumps and pass them to JumpProblem?","text":"","category":"section"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"We can use JumpSets to collect jumps together, and then pass them into JumpProblems directly. For example, using the MassActionJump and ConstantRateJump defined earlier we can write","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"jset = JumpSet(mass_act_jump, birth_jump)\njump_prob = JumpProblem(prob, Direct(), jset)\nsol = solve(jump_prob, SSAStepper())","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"If you have many jumps in tuples or vectors it is easiest to use the keyword argument-based constructor:","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"cj1 = ConstantRateJump(rate1,affect1!)\ncj2 = ConstantRateJump(rate2,affect2!)\ncjvec = [cj1,cj2]\n\nvj1 = VariableRateJump(rate3,affect3!)\nvj2 = VariableRateJump(rate4,affect4!)\nvjtuple = (vj1,vj2)\n\njset = JumpSet(; constant_jumps=cjvec, variable_jumps=vjtuple, \n                 massaction_jumps=mass_act_jump)","category":"page"},{"location":"tutorials/discrete_stochastic_example/#*5.-How-can-I-set-the-random-number-generator-used-in-the-jump-process-sampling-algorithms-(SSAs)?*","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"5. How can I set the random number generator used in the jump process sampling algorithms (SSAs)?","text":"","category":"section"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Random number generators can be passed to JumpProblem via the rng keyword argument. Continuing the previous example:","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"#] add RandomNumbers\nusing RandomNumbers\njprob = JumpProblem(dprob, Direct(), maj, rng=Xorshifts.Xoroshiro128Star(rand(UInt64)))","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"uses the Xoroshiro128Star generator from RandomNumbers.jl.","category":"page"},{"location":"tutorials/discrete_stochastic_example/#*6.-What-are-these-aggregators-and-aggregations-in-DiffEqJump?*","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"6. What are these aggregators and aggregations in DiffEqJump?","text":"","category":"section"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"DiffEqJump provides a variety of methods for sampling the time the next ConstantRateJump or MassActionJump occurs, and which jump type happens at that time. These methods are examples of stochastic simulation algorithms (SSAs), also known as Gillespie methods, Doob's method, or Kinetic Monte Carlo methods. In the DiffEqJump terminology we call such methods \"aggregators\", and the cache structures that hold their basic data \"aggregations\". See Constant Rate Jump Aggregators for a list of the available SSA aggregators.","category":"page"},{"location":"tutorials/discrete_stochastic_example/#*7.-How-should-jumps-be-ordered-in-dependency-graphs?*","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"7. How should jumps be ordered in dependency graphs?","text":"","category":"section"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"Internally, DiffEqJump SSAs (aggregators) order all MassActionJumps first, then all ConstantRateJumps. i.e. in the example","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"using DiffEqJump\nrs = [[1 => 1],[2=>1]]\nns = [[1 => -1, 2 => 1],[1=>1,2=>-1]]\np  = [1.0,0.0]\nmaj = MassActionJump(rs, ns; param_idxs=[1,2])\nrate1(u,p,t) = u[1]\nfunction affect1!(integrator) \n  u[1] -= 1\nend\ncj1 = ConstantRateJump(rate1,affect1)\nrate2(u,p,t) = u[2]\nfunction affect2!(integrator)\n  u[2] -= 1\nend \ncj2 = ConstantRateJump(rate2,affect2)\njset = JumpSet(; constant_jumps=[cj1,cj2], massaction_jump=maj)","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"The four jumps would be ordered by the first jump in maj, the second jump in maj, cj1, and finally cj2. Any user-generated dependency graphs should then follow this ordering when assigning an integer id to each jump. ","category":"page"},{"location":"tutorials/discrete_stochastic_example/","page":"Continuous-Time Jump Processes and Gillespie Methods","title":"Continuous-Time Jump Processes and Gillespie Methods","text":"See also Constant Rate Jump Aggregators Requiring Dependency Graphs for more on dependency graphs needed for the various SSAs.","category":"page"},{"location":"analysis/bifurcation/#Bifurcation-Analysis","page":"Bifurcation Analysis","title":"Bifurcation Analysis","text":"","category":"section"},{"location":"analysis/bifurcation/","page":"Bifurcation Analysis","title":"Bifurcation Analysis","text":"Bifurcation analysis on DifferentialEquations.jl types can be performed by:","category":"page"},{"location":"analysis/bifurcation/","page":"Bifurcation Analysis","title":"Bifurcation Analysis","text":"BifurcationKit.jl (currently the most comprehensive and activately maintained package)\nBifurcations.jl\nPyDSTool.jl (no longer recommended)","category":"page"},{"location":"analysis/bifurcation/","page":"Bifurcation Analysis","title":"Bifurcation Analysis","text":"BifurcationKit has integration with ODEProblem for some functionality (like computing periodic orbits via shooting). If oprob is an ODEProblem, one can also just pass oprob.f.f and oprob.f.jac to BifurcationKit methods as needed. Bifurcations.jl can directly generate a BifurcationProblem from an ODEProblem. ","category":"page"},{"location":"solvers/split_ode_solve/#split_ode_solve","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"","category":"section"},{"location":"solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"The solvers which are available for a SplitODEProblem depend on the input linearity and number of components. Each solver has functional form (or many) that it allows.","category":"page"},{"location":"solvers/split_ode_solve/#Implicit-Explicit-(IMEX)-ODE","page":"Split ODE Solvers","title":"Implicit-Explicit (IMEX) ODE","text":"","category":"section"},{"location":"solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"The Implicit-Explicit (IMEX) ODE is a SplitODEProblem with two functions:","category":"page"},{"location":"solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"fracdudt =  f_1(tu) + f_2(tu)","category":"page"},{"location":"solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"where the first function is the stiff part and the second function is the non-stiff part (implicit integration on f1, explicit integration on f2).","category":"page"},{"location":"solvers/split_ode_solve/#Recommended-Methods","page":"Split ODE Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"The recommended method in most cases is KenCarp4. In cases of extreme stiffness or for high tolerances, KenCarp3 can be a good choice. The ARKODE methods are generally inefficient and diverge unless the options are tweaked to match the problem, though for large enough PDEs the ARKODE method with linear_solver=:GMRES is a good choice.","category":"page"},{"location":"solvers/split_ode_solve/#OrdinaryDiffEq.jl","page":"Split ODE Solvers","title":"OrdinaryDiffEq.jl","text":"","category":"section"},{"location":"solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"SplitEuler: 1st order fully explicit method. Used for testing accuracy of splits.\nIMEXEuler : 1st order explicit Euler mixed with implicit Euler. Fixed time step only.\nCNAB2: Crank-Nicolson Adams Bashforth Order 2. Fixed time step only.\nCNLF: Crank-Nicolson Leapfrog of Order 2. Fixed time step only.\nSBDF2 : 2nd order IMEX BDF method. Fixed time step only.\nSBDF3 : 3rd order IMEX BDF method. Fixed time step only. In development.\nSBDF4 : 4th order IMEX BDF method. Fixed time step only. In development.\nKenCarp3: An A-L stable stiffly-accurate 3rd order ESDIRK method.\nKenCarp4: An A-L stable stiffly-accurate 4rd order ESDIRK method.\nKenCarp47 - An A-L stable stiffly-accurate 4th order seven-stage ESDIRK method with splitting\nKenCarp5: An A-L stable stiffly-accurate 5rd order ESDIRK method.\nKenCarp58 - An A-L stable stiffly-accurate 5th order eight-stage ESDIRK method with splitting","category":"page"},{"location":"solvers/split_ode_solve/#Sundials.jl","page":"Split ODE Solvers","title":"Sundials.jl","text":"","category":"section"},{"location":"solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"ARKODE: An additive Runge-Kutta method. Order between 3rd and 5th. For a list of available options, please see its ODE solver page","category":"page"},{"location":"solvers/split_ode_solve/#Semilinear-ODE","page":"Split ODE Solvers","title":"Semilinear ODE","text":"","category":"section"},{"location":"solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"The Semilinear ODE is a SplitODEProblem with one linear operator and one nonlinear function:","category":"page"},{"location":"solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"fracdudt =  Au + f(tu)","category":"page"},{"location":"solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"See the documentation page for DiffEqOperators for details about how to define linear operators from a matrix or finite difference discretization of derivative operators.","category":"page"},{"location":"solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"The appropriate algorithms for this form are:","category":"page"},{"location":"solvers/split_ode_solve/#OrdinaryDiffEq.jl-2","page":"Split ODE Solvers","title":"OrdinaryDiffEq.jl","text":"","category":"section"},{"location":"solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"LawsonEuler - First order exponential Euler scheme. Fixed timestepping only.\nNorsettEuler - First order exponential-RK scheme. Fixed timestepping only. Alias: ETD1.\nETD2 - Second order Exponential Time Differencing method (in development). Fixed timestepping only. Doesn't support Krylov approximation.\nETDRK2 - 2nd order exponential-RK scheme. Fixed timestepping only.\nETDRK3 - 3rd order exponential-RK scheme. Fixed timestepping only.\nETDRK4 - 4th order exponential-RK scheme. Fixed timestepping only.\nHochOst4 - 4th order exponential-RK scheme with stiff order 4. Fixed timestepping only.","category":"page"},{"location":"solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"Note that the generic algorithms GenericIIF1 and GenericIIF2 allow for a choice of nlsolve.","category":"page"},{"location":"solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"By default, the exponential methods cache matrix functions such as exp(dt*A) to accelerate the time stepping for small systems. For large systems, using Krylov-based versions of the methods can allow for lazy calculation of exp(dt*A)*v and similar entities, and thus improve performance.","category":"page"},{"location":"solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"To tell a solver to use Krylov methods, pass krylov=true to its constructor. You can also manually set the size of the Krylov subspace by setting the m parameter, which defaults to 30. For example","category":"page"},{"location":"solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"LawsonEuler(krylov=true, m=50)","category":"page"},{"location":"solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"constructs a Lawson-Euler method which uses a size-50 Krylov subspace. Note that m only sets an upper bound to the Krylov subspace size. If a convergence criterion is met (determined by the reltol of the integrator), \"happy breakdown\" will occur and the Krylov subspace will only be constructed partially.","category":"page"},{"location":"solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"For more advanced control over the Krylov algorithms, you can change the length of the incomplete orthogonalization procedure (IOP) [1] by setting the iop parameter in the constructor. By default, IOP is turned off and full Arnoldi iteration is used. Note that if the linear operator is hermitian, then the Lanczos algorithm will always be used and IOP setting is ignored.","category":"page"},{"location":"solvers/split_ode_solve/","page":"Split ODE Solvers","title":"Split ODE Solvers","text":"[1]: Koskela, A. (2015). Approximating the matrix exponential of an advection-diffusion operator using the incomplete orthogonalization method. In Numerical Mathematics and Advanced Applications-ENUMATH 2013 (pp. 345-353). Springer, Cham.","category":"page"},{"location":"tutorials/dde_example/#Delay-Differential-Equations","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"","category":"section"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"This tutorial will introduce you to the functionality for solving delay differential equations.","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"note: Note\nThis tutorial assumes you have read the Ordinary Differential Equations tutorial.","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"Delay differential equations are equations which have a delayed argument. To allow for specifying the delayed argument, the function definition for a delay differential equation is expanded to include a history function h(p, t) which uses interpolations throughout the solution's history to form a continuous extension of the solver's past and depends on parameters p and time t. The function signature for a delay differential equation is f(u, h, p, t) for not in-place computations, and f(du, u, h, p, t) for in-place computations.","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"In this example we will solve a model of breast cancer growth kinetics:","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"beginaligned\ndx_0 = fracv_01+beta_0left(x_2(t-tau)right)^2left(p_0-q_0right)x_0(t)-d_0x_0(t)\ndx_1 = fracv_01+beta_0left(x_2(t-tau)right)^2left(1-p_0+q_0right)x_0(t)\n       + fracv_11+beta_1left(x_2(t-tau)right)^2left(p_1-q_1right)x_1(t)-d_1x_1(t)\ndx_2 = fracv_11+beta_1left(x_2(t-tau)right)^2left(1-p_1+q_1right)x_1(t)-d_2x_2(t)\nendaligned","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"For this problem we note that tau is constant, and thus we can use a method which exploits this behavior. We first write out the equation using the appropriate function signature. Most of the equation writing is the same, though we use the history function by first interpolating and then choosing the components. Thus the ith component at time t-tau is given by h(p, t-tau)[i]. Components with no delays are written as in the ODE.","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"Thus, the function for this model is given by:","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"function bc_model(du,u,h,p,t)\n  p0,q0,v0,d0,p1,q1,v1,d1,d2,beta0,beta1,tau = p\n  hist3 = h(p, t-tau)[3]\n  du[1] = (v0/(1+beta0*(hist3^2))) * (p0 - q0)*u[1] - d0*u[1]\n  du[2] = (v0/(1+beta0*(hist3^2))) * (1 - p0 + q0)*u[1] +\n          (v1/(1+beta1*(hist3^2))) * (p1 - q1)*u[2] - d1*u[2]\n  du[3] = (v1/(1+beta1*(hist3^2))) * (1 - p1 + q1)*u[2] - d2*u[3]\nend","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"Now we build a DDEProblem. The signature","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"prob = DDEProblem(f, u0, h, tspan, p=SciMLBase.NullParameters();\n                  constant_lags=[], dependent_lags=[], kwargs...)","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"is very similar to ODEs, where we now have to give the lags and a function h. h is the history function that declares what the values were before the time the model starts. Here we will assume that for all time before t0 the values were 1 and define h as an out-of-place function:","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"h(p, t) = ones(3)","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"To use the constant lag model, we have to declare the lags. Here we will use tau=1.","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"tau = 1\nlags = [tau]","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"Next, we choose to solve on the timespan (0.0,10.0) and create the problem type:","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"p0 = 0.2; q0 = 0.3; v0 = 1; d0 = 5\np1 = 0.2; q1 = 0.3; v1 = 1; d1 = 1\nd2 = 1; beta0 = 1; beta1 = 1\np = (p0,q0,v0,d0,p1,q1,v1,d1,d2,beta0,beta1,tau)\ntspan = (0.0,10.0)\nu0 = [1.0,1.0,1.0]\n\nprob = DDEProblem(bc_model,u0,h,tspan,p; constant_lags=lags)","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"An efficient way to solve this problem (given the constant lags) is with the MethodOfSteps solver. Through the magic that is Julia, it translates an OrdinaryDiffEq.jl ODE solver method into a method for delay differential equations which is highly efficient due to sweet compiler magic. A good choice is the order 5 method Tsit5():","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"alg = MethodOfSteps(Tsit5())","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"For lower tolerance solving, one can use the BS3() algorithm to good effect (this combination is similar to the MATLAB dde23, but more efficient tableau), and for high tolerances the Vern6() algorithm will give an 6th order solution.","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"To solve the problem with this algorithm, we do the same thing we'd do with other methods on the common interface:","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"sol = solve(prob,alg)","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"Note that everything available to OrdinaryDiffEq.jl can be used here, including event handling and other callbacks. The solution object has the same interface as for ODEs. For example, we can use the same plot recipes to view the results:","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"using Plots; plot(sol)","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"(Image: DDE Example Plot)","category":"page"},{"location":"tutorials/dde_example/#Speeding-Up-Interpolations-with-Idxs","page":"Delay Differential Equations","title":"Speeding Up Interpolations with Idxs","text":"","category":"section"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"We can speed up the previous problem in two different ways. First of all, if we need to interpolate multiple values from a previous time, we can use the in-place form for the history function h(out, p, t) which writes the output to out. In this case, we must supply the history initial conditions as in-place as well. For the previous example, that's simply","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"h(out, p, t) = (out.=1.0)","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"and then our DDE is:","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"const out = zeros(3) # Define a cache variable\nfunction bc_model(du,u,h,p,t)\n  h(out, p, t-tau) # updates out to be the correct history function\n  du[1] = (v0/(1+beta0*(out[3]^2))) * (p0 - q0)*u[1] - d0*u[1]\n  du[2] = (v0/(1+beta0*(out[3]^2))) * (1 - p0 + q0)*u[1] +\n          (v1/(1+beta1*(out[3]^2))) * (p1 - q1)*u[2] - d1*u[2]\n  du[3] = (v1/(1+beta1*(out[3]^2))) * (1 - p1 + q1)*u[2] - d2*u[3]\nend","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"However, we can do something even slicker in most cases. We only ever needed to interpolate past values at index 3. Instead of generating a bunch of arrays, we can instead ask specifically for that value by passing the keyword idxs = 3. The DDE function bc_model is now:","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"function bc_model(du,u,h,p,t)\n  u3_past_sq = h(p, t-tau; idxs=3)^2\n  du[1] = (v0/(1+beta0*(u3_past_sq))) * (p0 - q0)*u[1] - d0*u[1]\n  du[2] = (v0/(1+beta0*(u3_past_sq))) * (1 - p0 + q0)*u[1] +\n          (v1/(1+beta1*(u3_past_sq))) * (p1 - q1)*u[2] - d1*u[2]\n  du[3] = (v1/(1+beta1*(u3_past_sq))) * (1 - p1 + q1)*u[2] - d2*u[3]\nend","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"Note that this requires that we define the historical values","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"h(p, t; idxs=nothing) = typeof(idxs) <: Number ? 1.0 : ones(3)","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"where idxs can be an integer for which variable in the history to compute, and here for any number idxs we give back 1.0. Note that if we wanted to use past values of the ith derivative then we would call the history function h(p, t, Val{i}) in our DDE function and would have to define a dispatch like","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"h(p, t, ::Type{Val{1}}) = zeros(3)","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"to say that derivatives before t0 are zero for any index. Again, we could use an in-place function instead or only compute specific indices by passing an idxs keyword.","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"The functional forms for the history function are discussed also on the DDEProblem page.","category":"page"},{"location":"tutorials/dde_example/#Undeclared-Delays-and-State-Dependent-Delays-via-Residual-Control","page":"Delay Differential Equations","title":"Undeclared Delays and State-Dependent Delays via Residual Control","text":"","category":"section"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"You might have noticed DifferentialEquations.jl allows you to solve problems with undeclared delays since you can interpolate h at any value. This is a feature, but use it with caution. Undeclared delays can increase the error in the solution. It's recommended that you use a method with a residual control, such as MethodOfSteps(RK4()) whenever there are undeclared delays. With this you can use interpolated derivatives, solve functional differential equations by using quadrature on the interpolant, etc. However, note that residual control solves with a low level of accuracy, so the tolerances should be made very small and the solution should not be trusted for more than 2-3 decimal places.","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"Note: MethodOfSteps(RK4()) with undeclared delays is similar to MATLAB's ddesd. Thus, for example, the following is similar to solving the example from above with residual control:","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"prob = DDEProblem(bc_model,u0,h,tspan)\nalg = MethodOfSteps(RK4())\nsol = solve(prob,alg)","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"Note that this method can solve problems with state-dependent delays.","category":"page"},{"location":"tutorials/dde_example/#State-Dependent-Delay-Discontinuity-Tracking","page":"Delay Differential Equations","title":"State-Dependent Delay Discontinuity Tracking","text":"","category":"section"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"State-dependent delays are problems where the delay is allowed to be a function of the current state. They can be more efficiently solved with discontinuity tracking. To do this in DifferentialEquations.jl, requires to pass lag functions g(u,p,t) as keyword dependent_lags to the DDEProblem definition. Other than that, everything else is the same, and one solves that problem using the common interface.","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"We can solve the above problem with dependent delay tracking by declaring the dependent lags and solving with a MethodOfSteps algorithm:","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"prob = DDEProblem(bc_model,u0,h,tspan; dependent_lags = ((u,p,t) -> tau,))\nalg = MethodOfSteps(Tsit5())\nsol = solve(prob,alg)","category":"page"},{"location":"tutorials/dde_example/","page":"Delay Differential Equations","title":"Delay Differential Equations","text":"Here we treated the single lag t-tau as a state-dependent delay. Of course, you can then replace that tuple of functions with whatever functions match your lags.","category":"page"},{"location":"features/progress_bar/#Progress-Bar-Integration","page":"Progress Bar Integration","title":"Progress Bar Integration","text":"","category":"section"},{"location":"features/progress_bar/","page":"Progress Bar Integration","title":"Progress Bar Integration","text":"DifferentialEquations.jl integrates with the Juno progress bar in order to make long calculations more manageable. By default this feature is off for ODE and SDE solvers, but can be turned on via the keyword argument progress=true. The progress bar updates every progress_steps timesteps, which has a default value of 1000. Note that making this value really low could cause a performance hit, though from some basic testing it seems that with updates of at least 1000 steps on number (the fastest problems) there's no discernable performance degradation, giving a high upper bound.","category":"page"},{"location":"features/progress_bar/","page":"Progress Bar Integration","title":"Progress Bar Integration","text":"Note that the progress bar also includes a time estimate. This time-estimate is provided by linear extrapolation for how long it has taken to get to what percentage. For adaptive timestepping methods this should only be used as a rough estimate since the timesteps may (and will) change. By scrolling over the progress bar one will also see the current timestep. This can be used to track the solution's progress and find tough locations for the solvers.","category":"page"},{"location":"features/progress_bar/#Using-Progress-Bars-Outside-Juno","page":"Progress Bar Integration","title":"Using Progress Bars Outside Juno","text":"","category":"section"},{"location":"features/progress_bar/","page":"Progress Bar Integration","title":"Progress Bar Integration","text":"To use the progress bars outside of Juno, use TerminalLoggers.jl. Follow these directions to add TerminalLogging to your startup.jl, if you want it enabled by default.","category":"page"},{"location":"features/progress_bar/","page":"Progress Bar Integration","title":"Progress Bar Integration","text":"Otherwise, follow the example down below. Note that global_logger is initialized  before any other julia call. This step is crucial, otherwise no logging will  appear in the terminal.","category":"page"},{"location":"features/progress_bar/","page":"Progress Bar Integration","title":"Progress Bar Integration","text":"using Logging: global_logger\nusing TerminalLoggers: TerminalLogger\nglobal_logger(TerminalLogger())\n\nusing OrdinaryDiffEq\n\nsolve(\n    ODEProblem((u, p, t) -> (sleep(0.01); -u), 1.0, nothing),\n    Euler();\n    dt = 0.5,\n    tspan = (0.0, 1000.0),\n    progress = true,\n    progress_steps = 1,\n)","category":"page"},{"location":"analysis/neural_networks/#Neural-Networks","page":"Neural Networks","title":"Neural Networks","text":"","category":"section"},{"location":"analysis/neural_networks/","page":"Neural Networks","title":"Neural Networks","text":"To use DifferentialEquations.jl with the Flux.jl neural network package, consult the documentation at DiffEqFlux.jl.","category":"page"},{"location":"solvers/dae_solve/#DAE-Solvers","page":"DAE Solvers","title":"DAE Solvers","text":"","category":"section"},{"location":"solvers/dae_solve/#Recommended-Methods","page":"DAE Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"For medium to low accuracy small numbers of DAEs in constant mass matrices form, the  Rosenbrock23 and Rodas4 methods are good choices which will get good efficiency if the mass matrix is constant. Rosenbrock23 is better for low accuracy (error tolerance <1e-4) and Rodas4 is better for high accuracy. Another choice at high accuracy is RadauIIA5.","category":"page"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"Non-constant mass matrices are not directly supported: users are advised to transform their problem through substitution to a DAE with constant mass matrices.","category":"page"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"If the problem cannot be defined in mass matrix form, the recommended method for performance is IDA from the Sundials.jl package if you are solving problems with Float64. If Julia types are required, currently DABDF2 is the best method.","category":"page"},{"location":"solvers/dae_solve/#dae_solve_full","page":"DAE Solvers","title":"Full List of Methods","text":"","category":"section"},{"location":"solvers/dae_solve/#Initialization-Schemes","page":"DAE Solvers","title":"Initialization Schemes","text":"","category":"section"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"For all OrdinaryDiffEq.jl methods, an initialization scheme can be set with a common keyword argument initializealg. The choices are:","category":"page"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"BrownFullBasicInit: For Index-1 DAEs implicit DAEs and and semi-explicit DAEs in mass matrix form. Keeps the differential variables constant. Requires du0 when used on a DAEProblem.\nShampineCollocationInit: For Index-1 DAEs implicit DAEs and and semi-explicit DAEs in mass matrix form. Changes both the differential and algebraic variables.\nNoInit: Explicitly opts-out of DAE initialization.","category":"page"},{"location":"solvers/dae_solve/#OrdinaryDiffEq.jl-(Implicit-ODE)","page":"DAE Solvers","title":"OrdinaryDiffEq.jl (Implicit ODE)","text":"","category":"section"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"These methods from OrdinaryDiffEq are for DAEProblem specifications.","category":"page"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"DImplicitEuler - 1st order A-L and stiffly stable adaptive implicit Euler\nDABDF2 - 2nd order A-L stable adaptive BDF method.","category":"page"},{"location":"solvers/dae_solve/#OrdinaryDiffEq.jl-(Mass-Matrix)","page":"DAE Solvers","title":"OrdinaryDiffEq.jl (Mass Matrix)","text":"","category":"section"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"These methods require the DAE to be an ODEProblem in mass matrix form. For extra options for the solvers, see the ODE solver page.","category":"page"},{"location":"solvers/dae_solve/#Rosenbrock-Methods","page":"DAE Solvers","title":"Rosenbrock Methods","text":"","category":"section"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"ROS3P - 3rd order A-stable and stiffly stable Rosenbrock method. Keeps high accuracy on discretizations of nonlinear parabolic PDEs.\nRodas3 - 3rd order A-stable and stiffly stable Rosenbrock method.\nRosShamp4- An A-stable 4th order Rosenbrock method.\nVeldd4 - A 4th order D-stable Rosenbrock method.\nVelds4 - A 4th order A-stable Rosenbrock method.\nGRK4T - An efficient 4th order Rosenbrock method.\nGRK4A - An A-stable 4th order Rosenbrock method. Essentially \"anti-L-stable\" but efficient.\nRos4LStab - A 4th order L-stable Rosenbrock method.\nRodas4 - A 4th order A-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant\nRodas42 - A 4th order A-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant\nRodas4P - A 4th order A-stable stiffly stable Rosenbrock method with a stiff-aware 3rd order interpolant. 4th order on linear parabolic problems and 3rd order accurate on nonlinear parabolic problems (as opposed to lower if not corrected).\nRodas4P2 - A 4th order L-stable stiffly stable Rosenbrock method with a stiff-aware  3rd order interpolant. 4th order on linear parabolic problems and 3rd order accurate  on nonlinear parabolic problems. It is an improvement of Roadas4P and in case of  inexact Jacobians a second order W method.\nRodas5 - A 5th order A-stable stiffly stable Rosenbrock method. Currently has a Hermite interpolant because its stiff-aware 3rd order interpolant is not yet implemented. This means the interpolation is inaccurate on algebraic variables, meaning this algorithm should not be used with saveat or post-solution interpolation on DAEs.","category":"page"},{"location":"solvers/dae_solve/#Rosenbrock-W-Methods","page":"DAE Solvers","title":"Rosenbrock-W Methods","text":"","category":"section"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"Rosenbrock23 - An Order 2/3 L-Stable Rosenbrock-W method which is good for very stiff equations with oscillations at low tolerances. 2nd order stiff-aware interpolation.\nRosenbrock32 - An Order 3/2 A-Stable Rosenbrock-W method which is good for mildy stiff equations without oscillations at low tolerances. Note that this method is prone to instability in the presence of oscillations, so use with caution. 2nd order stiff-aware interpolation.\nRosenbrockW6S4OS - A 4th order L-stable Rosenbrock-W method (fixed step only).\nROS34PW1a - A 4th order L-stable Rosenbrock-W method.\nROS34PW1b - A 4th order L-stable Rosenbrock-W method.\nROS34PW2 - A 4th order stiffy accurate Rosenbrock-W method for PDAEs.\nROS34PW3 - A 4th order strongly A-stable (Rinf~0.63) Rosenbrock-W method.","category":"page"},{"location":"solvers/dae_solve/#FIRK-Methods","page":"DAE Solvers","title":"FIRK Methods","text":"","category":"section"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"RadauIIA5 - An A-B-L stable fully implicit Runge-Kutta method with internal tableau complex basis transform for efficiency.","category":"page"},{"location":"solvers/dae_solve/#SDIRK-Methods","page":"DAE Solvers","title":"SDIRK Methods","text":"","category":"section"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"ImplicitEuler - Stage order 1. A-B-L-stable. Adaptive timestepping through a divided differences estimate via memory. Strong-stability preserving (SSP).\nImplicitMidpoint - Stage order 1. Symplectic. Good for when symplectic integration is required.\nTrapezoid - A second order A-stable symmetric ESDIRK method. \"Almost symplectic\" without numerical dampening. Also known as Crank-Nicolson when applied to PDEs. Adaptive timestepping via divided differences on the memory. Good for highly stiff equations which are non-oscillatory.","category":"page"},{"location":"solvers/dae_solve/#Multistep-Methods","page":"DAE Solvers","title":"Multistep Methods","text":"","category":"section"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"Quasi-constant stepping is the time stepping strategy which matches the classic GEAR, LSODE,  and ode15s integrators. The variable-coefficient methods match the ideas of the classic EPISODE integrator and early VODE designs. The Fixed Leading Coefficient (FLC) methods match the behavior of the classic VODE and Sundials CVODE integrator.","category":"page"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"QNDF1 - An adaptive order 1 quasi-constant timestep L-stable numerical differentiation function (NDF) method. Optional parameter kappa defaults to Shampine's accuracy-optimal -0.1850.\nQBDF1 - An adaptive order 1 L-stable BDF method. This is equivalent to implicit Euler but using the BDF error estimator.\nABDF2 - An adaptive order 2 L-stable fixed leading coefficient multistep BDF method.\nQNDF2 - An adaptive order 2 quasi-constant timestep L-stable numerical differentiation function (NDF) method.\nQBDF2 - An adaptive order 2 L-stable BDF method using quasi-constant timesteps.\nQNDF - An adaptive order quasi-constant timestep NDF method. Utilizes Shampine's accuracy-optimal kappa values as defaults (has a keyword argument for a tuple of kappa coefficients).\nQBDF - An adaptive order quasi-constant timestep BDF method.","category":"page"},{"location":"solvers/dae_solve/#Sundials.jl","page":"DAE Solvers","title":"Sundials.jl","text":"","category":"section"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"Note that this setup is not automatically included with DifferentialEquations.jl. To use the following algorithms, you must install and use Sundials.jl:","category":"page"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"]add Sundials\nusing Sundials","category":"page"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"IDA - This is the IDA method from the Sundials.jl package.","category":"page"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"Note that the constructors for the Sundials algorithms take a main argument:","category":"page"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"linearsolver - This is the linear solver which is used in the Newton iterations. The choices are:\n:Dense - A dense linear solver.\n:Band - A solver specialized for banded Jacobians. If used, you must set the position of the upper and lower non-zero diagonals via jac_upper and jac_lower.\n:LapackDense - A version of the dense linear solver that uses the Julia-provided OpenBLAS-linked LAPACK for multithreaded operations. This will be faster than :Dense on larger systems but has noticable overhead on smaller (<100 ODE) systems.\n:LapackBand - A version of the banded linear solver that uses the Julia-provided OpenBLAS-linked LAPACK for multithreaded operations. This will be faster than :Band on larger systems but has noticable overhead on smaller (<100 ODE) systems.\n:GMRES - A GMRES method. Recommended first choice Krylov method\n:BCG - A Biconjugate gradient method.\n:PCG - A preconditioned conjugate gradient method. Only for symmetric linear systems.\n:TFQMR - A TFQMR method.\n:KLU - A sparse factorization method. Requires that the user specifies a Jacobian. The Jacobian must be set as a sparse matrix in the ODEProblem type.","category":"page"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"Example:","category":"page"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"IDA() # Newton + Dense solver\nIDA(linear_solver=:Band,jac_upper=3,jac_lower=3) # Banded solver with nonzero diagonals 3 up and 3 down\nIDA(linear_solver=:BCG) # Biconjugate gradient method                                   ","category":"page"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"All of the additional options are available. The constructor is:","category":"page"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"IDA(;linear_solver=:Dense,jac_upper=0,jac_lower=0,krylov_dim=0,\n    max_order = 5,\n    max_error_test_failures = 7,\n    max_nonlinear_iters = 3,\n    nonlinear_convergence_coefficient = 0.33,\n    nonlinear_convergence_coefficient_ic = 0.0033,\n    max_num_steps_ic = 5,\n    max_num_jacs_ic = 4,\n    max_num_iters_ic = 10,\n    max_num_backs_ic = 100,\n    use_linesearch_ic = true,\n    max_convergence_failures = 10,\n    init_all = false,\n    prec = nothing, psetup = nothing)","category":"page"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"See the Sundials manual for details on the additional options. The option init_all controls the initial condition consistency routine. If the initial conditions are inconsistant (i.e. they do not satisfy the implicit equation), init_all=false means that the algebraic variables and derivatives will be modified in order to satisfy the DAE. If init_all=true, all initial conditions will be modified to satify the DAE.","category":"page"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"Note that here prec is a (left) preconditioner function prec(z,r,p,t,y,fy,gamma,delta) where:","category":"page"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"z: the computed output vector\nr: the right-hand side vector of the linear system\np: the parameters\nt: the current independent variable\ndu: the current value of f(u,p,t)\ngamma: the gamma of W = M - gamma*J\ndelta: the iterative method tolerance","category":"page"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"and psetup is the preconditioner setup function for pre-computing Jacobian information. Where:","category":"page"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"p: the parameters\nt: the current independent variable\nresid: the current residual\nu: the current state\ndu: the current derivative of the state\ngamma: the gamma of W = M - gamma*J","category":"page"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"psetup is optional when prec is set.","category":"page"},{"location":"solvers/dae_solve/#DASKR.jl","page":"DAE Solvers","title":"DASKR.jl","text":"","category":"section"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"DASKR.jl is not automatically included by DifferentialEquations.jl. To use this algorithm, you will need to install and use the package:","category":"page"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"]add DASKR\nusing DASKR","category":"page"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"daskr - This is a wrapper for the well-known DASKR algorithm.","category":"page"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"All additional options are available. The constructor is:","category":"page"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"function daskr(;linear_solver=:Dense,\n                  jac_upper=0,jac_lower=0,max_order = 5,\n                  non_negativity_enforcement = 0,\n                  non_negativity_enforcement_array = nothing,\n                  max_krylov_iters = nothing,\n                  num_krylov_vectors = nothing,\n                  max_number_krylov_restarts = 5,\n                  krylov_convergence_test_constant = 0.05,\n                  exclude_algebraic_errors = false)","category":"page"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"Choices for the linear solver are:","category":"page"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":":Dense\n:Banded\n:SPIGMR, a Krylov method","category":"page"},{"location":"solvers/dae_solve/#DASSL.jl","page":"DAE Solvers","title":"DASSL.jl","text":"","category":"section"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"dassl - A native Julia implementation of the DASSL algorithm.","category":"page"},{"location":"solvers/dae_solve/#ODEInterfaceDiffEq.jl","page":"DAE Solvers","title":"ODEInterfaceDiffEq.jl","text":"","category":"section"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"These methods require the DAE to be an ODEProblem in mass matrix form. For extra options for the solvers, see the ODE solver page.","category":"page"},{"location":"solvers/dae_solve/","page":"DAE Solvers","title":"DAE Solvers","text":"seulex - Extrapolation-algorithm based on the linear implicit Euler method.\nradau - Implicit Runge-Kutta (Radau IIA) of variable order between 5 and 13.\nradau5 - Implicit Runge-Kutta method (Radau IIA) of order 5.\nrodas - Rosenbrock 4(3) method.","category":"page"},{"location":"tutorials/ode_example/#ode_example","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"","category":"section"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"This tutorial will introduce you to the functionality for solving ODEs. Other introductions can be found by checking out DiffEqTutorials.jl. Additionally, a video tutorial walks through this material.","category":"page"},{"location":"tutorials/ode_example/#Example-1-:-Solving-Scalar-Equations","page":"Ordinary Differential Equations","title":"Example 1 : Solving Scalar Equations","text":"","category":"section"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"In this example we will solve the equation","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"fracdudt = f(upt)","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"on the time interval tin01 where f(upt)=αu. We know by calculus that the solution to this equation is u(t)=u₀exp(αt).","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"The general workflow is to define a problem, solve the problem, and then analyze the solution. The full code for solving this problem is:","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"using DifferentialEquations\nf(u,p,t) = 1.01*u\nu0 = 1/2\ntspan = (0.0,1.0)\nprob = ODEProblem(f,u0,tspan)\nsol = solve(prob, Tsit5(), reltol=1e-8, abstol=1e-8)\n\nusing Plots\nplot(sol,linewidth=5,title=\"Solution to the linear ODE with a thick line\",\n     xaxis=\"Time (t)\",yaxis=\"u(t) (in μm)\",label=\"My Thick Line!\") # legend=false\nplot!(sol.t, t->0.5*exp(1.01t),lw=3,ls=:dash,label=\"True Solution!\")","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"where the pieces are described below.","category":"page"},{"location":"tutorials/ode_example/#Step-1:-Defining-a-Problem","page":"Ordinary Differential Equations","title":"Step 1: Defining a Problem","text":"","category":"section"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"To solve this numerically, we define a problem type by giving it the equation, the initial condition, and the timespan to solve over:","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"using DifferentialEquations\nf(u,p,t) = 1.01*u\nu0 = 1/2\ntspan = (0.0,1.0)\nprob = ODEProblem(f,u0,tspan)","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Note that DifferentialEquations.jl will choose the types for the problem based on the types used to define the problem type. For our example, notice that u0 is a Float64, and therefore this will solve with the dependent variables being Float64. Since tspan = (0.0,1.0) is a tuple of Float64's, the independent variables will be solved using Float64's (note that the start time and end time must match types). You can use this to choose to solve with arbitrary precision numbers, unitful numbers, etc. Please see the notebook tutorials for more examples.","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"The problem types include many other features, including the ability to define mass matrices and hold callbacks for events. Each problem type has a page which details its constructor and the available fields. For ODEs, the appropriate page is here. In addition, a user can specify additional functions to be associated with the function in order to speed up the solvers. These are detailed at the performance overloads page.","category":"page"},{"location":"tutorials/ode_example/#Step-2:-Solving-a-Problem","page":"Ordinary Differential Equations","title":"Step 2: Solving a Problem","text":"","category":"section"},{"location":"tutorials/ode_example/#Controlling-the-Solvers","page":"Ordinary Differential Equations","title":"Controlling the Solvers","text":"","category":"section"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"After defining a problem, you solve it using solve.","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"sol = solve(prob)","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"The solvers can be controlled using the available options are described on the Common Solver Options manual page. For example, we can lower the relative tolerance (in order to get a more correct result, at the cost of more timesteps) by using the command reltol:","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"sol = solve(prob,reltol=1e-6)","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"There are many controls for handling outputs. For example, we can choose to have the solver save every 0.1 time points by setting saveat=0.1. Chaining this with the tolerance choice looks like:","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"sol = solve(prob,reltol=1e-6,saveat=0.1)","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"More generally, saveat can be any collection of time points to save at. Note that this uses interpolations to keep the timestep unconstrained to speed up the solution. In addition, if we only care about the endpoint, we can turn off intermediate saving in general:","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"sol = solve(prob,reltol=1e-6,save_everystep=false)","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"which will only save the final time point.","category":"page"},{"location":"tutorials/ode_example/#Choosing-a-Solver-Algorithm","page":"Ordinary Differential Equations","title":"Choosing a Solver Algorithm","text":"","category":"section"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"DifferentialEquations.jl has a method for choosing the default solver algorithm which will find an efficient method to solve your problem. To help users receive the right algorithm, DifferentialEquations.jl offers a method for choosing algorithms through hints. This default chooser utilizes the precisions of the number types and the keyword arguments (such as the tolerances) to select an algorithm. Additionally one can provide alg_hints to help choose good defaults using properties of the problem and necessary features for the solution. For example, if we have a stiff problem where we need high accuracy, but don't know the best stiff algorithm for this problem, we can use:","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"sol = solve(prob,alg_hints=[:stiff],reltol=1e-8,abstol=1e-8)","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"You can also explicitly choose the algorithm to use. DifferentialEquations.jl offers a much wider variety of solver algorithms than traditional differential equations libraries. Many of these algorithms are from recent research and have been shown to be more efficient than the \"standard\" algorithms. For example, we can choose a 5th order Tsitouras method:","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"sol = solve(prob,Tsit5())","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Note that the solver controls can be combined with the algorithm choice. Thus we can for example solve the problem using Tsit5() with a lower tolerance via:","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"sol = solve(prob,Tsit5(),reltol=1e-8,abstol=1e-8)","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"In DifferentialEquations.jl, some good \"go-to\" choices for ODEs are:","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"AutoTsit5(Rosenbrock23()) handles both stiff and non-stiff equations. This is a good algorithm to use if you know nothing about the equation.\nAutoVern7(Rodas5()) handles both stiff and non-stiff equations in a way that's efficient for high accuracy.\nTsit5() for standard non-stiff. This is the first algorithm to try in most cases.\nBS3() for fast low accuracy non-stiff.\nVern7() for high accuracy non-stiff.\nRodas4() or Rodas5() for small stiff equations with Julia-defined types, events, etc.\nKenCarp4() or TRBDF2() for medium sized (100-2000 ODEs) stiff equations\nRadauIIA5() for really high accuracy stiff equations\nQNDF() for large stiff equations","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"For a comprehensive list of the available algorithms and detailed recommendations, Please see the solver documentation. Every problem type has an associated page detailing all of the solvers associated with the problem.","category":"page"},{"location":"tutorials/ode_example/#Step-3:-Analyzing-the-Solution","page":"Ordinary Differential Equations","title":"Step 3: Analyzing the Solution","text":"","category":"section"},{"location":"tutorials/ode_example/#Handling-the-Solution-Type","page":"Ordinary Differential Equations","title":"Handling the Solution Type","text":"","category":"section"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"The result of solve is a solution object. We can access the 5th value of the solution with:","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"julia> sol[5]\n0.637","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"or get the time of the 8th timestep by:","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"julia> sol.t[8]\n0.438","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Convenience features are also included. We can build an array using a comprehension over the solution tuples via:","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"[t+u for (u,t) in tuples(sol)]","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"or more generally","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"[t+2u for (u,t) in zip(sol.u,sol.t)]","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"allows one to use more parts of the solution type. The object that is returned by default acts as a continuous solution via an interpolation. We can access the interpolated values by treating sol as a function, for example:","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"sol(0.45) # The value of the solution at t=0.45","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Note the difference between these: indexing with [i] is the value at the ith step, while (t) is an interpolation at time t!","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"If in the solver dense=true (this is the default unless saveat is used), then this interpolation is a high order interpolation and thus usually matches the error of the solution time points. The interpolations associated with each solver is detailed at the solver algorithm page. If dense=false (unless specifically set, this only occurs when save_everystep=false or saveat is used) then this defaults to giving a linear interpolation.","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"For more details on handling the output, see the solution handling page.","category":"page"},{"location":"tutorials/ode_example/#Plotting-Solutions","page":"Ordinary Differential Equations","title":"Plotting Solutions","text":"","category":"section"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"While one can directly plot solution time points using the tools given above, convenience commands are defined by recipes for Plots.jl. To plot the solution object, simply call plot:","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"#]add Plots # You need to install Plots.jl before your first time using it!\nusing Plots\n#plotly() # You can optionally choose a plotting backend\nplot(sol)","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"(Image: ode_tutorial_linear_plot)","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"If you are in Juno, this will plot to the plot pane. To open an interactive GUI (dependent on the backend), use the gui command:","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"gui()","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"The plot function can be formatted using the attributes available in Plots.jl. Additional DiffEq-specific controls are documented at the plotting page.","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"For example, from the Plots.jl attribute page we see that the line width can be set via the argument linewidth. Additionally, a title can be set with title. Thus we add these to our plot command to get the correct output, fix up some axis labels, and change the legend (note we can disable the legend with legend=false) to get a nice looking plot:","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"plot(sol,linewidth=5,title=\"Solution to the linear ODE with a thick line\",\n     xaxis=\"Time (t)\",yaxis=\"u(t) (in μm)\",label=\"My Thick Line!\") # legend=false","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"We can then add to the plot using the plot! command:","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"plot!(sol.t,t->0.5*exp(1.01t),lw=3,ls=:dash,label=\"True Solution!\")","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"(Image: ode_tutorial_thick_linear)","category":"page"},{"location":"tutorials/ode_example/#Example-2:-Solving-Systems-of-Equations","page":"Ordinary Differential Equations","title":"Example 2: Solving Systems of Equations","text":"","category":"section"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"In this example we will solve the Lorenz equations:","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"beginaligned\nfracdxdt = σ(y-x) \nfracdydt = x(ρ-z) - y \nfracdzdt = xy - βz \nendaligned","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Defining your ODE function to be in-place updating can have performance benefits. What this means is that, instead of writing a function which outputs its solution, you write a function which updates a vector that is designated to hold the solution. By doing this, DifferentialEquations.jl's solver packages are able to reduce the amount of array allocations and achieve better performance.","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"The way we do this is we simply write the output to the 1st input of the function. For example, our Lorenz equation problem would be defined by the function:","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"function lorenz!(du,u,p,t)\n du[1] = 10.0*(u[2]-u[1])\n du[2] = u[1]*(28.0-u[3]) - u[2]\n du[3] = u[1]*u[2] - (8/3)*u[3]\nend","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"and then we can use this function in a problem:","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"u0 = [1.0;0.0;0.0]\ntspan = (0.0,100.0)\nprob = ODEProblem(lorenz!,u0,tspan)\nsol = solve(prob)","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Using the plot recipe tools defined on the plotting page, we can choose to do a 3D phase space plot between the different variables:","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"plot(sol,vars=(1,2,3))","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"(Image: Lorenz System)","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Note that the default plot for multi-dimensional systems is an overlay of each timeseries. We can plot the timeseries of just the second component using the variable choices interface once more:","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"plot(sol,vars=(0,2))","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"(Image: Lorenz Timeseries)","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Note that here \"variable 0\" corresponds to the independent variable (\"time\").","category":"page"},{"location":"tutorials/ode_example/#Defining-Parameterized-Functions","page":"Ordinary Differential Equations","title":"Defining Parameterized Functions","text":"","category":"section"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"In many cases you may want to explicitly have parameters associated with your differential equations. This can be used by things like parameter estimation routines. In this case, you use the p values via the syntax:","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"function parameterized_lorenz!(du,u,p,t)\n du[1] = p[1]*(u[2]-u[1])\n du[2] = u[1]*(p[2]-u[3]) - u[2]\n du[3] = u[1]*u[2] - p[3]*u[3]\nend","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"and then we add the parameters to the ODEProblem:","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"u0 = [1.0,0.0,0.0]\ntspan = (0.0,1.0)\np = [10.0,28.0,8/3]\nprob = ODEProblem(parameterized_lorenz!,u0,tspan,p)","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"We can make our functions look nicer by doing a few tricks. For example:","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"function parameterized_lorenz!(du,u,p,t)\n  x,y,z = u\n  σ,ρ,β = p\n  du[1] = dx = σ*(y-x)\n  du[2] = dy = x*(ρ-z) - y\n  du[3] = dz = x*y - β*z\nend","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Note that the type for the parameters p can be anything: you can use arrays, static arrays, named tuples, etc. to enclose your parameters in a way that is sensible for your problem.","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Since the parameters exist within the function, functions defined in this manner can also be used for sensitivity analysis, parameter estimation routines, and bifurcation plotting. This makes DifferentialEquations.jl a full-stop solution for differential equation analysis which also achieves high performance.","category":"page"},{"location":"tutorials/ode_example/#Example-3:-Solving-Nonhomogeneous-Equations-using-Parameterized-Functions","page":"Ordinary Differential Equations","title":"Example 3: Solving Nonhomogeneous Equations using Parameterized Functions","text":"","category":"section"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Parameterized functions can also be used for building nonhomogeneous ordinary differential equations (these are also referred to as ODEs with nonzero right-hand sides). They are frequently used as models for dynamical systems with external (in general time-varying) inputs. As an example, consider a model of a pendulum consisting of a slender rod of length l and mass m:","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"beginaligned\nfracmathrmdtheta(t)mathrmdt = omega(t)\nfracmathrmdomega(t)mathrmdt = - frac32fracglsintheta(t) + frac3ml^2M(t)\nendaligned","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"where θ and ω are the angular deviation of the pendulum from the vertical (hanging) orientation and the angular rate, respectively, M is an external torque (developed, say, by a wind or a motor), and finally, g stands for gravitional acceleration.","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"using DifferentialEquations\nusing Plots\n\nl = 1.0                             # length [m]\nm = 1.0                             # mass[m]\ng = 9.81                            # gravitational acceleration [m/s²]\n\nfunction pendulum!(du,u,p,t)\n    du[1] = u[2]                    # θ'(t) = ω(t)\n    du[2] = -3g/(2l)*sin(u[1]) + 3/(m*l^2)*p(t) # ω'(t) = -3g/(2l) sin θ(t) + 3/(ml^2)M(t)\nend\n\nθ₀ = 0.01                           # initial angular deflection [rad]\nω₀ = 0.0                            # initial angular velocity [rad/s]\nu₀ = [θ₀, ω₀]                       # initial state vector\ntspan = (0.0,10.0)                  # time interval\n\nM = t->0.1sin(t)                    # external torque [Nm]\n\nprob = ODEProblem(pendulum!,u₀,tspan,M)\nsol = solve(prob)\n\nplot(sol,linewidth=2,xaxis=\"t\",label=[\"θ [rad]\" \"ω [rad/s]\"],layout=(2,1))","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"(Image: Pendulum response)","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Note how the external time-varying torque M is introduced as a parameter in the pendulum! function. Indeed, as a general principle the parameters can be any type; here we specify M as time-varying by representing it by a function, which is expressed by appending the dependence on time (t) to the name of the parameter.  ","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Note also that, in contrast with the time-varying parameter, the (vector of) state variables u, which is generally also time-varying, is always used without the explicit dependence on time (t).","category":"page"},{"location":"tutorials/ode_example/#ode_other_types","page":"Ordinary Differential Equations","title":"Example 4: Using Other Types for Systems of Equations","text":"","category":"section"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"DifferentialEquations.jl can handle many different dependent variable types (generally, anything with a linear index should work!). So instead of solving a vector equation, let's let u be a matrix! To do this, we simply need to have u0 be a matrix, and define f such that it takes in a matrix and outputs a matrix. We can define a matrix of linear ODEs as follows:","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"A  = [1. 0  0 -5\n      4 -2  4 -3\n     -4  0  0  1\n      5 -2  2  3]\nu0 = rand(4,2)\ntspan = (0.0,1.0)\nf(u,p,t) = A*u\nprob = ODEProblem(f,u0,tspan)","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Here our ODE is on a 4x2 matrix, and the ODE is the linear system defined by multiplication by A. To solve the ODE, we do the same steps as before.","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"sol = solve(prob)\nplot(sol)","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"(Image: ODE System Solution)","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"We can instead use the in-place form by using Julia's in-place matrix multiplication function mul!:","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"using LinearAlgebra\nf(du,u,p,t) = mul!(du,A,u)","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Additionally, we can use non-traditional array types as well. For example, StaticArrays.jl offers immutable arrays which are stack-allocated, meaning that their usage does not require any (slow) heap-allocations that arrays normally have. This means that they can be used to solve the same problem as above, with the only change being the type for the initial condition and constants:","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"using StaticArrays, DifferentialEquations\nA  = @SMatrix [ 1.0  0.0 0.0 -5.0\n                4.0 -2.0 4.0 -3.0\n               -4.0  0.0 0.0  1.0\n                5.0 -2.0 2.0  3.0]\nu0 = @SMatrix rand(4,2)\ntspan = (0.0,1.0)\nf(u,p,t) = A*u\nprob = ODEProblem(f,u0,tspan)\nsol = solve(prob)\nusing Plots; plot(sol)","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Note that the analysis tools generalize over to systems of equations as well.","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"sol[4]","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"still returns the solution at the fourth timestep. It also indexes into the array as well. The last value is the timestep, and the beginning values are for the component. This means","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"sol[5,3]","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"is the value of the 5th component (by linear indexing) at the 3rd timepoint, or","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"sol[2,1,:]","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"is the timeseries for the component which is the 2nd row and 1 column.","category":"page"},{"location":"tutorials/ode_example/#Going-Beyond-ODEs:-How-to-Use-the-Documentation","page":"Ordinary Differential Equations","title":"Going Beyond ODEs: How to Use the Documentation","text":"","category":"section"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Not everything can be covered in the tutorials. Instead, this tutorial will end by pointing you in the directions for the next steps.","category":"page"},{"location":"tutorials/ode_example/#Common-API-for-Defining,-Solving,-and-Plotting","page":"Ordinary Differential Equations","title":"Common API for Defining, Solving, and Plotting","text":"","category":"section"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"One feature of DifferentialEquations.jl is that this pattern for solving equations is conserved across the different types of differential equations. Every equation has a problem type, a solution type, and the same solution handling (+ plotting) setup. Thus the solver and plotting commands in the Basics section applies to all sorts of equations, like stochastic differential equations and delay differential equations. Each of these different problem types are defined in the Problem Types section of the docs. Every associated solver algorithm is detailed in the Solver Algorithms section, sorted by problem type. The same steps for ODEs can then be used for the analysis of the solution.","category":"page"},{"location":"tutorials/ode_example/#Additional-Features-and-Analysis-Tools","page":"Ordinary Differential Equations","title":"Additional Features and Analysis Tools","text":"","category":"section"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"In many cases, the common workflow only starts with solving the differential equation. Many common setups have built-in solutions in DifferentialEquations.jl. For example, check out the features for:","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Handling, parallelizing, and analyzing large Ensemble experiments\nSaving the output to tabular formats like DataFrames and CSVs\nEvent handling\nParameter estimation (inverse problems)\nQuantification of numerical uncertainty and error","category":"page"},{"location":"tutorials/ode_example/","page":"Ordinary Differential Equations","title":"Ordinary Differential Equations","text":"Many more are defined in the relevant sections of the docs. Please explore the rest of the documentation, including tutorials for getting started with other types of equations. In addition, to get help, please either file an issue at the main repository or come have an informal discussion at our Gitter chatroom.","category":"page"},{"location":"models/physical/#Physical-Models","page":"Physical Models","title":"Physical Models","text":"","category":"section"},{"location":"models/physical/","page":"Physical Models","title":"Physical Models","text":"The physical modeling functionality is provided by DiffEqPhysics.jl and helps the user build and solve the differential equation based physical models.","category":"page"},{"location":"models/physical/#Hamiltonian-Problems","page":"Physical Models","title":"Hamiltonian Problems","text":"","category":"section"},{"location":"models/physical/","page":"Physical Models","title":"Physical Models","text":"ODEs defined by Hamiltonians is described in the Dynamical ODEs section.","category":"page"},{"location":"models/physical/#N-Body-Problems","page":"Physical Models","title":"N-Body Problems","text":"","category":"section"},{"location":"models/physical/","page":"Physical Models","title":"Physical Models","text":"N-Body problems can be solved by the implementation provided by NBodySimulator.jl using a defined potential:","category":"page"},{"location":"models/physical/","page":"Physical Models","title":"Physical Models","text":"nprob = NBodyProblem(f, mass, vel, pos, tspan)","category":"page"},{"location":"models/physical/","page":"Physical Models","title":"Physical Models","text":"where f is the potential function, mass is the mass matrix, pos and vel are ArrayPartitions for the intial positions and velocity, and tspan is the timespan to solve on.","category":"page"},{"location":"models/physical/#Example","page":"Physical Models","title":"Example","text":"","category":"section"},{"location":"models/physical/","page":"Physical Models","title":"Physical Models","text":"In this example we will model the outer solar system planets.","category":"page"},{"location":"models/physical/","page":"Physical Models","title":"Physical Models","text":"using NBodySimulator\nG = 2.95912208286e-4\nM = [1.00000597682, 0.000954786104043, 0.000285583733151, 0.0000437273164546, 0.0000517759138449, 1/1.3e8]\ninvM = inv.(M)\nplanets = [\"Sun\", \"Jupiter\", \"Saturn\", \"Uranus\", \"Neptune\", \"Pluto\"]\n\npos_x = [0.0,-3.5023653,9.0755314,8.3101420,11.4707666,-15.5387357]\npos_y = [0.0,-3.8169847,-3.0458353,-16.2901086,-25.7294829,-25.2225594]\npos_z = [0.0,-1.5507963,-1.6483708,-7.2521278,-10.8169456,-3.1902382]\npos = ArrayPartition(pos_x,pos_y,pos_z)\n\nvel_x = [0.0,0.00565429,0.00168318,0.00354178,0.00288930,0.00276725]\nvel_y = [0.0,-0.00412490,0.00483525,0.00137102,0.00114527,-0.00170702]\nvel_z = [0.0,-0.00190589,0.00192462,0.00055029,0.00039677,-0.00136504]\nvel = ArrayPartition(vel_x,vel_y,vel_z)\n\ntspan = (0.,200_000)\n\nconst ∑ = sum\nconst N = 6\npotential(p, t, x, y, z, M) = -G*∑(i->∑(j->(M[i]*M[j])/sqrt((x[i]-x[j])^2 + (y[i]-y[j])^2 + (z[i]-z[j])^2), 1:i-1), 2:N)\nnprob = NBodyProblem(potential, M, vel, pos, tspan)\nsol = solve(nprob,Yoshida6(), dt=100)","category":"page"},{"location":"solvers/nonautonomous_linear_ode/#Non-autonomous-Linear-ODE-/-Lie-Group-ODE-Solvers","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"","category":"section"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"Non-autonomous linear ODE solvers focus on equations in the general form of","category":"page"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"u^prime = A(upt)u","category":"page"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"and utilize the Lie group structure of the solution to accelerate the numerical methods and capture certain properties in the solution process. One common simplification is for solvers to require state-independent operators, which implies the form:","category":"page"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"u^prime = A(t)u","category":"page"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"Another type of solvers are needed when the operators are state-dependent, i.e.","category":"page"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"u^prime = A(u)u","category":"page"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"Others specifically require linearity, i.e.","category":"page"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"u^prime = Au","category":"page"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"where A is a constant operator.","category":"page"},{"location":"solvers/nonautonomous_linear_ode/#Recommendations","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Recommendations","text":"","category":"section"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"It is recommended to always specialize on the properties of the operator as much as possible.","category":"page"},{"location":"solvers/nonautonomous_linear_ode/#Standard-ODE-Integrators","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Standard ODE Integrators","text":"","category":"section"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"The standard ODE integrators will work on Non-autonomous linear ODE problems via an automatic transformation to a first-order ODE. See the ODE solvers page for more details.","category":"page"},{"location":"solvers/nonautonomous_linear_ode/#Specialized-OrdinaryDiffEq.jl-Integrators","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Specialized OrdinaryDiffEq.jl Integrators","text":"","category":"section"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"Unless otherwise specified, the OrdinaryDiffEq algorithms all come with a 3rd order Hermite polynomial interpolation. The algorithms denoted as having a \"free\" interpolation means that no extra steps are required for the interpolation. For the non-free higher order interpolating functions, the extra steps are computed lazily (i.e. not during the solve).","category":"page"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"Note that all of these methods are fixed timestep unless otherwise specified.","category":"page"},{"location":"solvers/nonautonomous_linear_ode/#Exponential-Methods-for-Linear-and-Affine-Problems","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Exponential Methods for Linear and Affine Problems","text":"","category":"section"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"These methods require that A is constant.","category":"page"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"LinearExponential - Exact solution formula for linear, time-independent problems.","category":"page"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"Options:","category":"page"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"krylov - symbol. One of\n:off (default) - cache the operator beforehand. Requires Matrix(A) method defined for the operator A.\n:simple - uses simple Krylov approximations with fixed subspace size m.\n:adaptive - uses adaptive Krylov approximations with internal timestepping.\nm - integer, default: 30. Controls the size of Krylov subsapce if krylov=:simple, and the initial subspace size if krylov=:adaptive.\niop - integer, default: 0. If not zero, determines the length of the incomplete orthogonalization procedure (IOP) [1]. Note that if the linear operator/jacobian is hermitian, then the Lanczos algorithm will always be used and the IOP setting is ignored.","category":"page"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"_A = [2 -1;-3 -5]/5\nA = DiffEqArrayOperator(_A)\nprob = ODEProblem(A, [1.0,-1.0], (1.0, 6.0))\nsol = solve(prob, LinearExponential())","category":"page"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"note: Note\nLinearExponential is exact, and thus it uses dt=tspan[2]-tspan[1] by default. The interpolation used is inexact (3rd order Hermite). Thus values generated by the interpolation (via sol(t) or saveat) will be inexact with increasing error as the size of the time span grows. To counteract this, directly set dt or use tstops instead of of saveat. For more information, see this issue","category":"page"},{"location":"solvers/nonautonomous_linear_ode/#State-Independent-Solvers","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"State-Independent Solvers","text":"","category":"section"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"These methods require A is only dependent on the independent variable, i.e. A(t).","category":"page"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"MagnusMidpoint - Second order Magnus Midpoint method.\nMagnusLeapfrog- Second order Magnus Leapfrog method.\nMagnusGauss4 - Fourth order Magnus method approximated using a two stage Gauss quadrature.\nMagnusGL4- Fourth order Magnus method approximated using Gauss-Legendre quadrature.\nMagnusNC6- Sixth order Magnus method approximated using Newton-Cotes quadrature.\nMagnusGL6- Sixth order Magnus method approximated using Gauss-Legendre quadrature.\nMagnusNC8- Eighth order Magnus method approximated using Newton-Cotes quadrature.\nMagnusGL8- Eighth order Magnus method approximated using Gauss-Legendre quadrature.","category":"page"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"Example:","category":"page"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"function update_func(A,u,p,t)\n    A[1,1] = cos(t)\n    A[2,1] = sin(t)\n    A[1,2] = -sin(t)\n    A[2,2] = cos(t)\nend\nA = DiffEqArrayOperator(ones(2,2),update_func=update_func)\nprob = ODEProblem(A, ones(2), (1.0, 6.0))\nsol = solve(prob,MagnusGL6(),dt=1/10)","category":"page"},{"location":"solvers/nonautonomous_linear_ode/#State-Dependent-Solvers","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"State-Dependent Solvers","text":"","category":"section"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"These methods can be used when A is dependent on the state variables, i.e. A(u).","category":"page"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"CayleyEuler - First order method using Cayley transformations.\nLieEuler - First order Lie Euler method.\nRKMK2 - Second order Runge–Kutta–Munthe-Kaas method.\nRKMK4 - Fourth order Runge–Kutta–Munthe-Kaas method.\nLieRK4 - Fourth order Lie Runge-Kutta method.\nCG2 - Second order Crouch–Grossman method.\nMagnusAdapt4 - Fourth Order Adaptive Magnus method.","category":"page"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"Example:","category":"page"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"function update_func(A,u,p,t)\n    A[1,1] = 0\n    A[2,1] = sin(u[1])\n    A[1,2] = -1\n    A[2,2] = 0\nend\nA = DiffEqArrayOperator(ones(2,2),update_func=update_func)\nprob = ODEProblem(A, ones(2), (0, 30.))\nsol = solve(prob,LieRK4(),dt=1/4)","category":"page"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"The above example solves a non-stiff Non-Autonomous Linear ODE with a state dependent operator, using the LieRK4 method. Similarly, a stiff Non-Autonomous Linear ODE with state dependent operators can be solved using specialized adaptive algorithms, like MagnusAdapt4. ","category":"page"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"Example:","category":"page"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"function update_func(A,u,p,t)\n    A[1,1] = 0\n    A[2,1] = 1\n    A[1,2] = -2*(1 - cos(u[2]) - u[2]*sin(u[2]))\n    A[2,2] = 0\nend\nA = DiffEqArrayOperator(ones(2,2),update_func=update_func)\nprob = ODEProblem(A, ones(2), (30, 150.))\nsol = solve(prob,MagnusAdapt4())","category":"page"},{"location":"solvers/nonautonomous_linear_ode/#Time-and-State-Dependent-Operators","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Time and State-Dependent Operators","text":"","category":"section"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"These methods can be used when A is dependent on both time and state variables, i.e. A(ut)","category":"page"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"CG3 - Third order Crouch-Grossman method.","category":"page"},{"location":"solvers/nonautonomous_linear_ode/","page":"Non-autonomous Linear ODE / Lie Group ODE Solvers","title":"Non-autonomous Linear ODE / Lie Group ODE Solvers","text":"[1]: A description of IOP can be found in this paper.","category":"page"},{"location":"types/discrete_types/#Discrete-Problems","page":"Discrete Problems","title":"Discrete Problems","text":"","category":"section"},{"location":"types/discrete_types/#Mathematical-Specification-of-a-Discrete-Problem","page":"Discrete Problems","title":"Mathematical Specification of a Discrete Problem","text":"","category":"section"},{"location":"types/discrete_types/","page":"Discrete Problems","title":"Discrete Problems","text":"To define an Discrete Problem, you simply need to give the function f and the initial condition u₀ which define a function map:","category":"page"},{"location":"types/discrete_types/","page":"Discrete Problems","title":"Discrete Problems","text":"u_n+1 = f(upt_n+1)","category":"page"},{"location":"types/discrete_types/","page":"Discrete Problems","title":"Discrete Problems","text":"f should be specified as f(u,p,t) (or in-place as f(du,u,p,t)), and u₀ should be an AbstractArray (or number) whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher dimension tensors as well. t_n+1 is the current time at which the map is applied. For a FunctionMap with defaults, t_n = t_0 + n*dt (with dt=1 being the default). For continuous-time Markov chains this is the time at which the change is occuring.","category":"page"},{"location":"types/discrete_types/","page":"Discrete Problems","title":"Discrete Problems","text":"Note that if the discrete solver is set to have scale_by_time=true, then the problem is interpreted as the map:","category":"page"},{"location":"types/discrete_types/","page":"Discrete Problems","title":"Discrete Problems","text":"u_n+1 = u_n + dt f(upt_n)","category":"page"},{"location":"types/discrete_types/#Problem-Type","page":"Discrete Problems","title":"Problem Type","text":"","category":"section"},{"location":"types/discrete_types/#Constructors","page":"Discrete Problems","title":"Constructors","text":"","category":"section"},{"location":"types/discrete_types/","page":"Discrete Problems","title":"Discrete Problems","text":"DiscreteProblem{isinplace}(f::ODEFunction,u0,tspan,p=NullParameters();kwargs...) : Defines the discrete problem with the specified functions.\nDiscreteProblem{isinplace}(f,u0,tspan,p=NullParameters();kwargs...) : Defines the discrete problem with the specified functions.\nDiscreteProblem{isinplace}(u0,tspan,p=NullParameters();kwargs...) : Defines the discrete problem with the identity map.","category":"page"},{"location":"types/discrete_types/","page":"Discrete Problems","title":"Discrete Problems","text":"Parameters are optional, and if not given then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.","category":"page"},{"location":"types/discrete_types/","page":"Discrete Problems","title":"Discrete Problems","text":"For specifying Jacobians and mass matrices, see the DiffEqFunctions page.","category":"page"},{"location":"types/discrete_types/#Fields","page":"Discrete Problems","title":"Fields","text":"","category":"section"},{"location":"types/discrete_types/","page":"Discrete Problems","title":"Discrete Problems","text":"f: The function in the map.\nu0: The initial condition.\ntspan: The timespan for the problem.\np: The parameters for the problem. Defaults to NullParameters\nkwargs: The keyword arguments passed onto the solves.","category":"page"},{"location":"types/discrete_types/#Note-About-Timing","page":"Discrete Problems","title":"Note About Timing","text":"","category":"section"},{"location":"types/discrete_types/","page":"Discrete Problems","title":"Discrete Problems","text":"Note that if no dt and not tstops is given, it's assumed that dt=1 and thus tspan=(0,n) will solve for n iterations. If in the solver dt is given, then the number of iterations will change. And if tstops is not empty, the solver will revert to the standard behavior of fixed timestep methods, which is \"step to each tstop\".","category":"page"},{"location":"analysis/uncertainty_quantification/#uncertainty_quantification","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"","category":"section"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"Uncertainty quantification allows a user to identify the uncertainty associated with the numerical approximation given by DifferentialEquations.jl. This page describes the different methods available for quantifying such uncertainties. Note that this requires one of the native Julia solvers like OrdinaryDiffEq.jl, StochasticDiffEq.jl, or DelayDiffEq.jl.","category":"page"},{"location":"analysis/uncertainty_quantification/#Installation","page":"Uncertainty Quantification","title":"Installation","text":"","category":"section"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"This functionality does not come standard with DifferentialEquations.jl. To use this functionality, you must install DiffEqUncertainty.jl:","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"]add DiffEqUncertainty\nusing DiffEqUncertainty","category":"page"},{"location":"analysis/uncertainty_quantification/#ProbInts","page":"Uncertainty Quantification","title":"ProbInts","text":"","category":"section"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"The ProbInts method for uncertainty quantification involves the transformation of an ODE into an associated SDE where the noise is related to the timesteps and the order of the algorithm. This is implemented into the DiffEq system via a callback function. The first form is:","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"ProbIntsUncertainty(σ,order,save=true)","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"σ is the noise scaling factor and order is the order of the algorithm. save is for choosing whether this callback should control the saving behavior. Generally this is true unless one is stacking callbacks in a CallbackSet. It is recommended that σ is representative of the size of the errors in a single step of the equation.","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"If you are using an adaptive algorithm, the callback","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"AdaptiveProbIntsUncertainty(order,save=true)","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"determines the noise scaling automatically using an internal error estimate.","category":"page"},{"location":"analysis/uncertainty_quantification/#Example-1:-FitzHugh-Nagumo","page":"Uncertainty Quantification","title":"Example 1: FitzHugh-Nagumo","text":"","category":"section"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"In this example we will determine our uncertainty when solving the FitzHugh-Nagumo model with the Euler() method. We define the FitzHugh-Nagumo model:","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"function fitz(du,u,p,t)\n  V,R = u\n  a,b,c = p\n  du[1] = c*(V - V^3/3 + R)\n  du[2] = -(1/c)*(V -  a - b*R)\nend\nu0 = [-1.0;1.0]\ntspan = (0.0,20.0)\np = (0.2,0.2,3.0)\nprob = ODEProblem(fitz,u0,tspan,p)","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"Now we define the ProbInts callback. In this case, our method is the Euler method and thus it is order 1. For the noise scaling, we will try a few different values and see how it changes. For σ=0.2, we define the callback as:","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"cb = ProbIntsUncertainty(0.2,1)","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"This is akin to having an error of approximately 0.2 at each step. We now build and solve a EnsembleProblem for 100 trajectories:","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"ensemble_prob = EnsembleProblem(prob)\nsim = solve(ensemble_prob,Euler(),trajectories=100,callback=cb,dt=1/10)","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"Now we can plot the resulting Monte Carlo solution:","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"using Plots; plotly(); plot(sim,vars=(0,1),linealpha=0.4)","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"(Image: uncertainty_02)","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"If we increase the amount of error, we see that some parts of the equation have less uncertainty than others. For example, at σ=0.5:","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"cb = ProbIntsUncertainty(0.5,1)\nensemble_prob = EnsembleProblem(prob)\nsim = solve(ensemble_prob,Euler(),trajectories=100,callback=cb,dt=1/10)\nusing Plots; plotly(); plot(sim,vars=(0,1),linealpha=0.4)","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"(Image: uncertainty_05)","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"But at this amount of noise, we can see how we contract to the true solution by decreasing dt:","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"cb = ProbIntsUncertainty(0.5,1)\nensemble_prob = EnsembleProblem(prob)\nsim = solve(ensemble_prob,Euler(),trajectories=100,callback=cb,dt=1/100)\nusing Plots; plotly(); plot(sim,vars=(0,1),linealpha=0.4)","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"(Image: uncertainty_lowh)","category":"page"},{"location":"analysis/uncertainty_quantification/#Example-2:-Adaptive-ProbInts-on-FitzHugh-Nagumo","page":"Uncertainty Quantification","title":"Example 2: Adaptive ProbInts on FitzHugh-Nagumo","text":"","category":"section"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"While the first example is academic and shows how the ProbInts method scales, the fact that one should have some idea of the error in order to calibrate σ can lead to complications. Thus the more useful method in many cases is the AdaptiveProbIntsUncertainty version. In this version, no σ is required since this is calculated using an internal error estimate. Thus this gives an accurate representation of the possible error without user input.","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"Let's try this with the order 5 Tsit5() method on the same problem as before:","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"cb = AdaptiveProbIntsUncertainty(5)\nsol = solve(prob,Tsit5())\nensemble_prob = EnsembleProblem(prob)\nsim = solve(ensemble_prob,Tsit5(),trajectories=100,callback=cb)\nusing Plots; plotly(); plot(sim,vars=(0,1),linealpha=0.4)","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"(Image: uncertainty_adaptive_default)","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"In this case, we see that the default tolerances give us a very good solution. However, if we increase the tolerance a lot:","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"cb = AdaptiveProbIntsUncertainty(5)\nsol = solve(prob,Tsit5())\nensemble_prob = EnsembleProblem(prob)\nsim = solve(ensemble_prob,Tsit5(),trajectories=100,callback=cb,abstol=1e-3,reltol=1e-1)\nusing Plots; plotly(); plot(sim,vars=(0,1),linealpha=0.4)","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"(Image: uncertainty_adaptive_default)","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"we can see that the moments just after the rise can be uncertain.","category":"page"},{"location":"analysis/uncertainty_quantification/#Example-3:-Adaptive-ProbInts-on-the-Lorenz-Attractor","page":"Uncertainty Quantification","title":"Example 3: Adaptive ProbInts on the Lorenz Attractor","text":"","category":"section"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"One very good use of uncertainty quantification is on chaotic models. Chaotic equations diverge from the true solution according to the error exponentially. This means that as time goes on, you get further and further from the solution. The ProbInts method can help diagnose how much of the timeseries is reliable.","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"As in the previous example, we first define the model:","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"function g(du,u,p,t)\n du[1] = p[1]*(u[2]-u[1])\n du[2] = u[1]*(p[2]-u[3]) - u[2]\n du[3] = u[1]*u[2] - p[3]*u[3]\nend\nu0 = [1.0;0.0;0.0]\ntspan = (0.0,30.0)\np = [10.0,28.0,8/3]\nprob = ODEProblem(g,u0,tspan,p)","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"and then we build the ProbInts type. Let's use the order 5 Tsit5 again.","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"cb = AdaptiveProbIntsUncertainty(5)","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"Then we solve the MonteCarloProblem","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"ensemble_prob = EnsembleProblem(prob)\nsim = solve(ensemble_prob,Tsit5(),trajectories=100,callback=cb)\nusing Plots; plotly(); plot(sim,vars=(0,1),linealpha=0.4)","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"(Image: uncertainty_chaos)","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"Here we see that by t about 22 we start to receive strong deviations from the \"true\" solution. We can increase the amount of time before error explosion by using a higher order method with stricter tolerances:","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"tspan = (0.0,40.0)\nprob = ODEProblem(g,u0,tspan,p)\ncb = AdaptiveProbIntsUncertainty(7)\nensemble_prob = EnsembleProblem(prob)\nsim = solve(ensemble_prob,Vern7(),trajectories=100,callback=cb,reltol=1e-6)\nusing Plots; plotly(); plot(sim,vars=(0,1),linealpha=0.4)","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"(Image: uncertainty_high_order)","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"we see that we can extend the amount of time until we deviate strongly from the \"true\" solution. Of course, for a chaotic system like the Lorenz one presented here, it is impossible to follow the true solution for long times, due to the fact that the system is chaotic and unavoidable deviations due to the numerical precision of a computer get amplified exponentially.","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"However, not all hope is lost. The shadowing theorem is a strong statement for having confidence in numerical evolution of chaotic systems:","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"Although a numerically computed chaotic trajectory diverges exponentially from the true trajectory with the same initial coordinates, there exists an errorless trajectory with a slightly different initial condition that stays near (\"shadows\") the numerically computed one.","category":"page"},{"location":"analysis/uncertainty_quantification/","page":"Uncertainty Quantification","title":"Uncertainty Quantification","text":"For more info on the shadowing theorem, please see the book Chaos in Dynamical Systems by E. Ott.","category":"page"},{"location":"types/ode_types/#ode_prob","page":"ODE Problems","title":"ODE Problems","text":"","category":"section"},{"location":"types/ode_types/#Mathematical-Specification-of-an-ODE-Problem","page":"ODE Problems","title":"Mathematical Specification of an ODE Problem","text":"","category":"section"},{"location":"types/ode_types/","page":"ODE Problems","title":"ODE Problems","text":"To define an ODE Problem, you simply need to give the function f and the initial condition u₀ which define an ODE:","category":"page"},{"location":"types/ode_types/","page":"ODE Problems","title":"ODE Problems","text":"fracdudt = f(upt)","category":"page"},{"location":"types/ode_types/","page":"ODE Problems","title":"ODE Problems","text":"f should be specified as f(u,p,t) (or in-place as f(du,u,p,t)), and u₀ should be an AbstractArray (or number) whose geometry matches the desired geometry of u. Note that we are not limited to numbers or vectors for u₀; one is allowed to provide u₀ as arbitrary matrices / higher dimension tensors as well.","category":"page"},{"location":"types/ode_types/#Problem-Type","page":"ODE Problems","title":"Problem Type","text":"","category":"section"},{"location":"types/ode_types/#Constructors","page":"ODE Problems","title":"Constructors","text":"","category":"section"},{"location":"types/ode_types/","page":"ODE Problems","title":"ODE Problems","text":"ODEProblem(f::ODEFunction,u0,tspan,p=NullParameters();kwargs...)\nODEProblem{isinplace}(f,u0,tspan,p=NullParameters();kwargs...) : Defines the ODE with the specified functions. isinplace optionally sets whether the function is inplace or not. This is determined automatically, but not inferred.","category":"page"},{"location":"types/ode_types/","page":"ODE Problems","title":"ODE Problems","text":"Parameters are optional, and if not given then a NullParameters() singleton will be used which will throw nice errors if you try to index non-existent parameters. Any extra keyword arguments are passed on to the solvers. For example, if you set a callback in the problem, then that callback will be added in every solve call.","category":"page"},{"location":"types/ode_types/","page":"ODE Problems","title":"ODE Problems","text":"For specifying Jacobians and mass matrices, see the DiffEqFunctions page.","category":"page"},{"location":"types/ode_types/#Fields","page":"ODE Problems","title":"Fields","text":"","category":"section"},{"location":"types/ode_types/","page":"ODE Problems","title":"ODE Problems","text":"f: The function in the ODE.\nu0: The initial condition.\ntspan: The timespan for the problem.\np: The parameters.\nkwargs: The keyword arguments passed onto the solves.","category":"page"},{"location":"types/ode_types/#Example-Problems","page":"ODE Problems","title":"Example Problems","text":"","category":"section"},{"location":"types/ode_types/","page":"ODE Problems","title":"ODE Problems","text":"Example problems can be found in DiffEqProblemLibrary.jl.","category":"page"},{"location":"types/ode_types/","page":"ODE Problems","title":"ODE Problems","text":"To use a sample problem, such as prob_ode_linear, you can do something like:","category":"page"},{"location":"types/ode_types/","page":"ODE Problems","title":"ODE Problems","text":"#] add DiffEqProblemLibrary\nusing DiffEqProblemLibrary.ODEProblemLibrary\n# load problems\nODEProblemLibrary.importodeproblems()\nprob = ODEProblemLibrary.prob_ode_linear\nsol = solve(prob)","category":"page"},{"location":"types/ode_types/","page":"ODE Problems","title":"ODE Problems","text":"CurrentModule = ODEProblemLibrary","category":"page"},{"location":"types/ode_types/","page":"ODE Problems","title":"ODE Problems","text":"prob_ode_linear\nprob_ode_2Dlinear\nprob_ode_bigfloatlinear\nprob_ode_bigfloat2Dlinear\nprob_ode_large2Dlinear\nprob_ode_2Dlinear_notinplace\nprob_ode_lotkavoltera\nprob_ode_fitzhughnagumo\nprob_ode_threebody\nprob_ode_pleiades\nprob_ode_vanderpol\nprob_ode_vanstiff\nprob_ode_rober\nprob_ode_rigidbody\nprob_ode_hires\nprob_ode_orego\nprob_ode_pollution\nprob_ode_nonlinchem\nprob_ode_brusselator_1d\nprob_ode_brusselator_2d\nprob_ode_filament\nprob_ode_thomas\nprob_ode_lorenz\nprob_ode_aizawa\nprob_ode_dadras\nprob_ode_chen\nprob_ode_rossler\nprob_ode_rabinovich_fabrikant\nprob_ode_sprott\nprob_ode_hindmarsh_rose","category":"page"},{"location":"types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_linear","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_linear","text":"Linear ODE\n\nfracdudt = αu\n\nwith initial condition u_0=frac12, α=101, and solution\n\nu(t) = u_0e^αt\n\nwith Float64s. The parameter is α\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_2Dlinear","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_2Dlinear","text":"4x2 version of the Linear ODE\n\nfracdudt = αu\n\nwith initial condition u_0 as all uniformly distributed random numbers,  α=101, and solution\n\nu(t) = u_0e^αt\n\nwith Float64s\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_bigfloatlinear","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_bigfloatlinear","text":"Linear ODE\n\nfracdudt = αu\n\nwith initial condition u_0=frac12, α=101, and solution\n\nu(t) = u_0e^αt\n\nwith BigFloats\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_bigfloat2Dlinear","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_bigfloat2Dlinear","text":"4x2 version of the Linear ODE\n\nfracdudt = αu\n\nwith initial condition u_0 as all uniformly distributed random numbers,  α=101, and solution\n\nu(t) = u_0e^αt\n\nwith BigFloats\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_large2Dlinear","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_large2Dlinear","text":"100x100 version of the Linear ODE\n\nfracdudt = αu\n\nwith initial condition u_0 as all uniformly distributed random numbers,  α=101, and solution\n\nu(t) = u_0e^αt\n\nwith Float64s\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_2Dlinear_notinplace","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_2Dlinear_notinplace","text":"4x2 version of the Linear ODE\n\nfracdudt = αu\n\nwith initial condition u_0 as all uniformly distributed random numbers,  α=101, and solution\n\nu(t) = u_0e^αt\n\non Float64. Purposefully not in-place as a test.\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_lotkavoltera","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_lotkavoltera","text":"Lotka-Voltera Equations (Non-stiff)\n\nfracdxdt = ax - bxy\n\nfracdydt = -cy + dxy\n\nwith initial condition x=y=1\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_fitzhughnagumo","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_fitzhughnagumo","text":"Fitzhugh-Nagumo (Non-stiff)\n\nfracdvdt = v - fracv^33 - w + I_est\n\nτ fracdwdt = v + a -bw\n\nwith initial condition v=w=1\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_threebody","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_threebody","text":"The ThreeBody problem as written by Hairer: (Non-stiff)\n\nfracdy₁dt = y₁ + 2fracdy₂dt - barμfracy₁+μD₁ - μfracy₁-barμD₂\n\nfracdy₂dt = y₂ - 2fracdy₁dt - barμfracy₂D₁ - μfracy₂D₂\n\nD₁ = ((y₁+μ)^2 + y₂^2)^32\n\nD₂ = ((y₁-barμ)^2+y₂^2)^32\n\nμ = 0012277471\n\nbarμ =1-μ\n\nFrom Hairer Norsett Wanner Solving Ordinary Differential Equations I - Nonstiff Problems Page 129\n\nUsually solved on t₀ = 00 and T = 170652165601579625588917206249 Periodic with that setup.\n\n\n\n","category":"constant"},{"location":"types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_pleiades","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_pleiades","text":"Pleiades Problem (Non-stiff)\n\nfracd^2xᵢdt^2 = sum_ji mⱼ(xⱼ-xᵢ)rᵢⱼ\n\nfracd^2yᵢdt^2 = sum_ji mⱼ(yⱼ-yᵢ)rᵢⱼ\n\nwhere\n\nrᵢⱼ = ((xᵢ-xⱼ)^2 + (yᵢ-yⱼ)^2)^32\n\nand initial conditions are\n\nx₁(0) = 3\n\nx₂(0) = 3\n\nx₃(0) = -1\n\nx₄(0) = -3\n\nx₅(0) = 2\n\nx₆(0) = -2\n\nx₇(0) = 2\n\ny₁(0) = 3\n\ny₂(0) = -3\n\ny₃(0) = 2\n\ny₄(0) = 0\n\ny₅(0) = 0\n\ny₆(0) = -4\n\ny₇(0) = 4\n\nand with fracdxᵢ(0)dt=fracdyᵢ(0)dt=0 except for\n\nfracdx₆(0)dt = 175\n\nfracdx₇(0)dt = -15\n\nfracdy₄(0)dt = -125\n\nfracdy₅(0)dt = 1\n\nFrom Hairer Norsett Wanner Solving Ordinary Differential Equations I - Nonstiff Problems Page 244\n\nUsually solved from 0 to 3.\n\n\n\n","category":"constant"},{"location":"types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_vanderpol","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_vanderpol","text":"Van der Pol Equations\n\nfracdxdt = y\n\nfracdydt = μ((1-x^2)y -x)\n\nwith μ=10 and u_0=0sqrt3\n\nNon-stiff parameters.\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_vanstiff","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_vanstiff","text":"Van der Pol Equations\n\nfracdxdt = y\n\nfracdydt = μ((1-x^2)y -x)\n\nwith μ=10^6 and u_0=0sqrt3\n\nStiff parameters.\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_rober","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_rober","text":"The Robertson biochemical reactions: (Stiff)\n\nfracdy₁dt = -k₁y₁+k₃y₂y₃\n\nfracdy₂dt =  k₁y₁-k₂y₂^2-k₃y₂y₃\n\nfracdy₃dt =  k₂y₂^2\n\nwhere k₁=004, k₂=3times10^7, k₃=10^4. For details, see:\n\nHairer Norsett Wanner Solving Ordinary Differential Equations I - Nonstiff Problems Page 129\n\nUsually solved on 01e11\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_rigidbody","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_rigidbody","text":"Rigid Body Equations (Non-stiff)\n\nfracdy₁dt  = I₁y₂y₃\n\nfracdy₂dt  = I₂y₁y₃\n\nfracdy₃dt  = I₃y₁y₂\n\nwith I₁=-2, I₂=125, and I₃=-12.\n\nThe initial condition is y=100009.\n\nFrom Solving Differential Equations in R by Karline Soetaert\n\nor Hairer Norsett Wanner Solving Ordinary Differential Equations I - Nonstiff Problems Page 244\n\nUsually solved from 0 to 20.\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_hires","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_hires","text":"Hires Problem (Stiff)\n\nIt is in the form of \n\nfracdydt = f(y)\n\nwith\n\n y(0)=y_0 quad y in ℝ^8 quad 0  t  3218122\n\nwhere f is defined by\n\nf(y) = beginpmatrix 171y_1  +043y_2  +832y_3  +00007y_4   171y_1  875y_2     1003y_3  +043y_4  +0035y_5    832y_2  +171y_3  112y_4    1745y_5  +043y_6  +043y_7    280y_6y_8  +069y_4  +171y_5  043y_6  +069y_7  280y_6y_8  181y_7     280y_6y_8  +181y_7    endpmatrix\n\nReference: demohires.pdf   Notebook: Hires.ipynb\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_orego","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_orego","text":"Orego Problem (Stiff)\n\nIt is in the form of fracdydt=f(y) quad y(0)=y0 with\n\ny in ℝ^3 quad 0  t  360\n\nwhere f is defined by\n\nf(y) = beginpmatrix s(y_2 - y_1(1-qy_1-y_2))  (y_3 - y_2(1+y_1))s  w(y_1-y_3) endpmatrix\n\nwhere s=7727, w=0161 and q=837510^-6.\n\nReference: demoorego.pdf Notebook: Orego.ipynb\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_pollution","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_pollution","text":"Pollution Problem (Stiff)\n\nThis IVP is a stiff system of 20 non-linear Ordinary Differential Equations. It is in the form of \n\nfracdydt=f(y)\n\nwith\n\ny(0)=y0 quad y in ℝ^20 quad 0  t  60\n\nwhere f is defined by\n\nf(y) = beginpmatrix -sum_j110142324 r_j + sum_j23911122225 r_j  -r_2 - r_3 - r_9 - r_12 + r_1 + r_21  -r_15 + r_1 + r_17 + r_19 + r_22  -r_2 - r_16 - r_17 - r_23 + r_15  -r_3 + 2r_4 + r_6 + r_7 + r_13 + r_20  -r_6 - r_8 - r_14 - r_20 + r_3 + 2r_18  -r_4 - r_5 - r_6 + r_13  r_4 + r_5 + r_6 + r_7  -r_7 - r_8  -r_12 + r_7 + r_9  -r_9 - r_10 + r_8 + r_11  r_9  -r_11 + r_10  -r_13 + r_12  r_14  -r_18 - r_19 + r_16  -r_20  r_20  -r21 - r_22 - r_24 + r_23 + r_25  -r_25 + r_24 endpmatrix\n\nwith the initial condition of\n\ny0 = (0 02 0 004 0 0 01 03 001 0 0 0 0 0 0 0 0007 0 0 0)^T\n\nAnalytical Jacobian is included.\n\nReference: pollu.pdf Notebook: Pollution.ipynb\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_nonlinchem","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_nonlinchem","text":"Nonlinear system of reactions with an analytical solution\n\nfracdy_1dt = -y_1\n\nfracdy_2dt = y_1 - y_2^2\n\nfracdy_3dt = y_2^2\n\nwith initial condition y=100 on a time span of t in (020)\n\nFrom\n\nLiu, L. C., Tian, B., Xue, Y. S., Wang, M., & Liu, W. J. (2012). Analytic solution  for a nonlinear chemistry system of ordinary differential equations. Nonlinear  Dynamics, 68(1-2), 17-21.\n\nThe analytical solution is implemented, allowing easy testing of ODE solvers.\n\n\n\n","category":"constant"},{"location":"types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_brusselator_1d","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_brusselator_1d","text":"1D Brusselator\n\nfracpartial upartial t = A + u^2v - (B+1)u + alphafracpartial^2 upartial x^2\n\nfracpartial vpartial t = Bu - u^2v + alphafracpartial^2 upartial x^2\n\nand the initial conditions are\n\nu(x0) = 1+sin(2π x)\n\nv(x0) = 3\n\nwith the boundary condition\n\nu(0t) = u(1t) = 1\n\nv(0t) = v(1t) = 3\n\nFrom Hairer Norsett Wanner Solving Ordinary Differential Equations II - Stiff and Differential-Algebraic Problems Page 6\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_brusselator_2d","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_brusselator_2d","text":"2D Brusselator\n\nfracpartial upartial t = 1 + u^2v - 44u + alpha(fracpartial^2 upartial x^2 + fracpartial^2 upartial y^2) + f(x y t)\n\nfracpartial vpartial t = 34u - u^2v + alpha(fracpartial^2 upartial x^2 + fracpartial^2 upartial y^2)\n\nwhere\n\nf(x y t) = begincases 5  quad textif  (x-03)^2+(y-06)^2  01^2 text and  t  11  0  quad textelse endcases\n\nand the initial conditions are\n\nu(x y 0) = 22cdot y(1-y)^32\n\nv(x y 0) = 27cdot x(1-x)^32\n\nwith the periodic boundary condition\n\nu(x+1yt) = u(xyt)\n\nu(xy+1t) = u(xyt)\n\nFrom Hairer Norsett Wanner Solving Ordinary Differential Equations II - Stiff and Differential-Algebraic Problems Page 152\n\n\n\n\n\n","category":"constant"},{"location":"types/ode_types/#DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_filament","page":"ODE Problems","title":"DiffEqProblemLibrary.ODEProblemLibrary.prob_ode_filament","text":"Filament PDE Discretization\n\nNotebook: Filament.ipynb\n\nIn this problem is a real-world biological model from a paper entitled Magnetic dipole with a flexible tail as a self-propelling microdevice. It is a system of PDEs representing a Kirchhoff model of an elastic rod, where the equations of motion are given by the Rouse approximation with free boundary conditions.\n\n\n\n\n\n","category":"constant"},{"location":"solvers/sdde_solve/#SDDE-Solvers","page":"SDDE Solvers","title":"SDDE Solvers","text":"","category":"section"},{"location":"solvers/sdde_solve/","page":"SDDE Solvers","title":"SDDE Solvers","text":"solve(prob::AbstractSDDEProblem, alg; kwargs)","category":"page"},{"location":"solvers/sdde_solve/","page":"SDDE Solvers","title":"SDDE Solvers","text":"Solves the SDDE defined by prob using the algorithm alg. If no algorithm is given, a default algorithm will be chosen.","category":"page"},{"location":"solvers/sdde_solve/#Recommended-Methods","page":"SDDE Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"solvers/sdde_solve/","page":"SDDE Solvers","title":"SDDE Solvers","text":"The recommended method for SDDE problems are the SDE algorithms. On SDEs you simply reuse the same algorithm as the SDE solver, and StochasticDelayDiffEq.jl will convert it to an SDDE solver. The recommendations for SDDE solvers match those of SDEs, except that only up to strong order 1 is recommended. Note too that order 1 is currently only attainable if there is no delay term in the diffusion function g: delays in the drift function f are compatible with first order convergence. Theoretical issues with higher order methods (1.5+) on SDDEs is currently unknown.","category":"page"},{"location":"solvers/sdde_solve/","page":"SDDE Solvers","title":"SDDE Solvers","text":"Note that adaptive time stepping utilizes the same rejection sampling with memory technique as SDEs, but no proof of convergence is known for SDDEs.","category":"page"},{"location":"solvers/sdde_solve/#Example","page":"SDDE Solvers","title":"Example","text":"","category":"section"},{"location":"solvers/sdde_solve/","page":"SDDE Solvers","title":"SDDE Solvers","text":"function hayes_modelf(du,u,h,p,t)\n    τ,a,b,c,α,β,γ = p\n    du .= a.*u .+ b .* h(p,t-τ) .+ c\nend\nfunction hayes_modelg(du,u,h,p,t)\n    τ,a,b,c,α,β,γ = p\n    du .= α.*u .+ γ\nend\nh(p,t) = (ones(1) .+ t);\ntspan = (0.,10.)\n\npmul = [1.0,-4.,-2.,10.,-1.3,-1.2, 1.1]\npadd = [1.0,-4.,-2.,10.,-0.0,-0.0, 0.1]\n\nprob = SDDEProblem(hayes_modelf, hayes_modelg, [1.], h, tspan, pmul; constant_lags = (pmul[1],));\nsol = solve(prob,RKMil())","category":"page"},{"location":"features/noise_process/#noise_process","page":"Noise Processes","title":"Noise Processes","text":"","category":"section"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"Noise processes are essential in continuous stochastic modeling. The NoiseProcess types are distributionally-exact, meaning they are not solutions of stochastic differential equations and instead are directly generated according to their analytical distributions. These processes are used as the noise term in the SDE and RODE solvers. Additionally, the noise processes themselves can be simulated and solved using the DiffEq common interface (including the Monte Carlo interface).","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"This page first describes how to use noise processes in SDEs, and analyze/simulate noise processes directly. Then it describes the standard noise processes which are available. Processes like WienerProcess, CorrelatedWienerProcess, GeometricBrownianMotionProcess, BrownianBridgeProcess and OrnsteinUhlenbeckProcess are pre-defined. Then it is shown how one can define the distributions for a new NoiseProcess.","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"In addition to the NoiseProcess type, more general AbstractNoiseProcesses are defined. The NoiseGrid allows you to define a noise process from a set of pre-calculated points (the \"normal\" way). The NoiseApproximation allows you to define a new noise process as the solution to some stochastic differential equation. While these methods are only approximate, they are more general and allow the user to easily define their own colored noise to use in simulations.","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"The NoiseWrapper allows one to wrap a NoiseProcess from a previous simulation to re-use it in a new simulation in a way that follows the same stochastic trajectory (even if different points are hit, for example solving with a smaller dt) in a distributionally-exact manner. It is demonstrated how the NoiseWrapper can be used to wrap the NoiseProcess of one SDE/RODE solution in order to re-use the same noise process in another simulation.","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"The VirtualBrownianTree allows one to trade speed for O(1) memory usage. Instead of storing Brownian motion increments, the VirtualBrownianTree samples recursively from the midpoint tmid of Brownian bridges, using a splittable PRNG. The recursion terminates when the query time agrees within some tolerance with tmid or when the maximum depth of the tree is reached.","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"Lastly, the NoiseFunction allows you to use any function of time as the noise process. Together, this functionality allows you to define any colored noise process and use this efficiently and accurately in your simulations.","category":"page"},{"location":"features/noise_process/#Using-Noise-Processes","page":"Noise Processes","title":"Using Noise Processes","text":"","category":"section"},{"location":"features/noise_process/#Passing-a-Noise-Process-to-a-Problem-Type","page":"Noise Processes","title":"Passing a Noise Process to a Problem Type","text":"","category":"section"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"AbstractNoiseProcesses can be passed directly to the problem types to replace the standard Wiener process (Brownian motion) with your choice of noise. To do this, simply construct the noise and pass it to the noise keyword argument:","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"μ = 1.0\nσ = 2.0\nW = GeometricBrownianMotionProcess(μ,σ,0.0,1.0,1.0)\n# ...\n# Define f,g,u0,tspan for a SDEProblem\n# ...\nprob = SDEProblem(f,g,u0,tspan,noise=W)","category":"page"},{"location":"features/noise_process/#Basic-Interface","page":"Noise Processes","title":"Basic Interface","text":"","category":"section"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"The NoiseProcess acts like a DiffEq solution. For some noise process W, you can get its ith timepoint like W[i] and the associated time W.t[i]. If the NoiseProcess has a bridging distribution defined, it can be interpolated to arbitrary time points using W(t). Note that every interpolated value is saved to the NoiseProcess so that way it can stay distributionally correct. A plot recipe is provided which plots the timeseries.","category":"page"},{"location":"features/noise_process/#Direct-Simulation-of-the-Noise-Process","page":"Noise Processes","title":"Direct Simulation of the Noise Process","text":"","category":"section"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"Since the NoiseProcess types are distribution-exact and do not require the stochastic differential equation solvers, many times one would like to directly simulate trajectories from these proecesses. The NoiseProcess has a NoiseProcessProblem type:","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"NoiseProblem(noise,tspan)","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"for which solve works. For example, we can simulate a distributionally-exact Geometric Brownian Motion solution by:","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"μ = 1.0\nσ = 2.0\nW = GeometricBrownianMotionProcess(μ,σ,0.0,1.0,1.0)\nprob = NoiseProblem(W,(0.0,1.0))\nsol = solve(prob;dt=0.1)","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"solve requires the dt is given, the solution it returns is a NoiseProcess which has stepped through the timespan. Because this follows the common interface, all of the normal functionality works. For example, we can use the Monte Carlo functionality as follows:","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"monte_prob = MonteCarloProblem(prob)\nsol = solve(monte_prob;dt=0.1,num_monte=100)","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"simulates 100 Geometric Brownian Motions.","category":"page"},{"location":"features/noise_process/#Direct-Interface","page":"Noise Processes","title":"Direct Interface","text":"","category":"section"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"Most of the time, a NoiseProcess is received from the solution of a stochastic or random differential equation, in which case sol.W gives the NoiseProcess and it is already defined along some timeseries. In other cases, NoiseProcess types are directly simulated (see below). However, NoiseProcess types can also be directly acted on. The basic functionality is given by calculate_step! to calculate a future time point, and accept_step! to accept the step. If steps are rejected, the Rejection Sampling with Memory algorithm is applied to keep the solution distributionally exact. This kind of stepping is done via:","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"W = WienerProcess(0.0,1.0,1.0)\ndt = 0.1\nW.dt = dt\nu = nothing; p = nothing # for state-dependent distributions\ncalculate_step!(W,dt,u,p)\nfor i in 1:10\n  accept_step!(W,dt,u,p)\nend","category":"page"},{"location":"features/noise_process/#Noise-Process-Types","page":"Noise Processes","title":"Noise Process Types","text":"","category":"section"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"This section describes the available NoiseProcess types. Note that all keyword arguments are splatted into the NoiseProcess constructor, and thus options like reset are available on the pre-built processes.","category":"page"},{"location":"features/noise_process/#Wiener-Process","page":"Noise Processes","title":"Wiener Process","text":"","category":"section"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"The WienerProcess, also known as Brownian motion, or the noise in the Langevin equation, is the stationary process with white noise increments and a distribution N(0,dt). The constructor is:","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"WienerProcess(t0,W0,Z0=nothing;kwargs...)\nWienerProcess!(t0,W0,Z0=nothing;kwargs...)","category":"page"},{"location":"features/noise_process/#Real-Wiener-Process","page":"Noise Processes","title":"Real Wiener Process","text":"","category":"section"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"The RealWienerProcess is a Brownian motion that is forced to be real-valued. While the normal WienerProcess becomes complex valued if W0 is complex, this verion is real valued for when you want to, for example, solve an SDE defined by complex numbers where the noise is in the reals.","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"RealWienerProcess(t0,W0,Z0=nothing;kwargs...)\nRealWienerProcess!(t0,W0,Z0=nothing;kwargs...)","category":"page"},{"location":"features/noise_process/#Correlated-Noise","page":"Noise Processes","title":"Correlated Noise","text":"","category":"section"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"One can define a CorrelatedWienerProcess which is a Wiener process with correlations between the Wiener processes. The constructor is:","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"CorrelatedWienerProcess(Γ,t0,W0,Z0=nothing;kwargs...)\nCorrelatedWienerProcess!(Γ,t0,W0,Z0=nothing;kwargs...)","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"where Γ is the constant covariance matrix.","category":"page"},{"location":"features/noise_process/#Geometric-Brownian-Motion","page":"Noise Processes","title":"Geometric Brownian Motion","text":"","category":"section"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"A GeometricBrownianMotion process is a Wiener process with constant drift μ and constant diffusion σ. I.e. this is the solution of the stochastic differential equation","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"dX_t = mu X_t dt + sigma X_t dW_t","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"The GeometricBrownianMotionProcess is distribution exact (meaning, not a numerical solution of the stochastic differential equation, and instead follows the exact distribution properties). It can be back interpolated exactly as well. The constructor is:","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"GeometricBrownianMotionProcess(μ,σ,t0,W0,Z0=nothing;kwargs...)\nGeometricBrownianMotionProcess!(μ,σ,t0,W0,Z0=nothing;kwargs...)","category":"page"},{"location":"features/noise_process/#Brownian-Bridge","page":"Noise Processes","title":"Brownian Bridge","text":"","category":"section"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"A BrownianBridge process is a Wiener process with a pre-defined start and end value. This process is distribution exact and back be back interpolated exactly as well. The constructor is:","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"BrownianBridge(t0,tend,W0,Wend,Z0=nothing,Zend=nothing;kwargs...)\nBrownianBridge!(t0,tend,W0,Wend,Z0=nothing,Zend=nothing;kwargs...)","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"where W(t0)=W₀, W(tend)=Wend, and likewise for the Z process if defined.","category":"page"},{"location":"features/noise_process/#Ornstein-Uhlenbeck","page":"Noise Processes","title":"Ornstein-Uhlenbeck","text":"","category":"section"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"One can define a Ornstein-Uhlenbeck process which is a Wiener process defined by the stochastic differential equation","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"dX_t = theta (mu - X_t) dt + sigma dW_t","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"The OrnsteinUhlenbeckProcess is distribution exact (meaning, not a numerical solution of the stochastic differential equation, and instead follows the exact distribution properties). The constructor is:","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"OrnsteinUhlenbeckProcess(Θ,μ,σ,t0,W0,Z0=nothing;kwargs...)\nOrnsteinUhlenbeckProcess!(Θ,μ,σ,t0,W0,Z0=nothing;kwargs...)","category":"page"},{"location":"features/noise_process/#Direct-Construction-of-a-NoiseProcess","page":"Noise Processes","title":"Direct Construction of a NoiseProcess","text":"","category":"section"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"A NoiseProcess is a type defined as","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"NoiseProcess(t0,W0,Z0,dist,bridge;\n             iip=SciMLBase.isinplace(dist,3),\n             rswm = RSWM(),save_everystep=true,\n             rng = Xorshifts.Xoroshiro128Plus(rand(UInt64)),\n             reset = true, reseed = true)","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"t0 is the first timepoint\nW0 is the first value of the process.\nZ0 is the first value of the pseudo-process. This is necessary for higher order algorithms. If it's not needed, set to nothing.\ndist the distribution for the steps over time.\nbridge the bridging distribution. Optional, but required for adaptivity and interpolating at new values.\nsave_everystep whether to save every step of the Brownian timeseries.\nrng the local RNG used for generating the random numbers.\nreset whether to reset the process with each solve.\nreseed whether to reseed the process with each solve.","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"The signature for the dist is","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"dist!(rand_vec,W,dt,rng)","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"for inplace functions, and","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"rand_vec = dist(W,dt,rng)","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"otherwise. The signature for bridge is","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"bridge!(rand_vec,W,W0,Wh,q,h,rng)","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"and the out of place syntax is","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"rand_vec = bridge!(W,W0,Wh,q,h,rng)","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"Here, W is the noise process, W0 is the left side of the current interval, Wh is the right side of the current interval, h is the interval length, and q is the proportion from the left where the interpolation is occuring.","category":"page"},{"location":"features/noise_process/#Direct-Construction-Example","page":"Noise Processes","title":"Direct Construction Example","text":"","category":"section"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"The easiest way to show how to directly construct a NoiseProcess is by example. Here we will show how to directly construct a NoiseProcess which generates Gaussian white noise.","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"This is the noise process which uses randn!. A special dispatch is added for complex numbers for (randn()+im*randn())/sqrt(2). This function is DiffEqNoiseProcess.wiener_randn (or with ! respectively).","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"The first function that must be defined is the noise distribution. This is how to generate W(t+dt) given that we know W(x) for xt₀t. For Gaussian white noise, we know that","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"W(dt)  N(0dt)","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"for W(0)=0 which defines the stepping distribution. Thus its noise distribution function is:","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"@inline function WHITE_NOISE_DIST(W,dt,rng)\n  if typeof(W.dW) <: AbstractArray && !(typeof(W.dW) <: SArray)\n    return @fastmath sqrt(abs(dt))*wiener_randn(rng,W.dW)\n  else\n    return @fastmath sqrt(abs(dt))*wiener_randn(rng,typeof(W.dW))\n  end\nend","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"for the out of place versions, and for the inplace versions","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"function INPLACE_WHITE_NOISE_DIST(rand_vec,W,dt,rng)\n  wiener_randn!(rng,rand_vec)\n  sqrtabsdt = @fastmath sqrt(abs(dt))\n  @. rand_vec *= sqrtabsdt\nend","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"Optionally, we can provide a bridging distribution. This is the distribution of W(qh) for q01 given that we know W(0)=0 and W(h)=Wₕ. For Brownian motion, this is known as the Brownian Bridge, and is well known to have the distribution:","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"W(qh)  N(qWₕ(1-q)qh)","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"Thus we have the out-of-place and in-place versions as:","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"function WHITE_NOISE_BRIDGE(W,W0,Wh,q,h,rng)\n  if typeof(W.dW) <: AbstractArray\n    return @fastmath sqrt((1-q)*q*abs(h))*wiener_randn(rng,W.dW)+q*Wh\n  else\n    return @fastmath sqrt((1-q)*q*abs(h))*wiener_randn(rng,typeof(W.dW))+q*Wh\n  end\nend\nfunction INPLACE_WHITE_NOISE_BRIDGE(rand_vec,W,W0,Wh,q,h,rng)\n  wiener_randn!(rng,rand_vec)\n  #rand_vec .= sqrt((1.-q).*q.*abs(h)).*rand_vec.+q.*Wh\n  sqrtcoeff = @fastmath sqrt((1-q)*q*abs(h))\n  @. rand_vec = sqrtcoeff*rand_vec+q*Wh\nend","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"These functions are then placed in a noise process:","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"NoiseProcess(t0,W0,Z0,WHITE_NOISE_DIST,WHITE_NOISE_BRIDGE;kwargs)\nNoiseProcess(t0,W0,Z0,INPLACE_WHITE_NOISE_DIST,INPLACE_WHITE_NOISE_BRIDGE;kwargs)","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"Notice that we can optionally provide an alternative adaptive algorithm for the timestepping rejections. RSWM() defaults to the Rejection Sampling with Memory 3 algorithm (RSwM3).","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"Note that the standard constructors are simply:","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"WienerProcess(t0,W0,Z0=nothing) = NoiseProcess(t0,W0,Z0,WHITE_NOISE_DIST,WHITE_NOISE_BRIDGE;kwargs)\nWienerProcess!(t0,W0,Z0=nothing) = NoiseProcess(t0,W0,Z0,INPLACE_WHITE_NOISE_DIST,INPLACE_WHITE_NOISE_BRIDGE;kwargs)","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"These will generate a Wiener process, which can be stepped with step!(W,dt), and interpolated as W(t).","category":"page"},{"location":"features/noise_process/#Non-Standard-Noise-Processes","page":"Noise Processes","title":"Non-Standard Noise Processes","text":"","category":"section"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"In addition to the mathematically-defined noise processes above, there exists more generic functionality for building noise processes from other noise processes, from arbitrary functions, from arrays, and from approximations of stochastic differential equations.","category":"page"},{"location":"features/noise_process/#NoiseWrapper","page":"Noise Processes","title":"NoiseWrapper","text":"","category":"section"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"This produces a new noise process from an old one, which will use its interpolation to generate the noise. This allows you to re-use a previous noise process not just with the same timesteps, but also with new (adaptive) timesteps as well. Thus this is very good for doing Multi-level Monte Carlo schemes and strong convergence testing.","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"To wrap a noise process, simply use:","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"NoiseWrapper(W::NoiseProcess;reset=true)","category":"page"},{"location":"features/noise_process/#NoiseFunction","page":"Noise Processes","title":"NoiseFunction","text":"","category":"section"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"This allows you to use any arbitrary function W(t) as a NoiseProcess. This will use the function lazily, only caching values required to minimize function calls, but not store the entire noise array. This requires an initial time point t0 in the domain of W. A second function is needed if the desired SDE algorithm requires multiple processes.","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"NoiseFunction(t0,W,Z=nothing;noise_prototype=W(t0),reset=true)","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"Additionally, one can use an in-place function W(out1,out2,t) for more efficient generation of the arrays for multi-dimensional processes. When the in-place version is used without a dispatch for the out-of-place version, the noise_prototype needs to be set.","category":"page"},{"location":"features/noise_process/#NoiseGrid","page":"Noise Processes","title":"NoiseGrid","text":"","category":"section"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"A noise grid builds a noise process from arrays of points. For example, you can generate your desired noise process as an array W with timepoints t, and use the constructor:","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"NoiseGrid(t,W,Z=nothing;reset=true)","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"to build the associated noise process. This process comes with a linear interpolation of the given points, and thus the grid does not have to match the grid of integration. Thus this can be used for adaptive solutions as well. However, one must make note that the fidelity of the noise process is linked to how fine the noise grid is determined: if the noise grid is sparse on points compared to the integration, then its distributional properties may be slightly perturbed by the linear interpolation. Thus its suggested that the grid size at least approximately match the number of time steps in the integration to ensure accuracy.","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"For a one-dimensional process, W should be an AbstractVector of Numbers. For multi-dimensional processes, W should be an AbstractVector of the noise_prototype.","category":"page"},{"location":"features/noise_process/#NoiseApproximation","page":"Noise Processes","title":"NoiseApproximation","text":"","category":"section"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"In many cases, one would like to define a noise process directly by a stochastic differential equation which does not have an analytical solution. Of course, this will not be distributionally-exact and how well the properties match depends on how well the differential equation is integrated, but in many cases this can be used as a good approximation when other methods are much more difficult.","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"A NoiseApproximation is defined by a DEIntegrator. The constructor for a NoiseApproximation is:","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"NoiseApproximation(source1::DEIntegrator,source2::Union{DEIntegrator,Nothing}=nothing;reset=true)","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"The DEIntegrator should have a final time point of integration far enough such that it will not halt during the integration. For ease of use, you can use a final time point as Inf. Note that the time points do not have to match the time points of the future integration since the interpolant of the SDE solution will be used. Thus the limiting factor is error tolerance and not hitting specific points.","category":"page"},{"location":"features/noise_process/#VirtualBrownianTree","page":"Noise Processes","title":"VirtualBrownianTree","text":"","category":"section"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"A VirtualBrownianTree builds the noise process starting from an initial time t0, the first value of the proces W0, and (optionally) the first value Z0 for an auxiliary pseudo-process. The constructor is given as","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"VirtualBrownianTree(t0,W0,Z0=nothing,dist=WHITE_NOISE_DIST,bridge=VBT_BRIDGE;kwargs...)","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"where dist specifies the distribution that is used to generate the end point(s) Wend (Zend) of the noise process for the final time tend. bridge denotes the distribution of the employed Brownian bridge.  Per default tend is fixed to t0+1 but can be changed by passing a custom tend as a keyword argument. The following keyword arguments are available:","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"tend is the end time of the noise process.\nWend is the end value of the noise process.\nZend is the end value of the pseudo-noise process.\natol represents the absolute tolerance determining when the recursion is  terminated.\ntree_depth allows one to store a cache of seeds, noise values, and times  to speed up the simulation by reducing the recursion steps.\nsearch_depth maximal search depth for the tree if atol is not reached.\nrng the splittable PRNG used for generating the random numbers.  Default: Threefry4x() from the Random123 package.","category":"page"},{"location":"features/noise_process/#Examples-Using-Non-Standard-Noise-Processes","page":"Noise Processes","title":"Examples Using Non-Standard Noise Processes","text":"","category":"section"},{"location":"features/noise_process/#NoiseGrid-2","page":"Noise Processes","title":"NoiseGrid","text":"","category":"section"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"In this example, we will show you how to define your own version of Brownian motion using an array of pre-calculated points. In normal usage you should use WienerProcess instead since this will have distributionally-exact interpolations while the noise grid uses linear interpolations, but this is a nice example of the workflow.","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"To define a NoiseGrid you need to have a set of time points and a set of values for the process. Let's define a Brownian motion on (0.0,1.0) with a dt=0.001. To do this,","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"dt = 0.001\nt = 0:dt:1\nbrownian_values = cumsum([0;[sqrt(dt)*randn() for i in 1:length(t)-1]])","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"Now we build the NoiseGrid using these values:","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"W = NoiseGrid(t,brownian_values)","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"We can then pass W as the noise argument of an SDEProblem to use it in an SDE.","category":"page"},{"location":"features/noise_process/#NoiseWrapper-Example","page":"Noise Processes","title":"NoiseWrapper Example","text":"","category":"section"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"In this example, we will solve an SDE three times:","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"First to generate a noise process\nSecond with the same timesteps to show the values are the same\nThird with half-sized timsteps","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"First we will generate a noise process by solving an SDE:","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"using StochasticDiffEq,  DiffEqNoiseProcess\nf1(u, p, t) = 1.01u\ng1(u, p, t) = 1.01u\ndt = 1//2^(4)\nprob1 = SDEProblem(f1,g1,1.0,(0.0,1.0))\nsol1 = solve(prob1,EM(),dt=dt,save_noise = true)","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"Now we wrap the noise into a NoiseWrapper and solve the same problem:","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"W2 = NoiseWrapper(sol1.W)\nprob1 = SDEProblem(f1,g1,1.0,(0.0,1.0),noise=W2)\nsol2 = solve(prob1,EM(),dt=dt)","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"We can test","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"@test sol1.u ≈ sol2.u","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"to see that the values are essentially equal. Now we can use the same process to solve the same trajectory with a smaller dt:","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"W3 = NoiseWrapper(sol1.W)\nprob2 = SDEProblem(f1,g1,1.0,(0.0,1.0),noise=W3)\n\ndt = 1//2^(5)\nsol3 = solve(prob2,EM(),dt=dt)","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"We can plot the results to see what this looks like:","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"using Plots\nplot(sol1)\nplot!(sol2)\nplot!(sol3)","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"(Image: noise_process)","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"In this plot, sol2 covers up sol1 because they hit essentially the same values. You can see that sol3 its similar to the others, because it's using the same underlying noise process just sampled much finer.","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"To double check, we see that:","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"plot(sol1.W)\nplot!(sol2.W)\nplot!(sol3.W)","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"(Image: coupled_wiener)","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"the coupled Wiener processes coincide at every other time point, and the intermediate timepoints were calculated according to a Brownian bridge.","category":"page"},{"location":"features/noise_process/#Adaptive-NoiseWrapper-Example","page":"Noise Processes","title":"Adaptive NoiseWrapper Example","text":"","category":"section"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"Here we will show that the same noise can be used with the adaptive methods using the NoiseWrapper. SRI and SRIW1 use slightly different error estimators, and thus give slightly different stepping behavior. We can see how they solve the same 2D SDE differently by using the noise wrapper:","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"prob = SDEProblem(f1,g1,ones(2),(0.0,1.0))\nsol4 = solve(prob,SRI(),abstol=1e-8, save_noise = true)\n\nW2 = NoiseWrapper(sol4.W)\nprob2 = SDEProblem(f1,g1,ones(2),(0.0,1.0),noise=W2)\nsol5 = solve(prob2,SRIW1(),abstol=1e-8)\n\nusing Plots\nplot(sol4)\nplot!(sol5)","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"(Image: SRI_SRIW1_diff)","category":"page"},{"location":"features/noise_process/#NoiseApproximation-Example","page":"Noise Processes","title":"NoiseApproximation Example","text":"","category":"section"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"In this example we will show how to use the NoiseApproximation in order to build our own Geometric Brownian Motion from its stochastic differential equation definition. In normal usage, you should use the GeometricBrownianMotionProcess instead since that is more efficient and distributionally-exact.","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"First, let's define the SDEProblem. Here will use a timespan (0.0,Inf) so that way the noise can be used over an indefinite integral.","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"const μ = 1.5\nconst σ = 1.2\nf(u, p, t) = μ*u\ng(u, p, t) = σ*u\nprob = SDEProblem(f,g,1.0,(0.0,Inf))","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"Now we build the noise process by building the integrator and sending that integrator to the NoiseApproximation constructor:","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"integrator = init(prob,SRIW1())\nW = NoiseApproximation(integrator)","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"We can use this noise process like any other noise process. For example, we can now build a geometric Brownian motion whose noise process is colored noise that itself is a geometric Brownian motion:","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"prob = SDEProblem(f,g,1.0,(0.0,Inf),noise=W)","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"The possibilities are endless.","category":"page"},{"location":"features/noise_process/#VirtualBrownianTree-Example","page":"Noise Processes","title":"VirtualBrownianTree Example","text":"","category":"section"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"In this example, we define a multi-dimensional Brownian process based on a VirtualBrownianTree with a minimal tree_depth=0 such that memory consumption is minimized.","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"  W0 = zeros(10)\n  W = VirtualBrownianTree(0.0,W0; tree_depth=0)\n\n  prob = NoiseProblem(W,(0.0,1.0))\n  sol = solve(prob;dt=1/10)","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"Using a look-up cache by increasing tree_depth can significantly reduce the runtime. Thus, the VirtualBrownianTree allows for trading off speed for memory in a simple manner.","category":"page"},{"location":"features/noise_process/#NoiseFunction-Example","page":"Noise Processes","title":"NoiseFunction Example","text":"","category":"section"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"The NoiseFunction is pretty simple: pass a function. As a silly example, we can use exp as a noise process by doing:","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"f(t) = exp(t)\nW = NoiseFunction(0.0,f)","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"If it's multi-dimensional and an in-place function is used, the noise_prototype must be given. For example:","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"f(out,t) = (out.=exp(t))\nW = NoiseFunction(0.0,f,noise_prototype=rand(4))","category":"page"},{"location":"features/noise_process/","page":"Noise Processes","title":"Noise Processes","text":"This allows you to put arbitrarily weird noise into SDEs and RODEs. Have fun.","category":"page"},{"location":"basics/overview/#Overview-of-DifferentialEquations.jl","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"","category":"section"},{"location":"basics/overview/","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"The general workflow for using the package is as follows:","category":"page"},{"location":"basics/overview/","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"Define a problem\nSolve the problem\nAnalyze the output","category":"page"},{"location":"basics/overview/#Defining-Problems","page":"Overview of DifferentialEquations.jl","title":"Defining Problems","text":"","category":"section"},{"location":"basics/overview/","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"Problems are specified via a type interface. The problem types are designed to contain the necessary information to fully define their associated differential equation. Each problem type has a page explaining their problem type and the special features associated with them. For example, an ordinary differential equation is defined by","category":"page"},{"location":"basics/overview/","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"fracdudt = f(upt)","category":"page"},{"location":"basics/overview/","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"over some time interval tspan with some initial condition u0, and therefore the ODEProblem is defined by those components:","category":"page"},{"location":"basics/overview/","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"prob = ODEProblem(f,u0,tspan)\nprob = ODEProblem(f,u0,tspan,p)","category":"page"},{"location":"basics/overview/","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"Note that the number types in the solution will match the types you designate in the problem. For example, if one uses Rational{BigInt} for specifying the timespan and BigFloat for specifying the initial condition, then the solution will solve using Rational{BigInt} for the timesteps and BigFloat for the independent variables. A wide variety of number types are compatible with the solvers such as complex numbers, unitful numbers (via Unitful.jl), decimals (via DecFP), dual numbers, and many more which may not have been tested yet (thanks to the power of multiple dispatch!). For information on type-compatibilty, please see the solver pages for the specific problems.","category":"page"},{"location":"basics/overview/#Solving-the-Problems","page":"Overview of DifferentialEquations.jl","title":"Solving the Problems","text":"","category":"section"},{"location":"basics/overview/","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"Each type of differential equation has its own problem type which allow the solvers to dispatch to the right methods. The common interface for calling the solvers is:","category":"page"},{"location":"basics/overview/","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"sol = solve(prob,alg;kwargs)","category":"page"},{"location":"basics/overview/","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"Into the command, one passes the differential equation problem that they defined prob, optionally choose an algorithm alg (a default is given if not chosen), and change the properties of the solver using keyword arguments. The common arguments which are accepted by most methods is defined in the common solver options manual page. The solver returns a solution object sol which hold all of the details for the solution.","category":"page"},{"location":"basics/overview/#Analyzing-the-Solution","page":"Overview of DifferentialEquations.jl","title":"Analyzing the Solution","text":"","category":"section"},{"location":"basics/overview/","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"With the solution object, you do the analysis as you please! The solution type has a common interface which makes handling the solution similar between the different types of differential equations. Tools such as interpolations are seamlessly built into the solution interface to make analysis easy. This interface is described in the solution handling manual page.","category":"page"},{"location":"basics/overview/","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"Plotting functionality is provided by a recipe to Plots.jl. To use plot solutions, simply call the plot(sol) and the plotter will generate appropriate plots. If save_everystep was used, the plotters can generate animations of the solutions to evolution equations using the animate(sol) command. Plots can be customized using all of the keyword arguments provided by Plots.jl. Please see Plots.jl's documentation for more information.","category":"page"},{"location":"basics/overview/#Add-on-Tools","page":"Overview of DifferentialEquations.jl","title":"Add-on Tools","text":"","category":"section"},{"location":"basics/overview/","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"One of the most compelling features of DifferentialEquations.jl is that the common solver interface allows one to build tools which are \"algorithm and problem agnostic\". For example, one of the provided tools allows for performing parameter estimation on ODEProblems. Since the solve interface is the same for the different algorithms, one can use any of the associated solving algorithms. This modular structure allows one to mix and match overarching analysis tools with specialized algorithms to one's problem, leading to high performance with a large feature base. Isn't that the promise of Julia just being fulfilled?","category":"page"},{"location":"basics/overview/#Development-and-Testing-Tools","page":"Overview of DifferentialEquations.jl","title":"Development and Testing Tools","text":"","category":"section"},{"location":"basics/overview/","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"Lastly, one unique feature of DifferentialEquations.jl is the existence of algorithm development and testing functionality. This suite was designed by researchers in the field of numerical differential equations to both try out new ideas and distribute finalized results to large audiences. The tools for algorithm development allow for easy convergence testing, benchmarking, and higher order analysis (stability plotting, etc.). This is one of the reasons why DifferentialEquations.jl contains many algorithms which are unique and the results of recent publications! Please check out the developer documentation for more information on using the development tools.","category":"page"},{"location":"basics/overview/","page":"Overview of DifferentialEquations.jl","title":"Overview of DifferentialEquations.jl","text":"Note that DifferentialEquations.jl allows for distributed development, meaning that algorithms which \"plug-into ecosystem\" don't have to be a part of the major packages. If you are interested in adding your work to the ecosystem, checkout the developer documentation for more information.","category":"page"},{"location":"solvers/sdae_solve/#sdae_solve","page":"SDAE Solvers","title":"SDAE Solvers","text":"","category":"section"},{"location":"solvers/sdae_solve/#Recommended-Methods","page":"SDAE Solvers","title":"Recommended Methods","text":"","category":"section"},{"location":"solvers/sdae_solve/","page":"SDAE Solvers","title":"SDAE Solvers","text":"The recommendations for SDAEs are the same recommended implicit SDE methods for stiff equations when the SDAE is specified in mass matrix form.","category":"page"},{"location":"solvers/sdae_solve/#Mass-Matrix-Form","page":"SDAE Solvers","title":"Mass Matrix Form","text":"","category":"section"},{"location":"solvers/sdae_solve/","page":"SDAE Solvers","title":"SDAE Solvers","text":"ImplicitEM - An order 0.5 Ito drift-implicit method. This is a theta method which defaults to theta=1 or the Trapezoid method on the drift term. This method defaults to symplectic=false, but when true and theta=1/2 this is the implicit Midpoint method on the drift term and is symplectic in distribution. Can handle all forms of noise, including non-diagonal, scalar, and colored noise. Uses a 1.0/1.5 heuristic for adaptive time stepping.\nSTrapezoid - An alias for ImplicitEM with theta=1/2\nSImplicitMidpoint - An alias for ImplicitEM with theta=1/2 and symplectic=true\nImplicitEulerHeun - An order 0.5 Stratonovich drift-implicit method. This is a theta method which defaults to theta=1/2 or the Trapezoid method on the drift term. This method defaults to symplectic=false, but when true and theta=1 this is the implicit Midpoint method on the drift term and is symplectic in distribution. Can handle all forms of noise, including non-diagonal, scalar, and colored noise. Uses a 1.0/1.5 heuristic for adaptive time stepping.\nImplicitRKMil - An order 1.0 drift-implicit method. This is a theta method which defaults to theta=1 or the Trapezoid method on the drift term. Defaults to solving the Ito problem, but ImplicitRKMil(interpretation=:Stratonovich) makes it solve the Stratonovich problem. This method defaults to symplectic=false, but when true and theta=1/2 this is the implicit Midpoint method on the drift term and is symplectic in distribution. Handles diagonal and scalar noise. Uses a 1.5/2.0 heuristic for adaptive time stepping.\nISSEM - An order 0.5 split-step Ito implicit method. It is fully implicit, meaning it can handle stiffness in the noise term. This is a theta method which defaults to theta=1 or the Trapezoid method on the drift term. This method defaults to symplectic=false, but when true and theta=1/2 this is the implicit Midpoint method on the drift term and is symplectic in distribution. Can handle all forms of noise, including non-diagonal, scalar, and colored noise. Uses a 1.0/1.5 heuristic for adaptive time stepping.\nISSEulerHeun - An order 0.5 split-step Stratonovich implicit method. It is fully implicit, meaning it can handle stiffness in the noise term. This is a theta method which defaults to theta=1 or the Trapezoid method on the drift term. This method defaults to symplectic=false, but when true and theta=1/2 this is the implicit Midpoint method on the drift term and is symplectic in distribution. Can handle all forms of noise, including non-diagonal,Q scalar, and colored noise. Uses a 1.0/1.5 heuristic for adaptive time stepping.\nSKenCarp - Adaptive L-stable drift-implicit strong order 1.5 for additive Ito and Stratonovich SDEs with weak order 2. Can handle diagonal, non-diagonal and scalar additive noise.*†","category":"page"},{"location":"solvers/sdae_solve/#Notes","page":"SDAE Solvers","title":"Notes","text":"","category":"section"},{"location":"solvers/sdae_solve/","page":"SDAE Solvers","title":"SDAE Solvers","text":"†: Does not step to the interval endpoint. This can cause issues with discontinuity detection, and discrete variables need to be updated appropriately.","category":"page"},{"location":"solvers/sdae_solve/","page":"SDAE Solvers","title":"SDAE Solvers","text":"*:  Note that although SKenCarp uses the same table as KenCarp3, solving a ODE problem using SKenCarp by setting g(du,u,p,t) = du .= 0 will take much more steps than KenCarp3 because error estimator of SKenCarp is different (because of noise terms) and default value of qmax (maximum permissible ratio of relaxing/tightening dt for adaptive steps) is smaller for StochasticDiffEq algorithms.","category":"page"}]
}
